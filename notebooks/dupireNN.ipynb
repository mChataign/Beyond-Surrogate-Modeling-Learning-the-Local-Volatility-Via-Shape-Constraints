{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rQdgNr7Lt_rn"
   },
   "source": [
    "# Load modules and external files\n",
    "\n",
    "You need to import four python scripts for implied volatility calibration :\n",
    "- *newton.py*\n",
    "- *BSImplVol.py*\n",
    "- *BS.py*\n",
    "- *Bisect.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UbzsYb2nltYw"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.integrate as integrate\n",
    "from scipy import interpolate\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from io import StringIO\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import sklearn as skl\n",
    "from sklearn import preprocessing\n",
    "import importlib\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.stats as st\n",
    "import matplotlib.ticker as mtick\n",
    "import time\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RXQOZxW3tp7-"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5zOz93riuTb0"
   },
   "outputs": [],
   "source": [
    "#Load python files to google colaborative environment\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-APUi7EiuTS7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KySbpTpKuTO3"
   },
   "outputs": [],
   "source": [
    "from BS import bsformula\n",
    "from Bisect import bisect\n",
    "from newton import newton\n",
    "from BSImplVol import bsimpvol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ntAmGtpauTLK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b8kSmrOxuTEw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gdJ5B21juS5Y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0v-MujhxbB9L"
   },
   "source": [
    "# Load data with google colab \n",
    "\n",
    "You will find in github repository six days of data.\n",
    "For each day you need to load six csv files :\n",
    "- *underlying.csv* for the stock value.\n",
    "- *locvol.csv* for the local volatility calibrated with tree pricing and tikhonov volatility (see Cr√©pey (2002)).\n",
    "- *dividend.csv* for dividend extracted from put-call parity.\n",
    "- *discount.csv* for zero-coupon curve. \n",
    "- *dataTrain.csv* for prices and/or implied volatility used in training set.\n",
    "- *dataTest.csv* for prices and/or implied volatility used in testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tJh4umbkZOK3"
   },
   "outputs": [],
   "source": [
    "#Read csv files as dataFrames\n",
    "zeroCouponCurve = pd.read_csv(\"discount.csv\",decimal=\".\").apply(pd.to_numeric)\n",
    "dividendCurve = pd.read_csv(\"dividend.csv\",decimal=\".\").apply(pd.to_numeric)\n",
    "trainingData = pd.read_csv(\"dataTrain.csv\",decimal=\".\").apply(pd.to_numeric)\n",
    "testingData = pd.read_csv(\"dataTest.csv\",decimal=\".\").apply(pd.to_numeric)\n",
    "underlyingNative = pd.read_csv(\"underlying.csv\",decimal=\".\").apply(pd.to_numeric)\n",
    "localVolatilityNative = pd.read_csv(\"locvol.csv\",decimal=\".\").apply(pd.to_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JlyxIssgZOIH"
   },
   "outputs": [],
   "source": [
    "def parseDatFile(fileName):\n",
    "  s = open(fileName).read()\n",
    "  \n",
    "  defPos=s.find(\"[option]\")\n",
    "  finPos=s.find(\"[dividend]\")\n",
    "  df = pd.read_csv(StringIO(s[defPos:finPos].replace(\"\\n\\n\",\";\").replace(\"\\n\",\",\").replace(\";\",\";\\n\")),decimal=\".\", sep=\",\", header=None)\n",
    "  \n",
    "  matC = pd.to_numeric(df[1].str.split(pat=\"= \", expand=True)[1]).round(3)\n",
    "  strikeC = pd.to_numeric(df[3].str.split(pat=\"= \", expand=True)[1]).round()\n",
    "  priceC = pd.to_numeric(df[4].str.replace(\";\",\"\").str.split(pat=\"= \", expand=True)[1])\n",
    "  typeC = pd.to_numeric(df[2].str.split(pat=\"= \", expand=True)[1])\n",
    "  formattedDat = pd.DataFrame([matC, strikeC, priceC, typeC], index = [\"Maturity\", \"Strike\", \"Price\", \"Type\"]).transpose().astype({\"Type\":\"int32\"})\n",
    "  \n",
    "  filteredDat = formattedDat[formattedDat[\"Type\"]==2]\n",
    "  return filteredDat\n",
    "filteredDat = parseDatFile(\"7_8_2001__filterdax.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7--0wSGKZYhl"
   },
   "source": [
    "#### From Dat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RzR7VJaRZOFM"
   },
   "outputs": [],
   "source": [
    "def parseModelParamDatFile(fileName):\n",
    "    s = open(fileName).read()\n",
    "    \n",
    "    parts = s.split(\"\\n\\n\")\n",
    "    number1 = parts[0]\n",
    "    repo = parts[1]\n",
    "    dates = parts[2]\n",
    "    interestRates = parts[3]\n",
    "    dividendRates = parts[4]\n",
    "    \n",
    "    number2 = parts[5]\n",
    "    number3 = parts[6]\n",
    "    \n",
    "    n = parts[7]\n",
    "    sigmaRef = parts[8]\n",
    "    h = parts[9]\n",
    "    sigmaMax = parts[10]\n",
    "    sigmaMin = parts[11]\n",
    "    \n",
    "    number4 = parts[12]\n",
    "    underlying = parts[13]\n",
    "    \n",
    "    def splitRow(row):\n",
    "        return np.array(row.split(\"\\t\")).astype(np.float)\n",
    "    \n",
    "    tree = (\"\\n\".join(parts[14:])).split(\"\\n\")\n",
    "    tree.remove(\"\")\n",
    "    formattedTree = np.reshape(np.array(list(map(splitRow, tree))), (-1,3))\n",
    "    \n",
    "    \n",
    "    return pd.DataFrame(formattedTree, columns = [\"date\", \"stock(%)\", \"vol\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xAHor6QrZODA"
   },
   "outputs": [],
   "source": [
    "def parseImpliedVolDatFile(fileName):\n",
    "    s = open(fileName).read()\n",
    "    \n",
    "    parts = s.split(\"\\n\\n\")\n",
    "    \n",
    "    def splitRow(row):\n",
    "        return np.array(row.split(\"\\t\")).astype(np.float)\n",
    "    \n",
    "    testGrid = (\"\\n\".join(parts)).split(\"\\n\")\n",
    "    testGrid.remove(\"\")\n",
    "    formattedTestGrid = np.reshape(np.array(list(map(splitRow, testGrid))), (-1,4))\n",
    "    \n",
    "    return pd.DataFrame(formattedTestGrid, columns=[\"Strike\",\"Maturity\",\"Implied vol.\",\"Option price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MitIrOkaZOAQ"
   },
   "outputs": [],
   "source": [
    "def parseCalibrOutDatFile(fileName):\n",
    "    s = open(fileName).read()\n",
    "    \n",
    "    parts = s.split(\"\\n\")\n",
    "    \n",
    "    def splitRow(row):\n",
    "        return np.array(row.split(\"\\t\"))\n",
    "    def filterRow(row):\n",
    "        return len(row)==10\n",
    "    def formatRow(row):\n",
    "        return row.astype(np.float)\n",
    "    \n",
    "    #tree = (\"\\n\".join(parts)).split(\"\\n\")\n",
    "    #tree.remove(\"\")\n",
    "    filteredTrainingData = list(filter(filterRow , \n",
    "                                       list(map(splitRow, parts))))\n",
    "    formattedTrainingData = np.array(list(map(formatRow, filteredTrainingData)))\n",
    "    \n",
    "    colNames = [\"Active\", \"Option\\ntype\", \"Maturity\", \"Strike\", \"Moneyness\", \n",
    "                \"Option\\nprice\", \"Implied\\nvol.\", \"Calibrated\\nvol.\",\"Market vol. -\\nCalibrated vol.\"]\n",
    "    dfTrainingData = pd.DataFrame(formattedTrainingData[:,:-1], columns = colNames)\n",
    "    dfTrainingData[\"Active\"] = dfTrainingData[\"Active\"].astype(np.int) \n",
    "    dfTrainingData[\"Option\\ntype\"] = dfTrainingData[\"Option\\ntype\"].astype(np.int) \n",
    "    return dfTrainingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kg5chayfZN9u"
   },
   "outputs": [],
   "source": [
    "def parseDatFiles(fileName):\n",
    "    s = open(fileName).read()\n",
    "    \n",
    "    posUnderlying = s.find(\"[underlying]\")\n",
    "    posZeroCoupon = s.find(\"[zero_coupon]\")\n",
    "    posOption = s.find(\"[option]\")\n",
    "    posDividend = s.find(\"[dividend]\")\n",
    "    \n",
    "    underlyingString = s[posUnderlying:posZeroCoupon]\n",
    "    zeroCouponString = s[posZeroCoupon:posOption]\n",
    "    optionString = s[posOption:posDividend]\n",
    "    dividendString = s[posDividend:-2] \n",
    "    \n",
    "    def extractData(subStr, tag):\n",
    "        parts = subStr.replace(tag + \"\\n\", \"\").split(\"\\n\\n\")\n",
    "        try :\n",
    "            parts.remove(\"\")\n",
    "        except ValueError:\n",
    "            #Not found, we continue\n",
    "            pass\n",
    "        \n",
    "        def parseRow(row):\n",
    "            return (int(row.split(\" = \")[1]) if (row.split(\" = \")[0] == \"type\") else float(row.split(\" = \")[1]))\n",
    "        \n",
    "        def splitRow(row):\n",
    "            table = np.array(row.split(\"\\n\"))\n",
    "            parseTable = np.array(list(map(parseRow, table)))\n",
    "            return np.reshape(parseTable, (-1))\n",
    "        \n",
    "        return np.array(list(map(splitRow, parts)))\n",
    "    \n",
    "    \n",
    "    underlying = pd.DataFrame(extractData(underlyingString, \"[underlying]\"), \n",
    "                              columns=[\"S\",\"Repo\"])\n",
    "    zeroCoupon = pd.DataFrame(extractData(zeroCouponString, \"[zero_coupon] \"), \n",
    "                              columns=[\"Maturity\",\"Price\"])\n",
    "    option = pd.DataFrame(extractData(optionString, \"[option] \"), \n",
    "                          columns=[\"Maturity\",\"Type\", \"Price\", \"Strike\"])\n",
    "    option[\"Type\"] = option[\"Type\"].astype(np.int) \n",
    "    dividend = pd.DataFrame(extractData(dividendString, \"[dividend] \"), \n",
    "                            columns=[\"Maturity\",\"Amount\"])\n",
    "    return underlying, zeroCoupon, dividend, option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DjkbaSnJZN7a"
   },
   "outputs": [],
   "source": [
    "localVolatilityNative = parseModelParamDatFile(\"./esx/8_8_2001__filterdax.dat.modelparam.dat\")\n",
    "#localVolatilityNative = parseModelParamDatFile(\"./esx/30_11_1999__filteresx.dat.modelparam.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MPiBn9QFZN4x"
   },
   "outputs": [],
   "source": [
    "testingData = parseImpliedVolDatFile(\"./esx/8_8_2001__filterdax.dat.impliedvol.dat\")\n",
    "#testingData = parseImpliedVolDatFile(\"./esx/30_11_1999__filteresx.dat.impliedvol.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pw2SpL7NZN2X"
   },
   "outputs": [],
   "source": [
    "trainingData = parseCalibrOutDatFile(\"./esx/8_8_2001__filterdax.dat.calibr.out.dat\")\n",
    "#trainingData = parseCalibrOutDatFile(\"./esx/30_11_1999__filteresx.dat.calibr.out.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZA0CSToBZN0A"
   },
   "outputs": [],
   "source": [
    "underlyingNative, zeroCouponCurve, dividendCurve, filteredDat = parseDatFiles(\"./esx/8_8_2001__filterdax.dat\")\n",
    "#underlyingNative, zeroCouponCurve, dividendCurve, filteredDat = parseDatFiles(\"./esx/30_11_1999__filteresx.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N2OK4Zb0ZuMM"
   },
   "source": [
    "#### Cleaning datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T52qwu4QZNxV"
   },
   "outputs": [],
   "source": [
    "#Format dividend curve as a Pandas series\n",
    "dividendDf = dividendCurve.set_index('Maturity').sort_index()\n",
    "dividendDf.loc[1.0] = 0.0\n",
    "dividendDf.sort_index(inplace=True)\n",
    "dividendDf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R_xa3RQjZNuw"
   },
   "outputs": [],
   "source": [
    "#Format zero coupon curve as a Pandas series\n",
    "rateCurveDf = zeroCouponCurve.set_index('Maturity').sort_index()\n",
    "# keep only rates expriring before 1 year\n",
    "rateCurveDf = rateCurveDf.loc[rateCurveDf.index <= 1.01]\n",
    "rateCurveDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3iarO-REZNsN"
   },
   "outputs": [],
   "source": [
    "localVolatilityNative.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x906G5WuZNpu"
   },
   "outputs": [],
   "source": [
    "#Format local volatility\n",
    "localVolatility = localVolatilityNative.dropna()\n",
    "localVolatility[\"Strike\"] = localVolatility[\"stock(%)\"] * underlyingNative[\"S\"].values\n",
    "localVolatility[\"date\"] = localVolatility[\"date\"].round(decimals=3)\n",
    "renameDict = {\"date\": \"Maturity\", \n",
    "              \"vol\" : \"LocalVolatility\", \n",
    "              \"stock(%)\" : \"StrikePercentage\"}\n",
    "localVolatility = localVolatility.rename(columns=renameDict).set_index([\"Strike\", \"Maturity\"])\n",
    "localVolatility.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1R7vMJSnZNnN"
   },
   "outputs": [],
   "source": [
    "localVolatility.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9EJGDyjFZNko"
   },
   "outputs": [],
   "source": [
    "underlyingNative.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y7ClwgvOZNiQ"
   },
   "outputs": [],
   "source": [
    "testingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dwDYUSbFZ7NG"
   },
   "outputs": [],
   "source": [
    "#Treatment for training data\n",
    "filteredTestingData = testingData[(testingData[\"Implied vol.\"] > 0) * (testingData[\"Option price\"] > 0)]\n",
    "filteredTestingData[\"Maturity\"] = filteredTestingData[\"Maturity\"].round(decimals=3)\n",
    "renameDict = {\"Implied vol.\": \"ImpliedVol\", \n",
    "              \"Option price\" : \"Price\", \n",
    "              \"Implied delta\" : \"ImpliedDelta\", \n",
    "              \"Implied gamma\" : \"ImpliedGamma\",\n",
    "              \"Implied theta\" : \"ImpliedTheta\",\n",
    "              \"Local delta\" : \"LocalDelta\",\n",
    "              \"Local gamma\" : \"LocalGamma\"}\n",
    "formattedTestingData = filteredTestingData.rename(columns=renameDict).set_index([\"Strike\", \"Maturity\"])[\"ImpliedVol\"]\n",
    "formattedTestingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oo9dohxWZ7Kj"
   },
   "outputs": [],
   "source": [
    "trainingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l-NRe2jKZ7IW"
   },
   "outputs": [],
   "source": [
    "#Treatment for testing data\n",
    "filteredTrainingData = trainingData[(trainingData[\"Calibrated\\nvol.\"] > 0) * (trainingData[\"Option\\nprice\"] > 0) * (trainingData[\"Option\\ntype\"] == 2)]\n",
    "filteredTrainingData[\"Maturity\"] = filteredTrainingData[\"Maturity\"].round(decimals=3)\n",
    "renameDict = {\"Option\\ntype\" : \"OptionType\", \n",
    "              \"Option\\nprice\" : \"Price\", \n",
    "              \"Calibrated\\nvol.\" : \"ImpliedVol\",#\"LocalImpliedVol\", \n",
    "              \"Implied\\nvol.\" : \"LocalImpliedVol\"}#\"ImpliedVol\"}\n",
    "formattedTrainingData = filteredTrainingData.drop([\"Active\", \"Market vol. -\\nCalibrated vol.\"],axis=1).rename(columns=renameDict).set_index([\"Strike\",\"Maturity\"])\n",
    "formattedTrainingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K7Gm9kVJZ7Fs"
   },
   "outputs": [],
   "source": [
    "formattedTrainingData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vZgRcQ3nZ7BD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mvw46AEDZ6-U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tbzmkhNwZ670"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uix1XUaU8sa6"
   },
   "source": [
    "# Formatting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VljyG0NV1CID"
   },
   "source": [
    "### Boostsrapping Rate Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wRo7rm_oS-Fm"
   },
   "source": [
    "\n",
    "- For bootstrapping short rate $r$ and dividend rate $q$, we assume piecewise constant short rate for risk free rate and dividend i.e. \n",
    "$\\exp{(-\\int_{0}^{T} r_t d_t)} = \\exp{(-\\sum_{i} r_i h)}$ and $\\exp{(\\int_{0}^{T} q_t d_t)} = \\exp{(\\sum_{i} q_i h)}$.\n",
    "- $\\forall i \\in \\{0,..,N\\}$ with $ t_0 = 0$ and $t_N = T$, we have that $\\frac{\\log{B(0,t_{i+1})} - \\log{B(0,t_i)}}{h} = r_i$ with $B(0,T_i)$ the price of a bond expiring at time $t_i$. \n",
    "- For dividend, we just to substitute $B(0,T_i)$ with with spot action price plus dividend cash flow received until time $T_i$ i.e. $S_{t_0} + \\sum\\limits_i Div_{t_i}$.\n",
    "- Then we linearly interpolate $r$ and $q$.\n",
    "-  Linear interpolation is also used for integrals $\\int_{0}^{T} q_t d_t$ and $\\int_{0}^{T} r_t d_t$ in order to obtain discount factor or dividend factor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5qXMpZsnmeKv"
   },
   "outputs": [],
   "source": [
    "#Compute the integral and return the linear interpolation function \n",
    "def interpIntegral(curve):\n",
    "    #curve is piece-wise constant\n",
    "    timeDelta = curve.index.to_series().diff().fillna(0)\n",
    "    timeStep = np.linspace(0,0.99,100)\n",
    "    integralStepWise = (curve * timeDelta).cumsum()\n",
    "    integralStepWise.loc[0] = 0.0\n",
    "    integralStepWise.sort_index(inplace=True)\n",
    "    integralSpline = interpolate.interp1d(integralStepWise.index,\n",
    "                                          integralStepWise, \n",
    "                                          fill_value= 'extrapolate', \n",
    "                                          kind ='linear')\n",
    "    return pd.Series(integralSpline(timeStep),index=timeStep), integralSpline\n",
    "\n",
    "def bootstrapZeroCoupon(curvePrice, name):\n",
    "    #Bootstrap short rate curve\n",
    "    def computeShortRate(curve) :\n",
    "      shortRateList = [] \n",
    "      for i in range(curve.size):\n",
    "        if i == 0 :\n",
    "          shortRateList.append(-(np.log(curve.iloc[i]))/(curve.index[i]))\n",
    "        else : \n",
    "          shortRateList.append(-(np.log(curve.iloc[i])-np.log(curve.iloc[i-1]))/(curve.index[i]-curve.index[i-1]))\n",
    "      return pd.Series(shortRateList,index = curve.index)\n",
    "    #For t=0 we take the first available point to ensure right continuity\n",
    "    riskFreeCurve = computeShortRate(curvePrice)\n",
    "    riskFreeCurve.loc[0.00] = riskFreeCurve.iloc[0]\n",
    "    riskFreeCurve = riskFreeCurve.sort_index()\n",
    "\n",
    "    #Bootstrap yield curve\n",
    "    def zeroYield(x):\n",
    "      if(float(x.name) < 1):\n",
    "        return (1/x - 1)/float(x.name)\n",
    "      else:\n",
    "        return (x**(-1/float(x.name)) - 1)\n",
    "    yieldCurve = curvePrice.apply(zeroYield, axis = 1)\n",
    "    yieldCurve.loc[0.00] = yieldCurve.iloc[0]\n",
    "    yieldCurve = yieldCurve.sort_index()\n",
    "\n",
    "    plt.plot(riskFreeCurve, label = \"Short rate\")\n",
    "\n",
    "    #Interpolate short rate curve and yield curve\n",
    "    timeStep = np.linspace(0,0.99,100)\n",
    "    riskCurvespline = interpolate.interp1d(riskFreeCurve.index,\n",
    "                                           riskFreeCurve,#riskFreeCurve[name],\n",
    "                                           fill_value= 'extrapolate',\n",
    "                                           kind ='next')\n",
    "    interpolatedCurve = pd.Series(riskCurvespline(timeStep),index=timeStep)\n",
    "    plt.plot(interpolatedCurve, label=\"Interpolated short rate\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(yieldCurve, label = \"Yield curve\")\n",
    "    yieldCurvespline = interpolate.interp1d(yieldCurve.index,\n",
    "                                            yieldCurve['Price'],\n",
    "                                            fill_value= 'extrapolate',\n",
    "                                            kind ='next')\n",
    "    interpolatedCurve = pd.Series(yieldCurvespline(timeStep),index=timeStep)\n",
    "    plt.plot(interpolatedCurve, label = \"Interpolated Yield curve\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    #Integrate short rate\n",
    "    interpolatedIntegral, riskFreeIntegral = interpIntegral(riskFreeCurve)\n",
    "    plt.plot(interpolatedIntegral)\n",
    "    plt.show()\n",
    "\n",
    "    return riskFreeCurve, riskCurvespline, yieldCurve, yieldCurvespline, interpolatedIntegral, riskFreeIntegral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o-ahnsiAH9xh"
   },
   "outputs": [],
   "source": [
    "riskFreeCurve, riskCurvespline, yieldCurve, yieldCurvespline, interpolatedIntegral, riskFreeIntegral = bootstrapZeroCoupon(rateCurveDf, \"Short rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tkDf8dPPOW3Y"
   },
   "outputs": [],
   "source": [
    "riskFreeCurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yr0_fpL6OWdf"
   },
   "outputs": [],
   "source": [
    "interpolatedIntegral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JB4SRjCP1Wtv"
   },
   "source": [
    "### Boostraping dividend curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aawmDPX0EBMD"
   },
   "outputs": [],
   "source": [
    "def bootstrapDividend(curvePrice, underlying, name):\n",
    "    #Compute cumulative sum of dividend plus spot price\n",
    "    priceEvolution = underlying['S'].iloc[0] - curvePrice['Amount'].cumsum()\n",
    "    priceEvolution.loc[0] = underlying['S'].iloc[0]\n",
    "    priceEvolution.sort_index(inplace=True)\n",
    "\n",
    "    #Bootstrap short rate for dividend\n",
    "    def computeShortRate(curve) :\n",
    "      shortRateList = [] \n",
    "      for i in range(curve.size):\n",
    "        if i == 0 :\n",
    "          shortRateList.append(-(np.log(curve.iloc[i+1])-np.log(curve.iloc[i]))/(curve.index[i+1]-curve.index[i]))\n",
    "        else : \n",
    "          shortRateList.append(-(np.log(curve.iloc[i])-np.log(curve.iloc[i-1]))/(curve.index[i]-curve.index[i-1]))\n",
    "      return pd.Series(shortRateList,index = curve.index).dropna()\n",
    "    logReturnDividendDf = computeShortRate(priceEvolution)\n",
    "\n",
    "    #Dividend yield curve\n",
    "    def divYield(x):\n",
    "      return ((priceEvolution[x]/priceEvolution.iloc[0])**(1/float(x)) - 1) #np.log(priceEvolution[x]/priceEvolution.iloc[0])/x\n",
    "    dividendYield = logReturnDividendDf.index.to_series().tail(-1).apply(divYield)\n",
    "    dividendYield.loc[0.00] = dividendYield.iloc[0]\n",
    "    dividendYield = dividendYield.sort_index()\n",
    "\n",
    "    plt.plot(logReturnDividendDf, label = \"Short rate\")\n",
    "\n",
    "    #Interpolate short rate curve and yield curve\n",
    "    timeStep = np.linspace(0,0.99,100)\n",
    "    logReturnDividendSpline = interpolate.interp1d(logReturnDividendDf.index,\n",
    "                                                   logReturnDividendDf,#logReturnDividendDf[name],\n",
    "                                                   fill_value= 'extrapolate',\n",
    "                                                   kind ='next')\n",
    "    interpolatedCurve = pd.Series(logReturnDividendSpline(timeStep),index=timeStep)\n",
    "    plt.plot(interpolatedCurve, label=\"Interpolated short rate\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(dividendYield, label = \"Yield curve\")\n",
    "    yieldCurvespline = interpolate.interp1d(dividendYield.index,\n",
    "                                            dividendYield.values,\n",
    "                                            fill_value= 'extrapolate',\n",
    "                                            kind ='next')\n",
    "    interpolatedCurve = pd.Series(yieldCurvespline(timeStep),index=timeStep)\n",
    "    plt.plot(interpolatedCurve, label = \"Interpolated Yield curve\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    #Integrate short rate\n",
    "    interpolatedIntegral, logReturnDividendIntegral = interpIntegral(logReturnDividendDf)#logReturnDividendDf[name])\n",
    "    plt.plot(interpolatedIntegral)\n",
    "    plt.show()\n",
    "\n",
    "    return logReturnDividendDf, logReturnDividendSpline, dividendYield, yieldCurvespline, interpolatedIntegral, logReturnDividendIntegral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mgQ9b3rpbCBl"
   },
   "outputs": [],
   "source": [
    "spreadDividend, divSpline, yieldDividend, divYieldSpline, interpolatedIntegral, divSpreadIntegral  = bootstrapDividend(dividendDf, underlyingNative, \"Spread\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KJonVnKvOmfN"
   },
   "outputs": [],
   "source": [
    "spreadDividend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-50wFPo7OmWP"
   },
   "outputs": [],
   "source": [
    "interpolatedIntegral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ucimXKUjVzDE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OlfrW4Gn2N4B"
   },
   "source": [
    "### Pricing black-scholes price\n",
    "\n",
    "#### Change of variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J5RE5f5UaTUT"
   },
   "source": [
    "- In presence of dividend rate $d$ and risk free rate $r$ Dupire formula is :   $$\\sigma^2(T,K) = 2 \\frac{ \\partial_T P(T,K) + (r-q)\\partial_K P(T,K) + qP(T,K)}{K¬≤ \\partial_{K}^2 P(T,K)}$$ \n",
    "with Strike $K$, Maturity $T$, dividend rate $q$ and risk-free rate $r$, $P$ our pricing function. \n",
    "- We apply the following change of variable : $$ w(T,k) = \\exp{(\\int_{0}^{T} q_t dt)} P(T,K)$$ with $K = k \\exp{(\\int_{0}^{T} (r_t - q_t) dt)} $.\n",
    "\n",
    "- Then Dupire equation becomes :  $\\sigma^2(T,K) = 2 \\frac{ \\partial_T w(T,k)}{k¬≤ \\partial_{k}^2 w(T,k)}$. \n",
    "- If we learn the mapping $v$ with a neural network then we should obtain quickly by adjoint differentiation $\\partial_T w$ and $\\partial_{k¬≤}^2 w$ and therefore $\\sigma$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r8Cbfr7sT1jq"
   },
   "outputs": [],
   "source": [
    "#Linear interpolation combined with Nearest neighbor extrapolation\n",
    "def customInterpolator(interpolatedData, newStrike, newMaturity):\n",
    "  strikeRef = np.ravel(interpolatedData.index.get_level_values(\"Strike\").values)\n",
    "  maturityRef = np.ravel(interpolatedData.index.get_level_values(\"Maturity\").values)\n",
    "  xym = np.vstack((strikeRef, maturityRef)).T\n",
    "\n",
    "  fInterpolation = interpolate.griddata(xym,\n",
    "                                        interpolatedData.values.flatten(),\n",
    "                                        (newStrike, newMaturity),\n",
    "                                        method = 'linear',\n",
    "                                        rescale=True)\n",
    "\n",
    "  fExtrapolation =  interpolate.griddata(xym,\n",
    "                                         interpolatedData.values.flatten(),\n",
    "                                         (newStrike, newMaturity),\n",
    "                                         method = 'nearest',\n",
    "                                         rescale=True)\n",
    "    \n",
    "  return np.where(np.isnan(fInterpolation), fExtrapolation, fInterpolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s5Zydj3k5Y-d"
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "#Density derivative\n",
    "def dpdf(x):\n",
    "    v = 1\n",
    "    return -x*np.exp(-x**2/(2.0*v**2))/(v**3*np.sqrt(2.0*np.pi))\n",
    "    \n",
    "\n",
    "def generalizedGreeks(cp, s, k, rf, t, v, div, rfInt, divInt):\n",
    "        \"\"\" Price an option using the Black-Scholes model.\n",
    "        cp: +1/-1 for call/put\n",
    "        s: initial stock price\n",
    "        k: strike price\n",
    "        t: expiration time\n",
    "        v: volatility\n",
    "        rf: risk-free rate at time t\n",
    "        div: dividend at time t\n",
    "        rfInt: deterministic risk-free rate integrated between 0 and t\n",
    "        divInt: deterministic dividend integrated between 0 and t\n",
    "        \"\"\"\n",
    "\n",
    "        d1 = (np.log(s/k)+(rfInt-divInt+0.5*v*v*t))/(v*np.sqrt(t))\n",
    "        d2 = d1 - v*np.sqrt(t)\n",
    "        \n",
    "        Nd1 = st.norm.cdf(cp*d1)\n",
    "        Nd2 = st.norm.cdf(cp*d2)\n",
    "\n",
    "        discountFactor = np.exp(-rfInt)\n",
    "        forwardFactor = np.exp(-divInt)\n",
    "        avgDiv = divInt/t\n",
    "        avgRf = rfInt/t\n",
    "\n",
    "        optprice = (cp*s*forwardFactor*Nd1) - (cp*k*discountFactor*Nd2)\n",
    "\n",
    "        delta = cp*Nd1\n",
    "        vega  = s*np.sqrt(t)*st.norm.pdf(d1)\n",
    "        delta_k = -s*forwardFactor*Nd1/(v*np.sqrt(t)*k) - cp*discountFactor*Nd2 + k*discountFactor*Nd2/(v*np.sqrt(t)*k)\n",
    "        \n",
    "        gamma_k = s*forwardFactor/((v*np.sqrt(t)*k)**2)*(Nd1*v*np.sqrt(t) + cp*dpdf(cp*d1)) - k*discountFactor/((v*np.sqrt(t)*k)**2)*(Nd2*v*np.sqrt(t) + cp*dpdf(cp*d2)) +  2.0*discountFactor*Nd2/(v*np.sqrt(t)*k)  \n",
    "\n",
    "        dd1_dt = (avgRf-avgDiv+0.5*v*v)/(v*np.sqrt(t)) - 0.5*(np.log(s/k)+(rfInt-divInt+0.5*v*v*t))/(v*v*t**(3/2))\n",
    "        dd2_dt = dd1_dt - 0.5*v/np.sqrt(t)\n",
    "        delta_T = avgRf*cp*k*discountFactor*Nd2 - avgDiv*cp*s*forwardFactor*Nd1 + s*forwardFactor*Nd1*dd1_dt- k*discountFactor*Nd2*dd2_dt\n",
    "        \n",
    "        return optprice, delta, vega, delta_k, gamma_k, delta_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HbOR7gse5Y4x"
   },
   "outputs": [],
   "source": [
    "S0 = underlyingNative[\"S\"].values\n",
    "#Change of variable for deterministic discount curve and dividend curve\n",
    "def changeOfVariable(s,t):\n",
    "  def qInterp(m):\n",
    "    return divSpreadIntegral(m).astype(np.float32)\n",
    "  q = qInterp(t)\n",
    "  \n",
    "  def rInterp(m):\n",
    "    return riskFreeIntegral(m).astype(np.float32)\n",
    "  r = rInterp(t)\n",
    "\n",
    "  factorPrice = np.exp( - q )\n",
    "\n",
    "  divSpread = q-r\n",
    "\n",
    "  factorStrike = np.exp( divSpread )\n",
    "  adjustedStrike = np.multiply(s, factorStrike)\n",
    "  return adjustedStrike, factorPrice\n",
    "\n",
    "#Change of variable for constant discount and dividend short rate \n",
    "def changeOfVariable_BS(s,t):\n",
    "  \n",
    "  factorPrice = np.exp( - q*t )\n",
    "\n",
    "  divSpread = (q-r)*t\n",
    "\n",
    "  factorStrike = np.exp( divSpread )\n",
    "  adjustedStrike = np.multiply(s, factorStrike)\n",
    "  return adjustedStrike, factorPrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jPH_FwsHUznO"
   },
   "outputs": [],
   "source": [
    "#Generate a proper dataset from implied volatility\n",
    "def generateData(impliedVol,\n",
    "                 S0,\n",
    "                 rIntegralSpline,\n",
    "                 qIntegralSpline,\n",
    "                 rSpline,\n",
    "                 qSpline,\n",
    "                 priceDf = None,\n",
    "                 spotValue = True):\n",
    "  #Get grid coordinates\n",
    "  if priceDf is None :\n",
    "    x_train = impliedVol.index.to_frame()\n",
    "    #Get implied vol by interpolating another grid\n",
    "    x_train[\"ImpliedVol\"] = impliedVol\n",
    "  else :\n",
    "    x_train = pd.MultiIndex.from_arrays([priceDf[\"Strike\"], priceDf[\"Maturity\"]], \n",
    "                                        names=('Strike', 'Maturity')).to_frame()\n",
    "    #Get implied vol by interpolating another grid\n",
    "    x_train[\"ImpliedVol\"] = customInterpolator(impliedVol, \n",
    "                                               x_train[\"Strike\"], \n",
    "                                               x_train[\"Maturity\"])\n",
    "  #Get sensitivities and prices\n",
    "  isPut = True\n",
    "  cp = -1 if isPut else 1\n",
    "  impliedPriceFunction = lambda x : generalizedGreeks(cp, \n",
    "                                                      S0, \n",
    "                                                      x[\"Strike\"] , \n",
    "                                                      rSpline(x[\"Maturity\"]), \n",
    "                                                      x[\"Maturity\"], \n",
    "                                                      x[\"ImpliedVol\"], \n",
    "                                                      qSpline(x[\"Maturity\"]), \n",
    "                                                      rIntegralSpline(x[\"Maturity\"]), \n",
    "                                                      qIntegralSpline(x[\"Maturity\"]))\n",
    "  \n",
    "  res = np.reshape(np.array(list(zip(x_train.apply(impliedPriceFunction,axis=1).values))),\n",
    "                   (x_train.shape[0], 6))  # put greeks\n",
    "  prices = res[:,0] if priceDf is None else priceDf[\"Price\"].values\n",
    "  deltas = res[:,1]\n",
    "  vegas = res[:,2]\n",
    "  delta_ks = res[:,3]\n",
    "  gamma_ks = res[:,4]\n",
    "  delta_Ts = res[:,5]\n",
    "  \n",
    "  #Vega for optional loss weighting\n",
    "  sigmaRef = 0.25\n",
    "  impliedPriceFunction = lambda x : generalizedGreeks(cp, \n",
    "                                                      S0, \n",
    "                                                      x[\"Strike\"] , \n",
    "                                                      rSpline(x[\"Maturity\"]), \n",
    "                                                      x[\"Maturity\"], \n",
    "                                                      sigmaRef, \n",
    "                                                      qSpline(x[\"Maturity\"]), \n",
    "                                                      rIntegralSpline(x[\"Maturity\"]), \n",
    "                                                      qIntegralSpline(x[\"Maturity\"]))\n",
    "  \n",
    "  res1 = np.reshape(np.array(list(zip(x_train.apply(impliedPriceFunction,axis=1).values))),\n",
    "                    (x_train.shape[0], 6))  # put greeks\n",
    "  \n",
    "  #Get adjusted strike for the change of variables\n",
    "  changedVar = changeOfVariable(x_train[\"Strike\"],x_train[\"Maturity\"])\n",
    "  \n",
    "  multiIndex = x_train[\"ImpliedVol\"].index\n",
    "\n",
    "  #Gather all data as a Dataframe \n",
    "  cols = [\"Price\", \"Delta\", \"Vega\", \"Delta Strike\", \"Gamma Strike\", \n",
    "          \"Theta\", \"ChangedStrike\", \"DividendFactor\", \"Strike\", \"Maturity\", \"ImpliedVol\", \"VegaRef\"]\n",
    "\n",
    "  dfData = np.vstack((prices, deltas, vegas, delta_ks, gamma_ks, delta_Ts) + \n",
    "                     changedVar + (x_train[\"Strike\"], x_train[\"Maturity\"], x_train[\"ImpliedVol\"], res1[:,2]))\n",
    "  \n",
    "  df = pd.DataFrame(dfData.T , columns=cols, index = multiIndex)\n",
    "\n",
    "  #Add pricing with spot delivery\n",
    "  if spotValue : \n",
    "    KAvailable = multiIndex.get_level_values(\"Strike\").unique()\n",
    "    TSpot = np.zeros_like(KAvailable)\n",
    "    priceSpot = np.maximum(KAvailable- S0[0],0)\n",
    "    deltaSpot = -np.sign(np.maximum(KAvailable- S0[0],0))\n",
    "    gammaSpot = np.zeros_like(deltaSpot)\n",
    "    vegasSpot = gammaSpot\n",
    "    deltaKSpot = np.sign(np.maximum(KAvailable- S0[0],0))\n",
    "    thetaSpot = 1000000 * deltaKSpot\n",
    "\n",
    "    #Ignore implied vol for T=0\n",
    "    impliedSpot = np.zeros_like(thetaSpot)\n",
    "\n",
    "    changedVarSpot = changeOfVariable(KAvailable,TSpot)\n",
    "    \n",
    "    dfDataSpot = np.vstack((priceSpot, deltaSpot, vegasSpot, deltaKSpot, gammaSpot, thetaSpot) +\n",
    "                          changedVarSpot + (KAvailable, TSpot, impliedSpot, vegasSpot))\n",
    "    indexSpot = pd.MultiIndex.from_arrays([np.array(KAvailable), TSpot], names=('Strike', 'Maturity'))\n",
    "    dfSpot = pd.DataFrame(dfDataSpot.T , columns=cols, index = indexSpot)\n",
    "    df = df.append(dfSpot).sort_index()\n",
    "\n",
    "  #Add forward logmoneyness if we want to calibrate local volatility from implied volatilities\n",
    "  df[\"logMoneyness\"] = np.log(df[\"ChangedStrike\"] / S0[0]) \n",
    "  df[\"impliedTotalVariance\"] = np.square(df[\"ImpliedVol\"]) #*  df[\"Maturity\"]\n",
    "\n",
    "  return df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RKgC3tZXoVXJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ymMfIqK7oPkS"
   },
   "outputs": [],
   "source": [
    "S0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Navdr2beN2D"
   },
   "outputs": [],
   "source": [
    "formattedTrainingData.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KpFHsuuEovZw"
   },
   "outputs": [],
   "source": [
    "testingData.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3sQVozdp5Y1p"
   },
   "outputs": [],
   "source": [
    "testingDataSet = generateData(formattedTestingData,\n",
    "                              S0,\n",
    "                              riskFreeIntegral,\n",
    "                              divSpreadIntegral,\n",
    "                              riskCurvespline,\n",
    "                              divSpline)\n",
    "testingDataSet.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c3Do8EBH5YnO"
   },
   "outputs": [],
   "source": [
    "#Checking call put parity\n",
    "maturity = testingData.iloc[-4][\"Maturity\"]\n",
    "strike = testingData.iloc[-4][\"Strike\"]\n",
    "(S0 * np.exp(-divSpreadIntegral(maturity))  - np.exp(-riskFreeIntegral(maturity)) * strike) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aykFRj_xqKY_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jmAkv50Td1dJ"
   },
   "outputs": [],
   "source": [
    "#Put call parity\n",
    "testingData.iloc[-4][\"Option price\"] - testingDataSet.loc[(testingData.iloc[-4][\"Strike\"],round(testingData.iloc[-4][\"Maturity\"],3))][\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0ag0ChkGEkP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "715238Hglz63"
   },
   "outputs": [],
   "source": [
    "#Use all prices in dat files\n",
    "trainingDataSet = generateData(formattedTrainingData[\"ImpliedVol\"],\n",
    "                               S0,\n",
    "                               riskFreeIntegral,\n",
    "                               divSpreadIntegral,\n",
    "                               riskCurvespline,\n",
    "                               divSpline,\n",
    "                               priceDf = filteredDat)\n",
    "trainingDataSet.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KNvpFAU1FN78"
   },
   "outputs": [],
   "source": [
    "#Use same prices as those for tikhonov calibration\n",
    "trainingDataSet = generateData(formattedTrainingData[\"ImpliedVol\"], \n",
    "                               S0, \n",
    "                               riskFreeIntegral, \n",
    "                               divSpreadIntegral, \n",
    "                               riskCurvespline, \n",
    "                               divSpline,\n",
    "                               priceDf = formattedTrainingData.reset_index())\n",
    "trainingDataSet.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0TDRVwgkFN4c"
   },
   "outputs": [],
   "source": [
    "filteredTrainingData.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WCkjHBiSFN2D"
   },
   "outputs": [],
   "source": [
    "filteredTrainingData[filteredTrainingData[\"Strike\"]==5900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xj_4dnWpFNul"
   },
   "outputs": [],
   "source": [
    "trainingDataSet[trainingDataSet[\"Strike\"]==5900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z5u31XfRfe5s"
   },
   "outputs": [],
   "source": [
    "filteredTrainingData[filteredTrainingData[\"Strike\"]==4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1bGr3ZOPfev0"
   },
   "outputs": [],
   "source": [
    "trainingDataSet[trainingDataSet[\"Strike\"]==4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lC01cOOrB6kE"
   },
   "outputs": [],
   "source": [
    "localVolatility.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5oBekB4C5YyD"
   },
   "outputs": [],
   "source": [
    "testingData.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ttVRKskQEi4g"
   },
   "outputs": [],
   "source": [
    "#Get local volatility from Cr√©pey (2002) by nearest neighbour interpolation\n",
    "def interpolatedLocalVolatility(localVol, priceGrid):\n",
    "    \n",
    "    strikePrice = priceGrid.index.get_level_values(\"Strike\").values.flatten()\n",
    "    maturityPrice = priceGrid.index.get_level_values(\"Maturity\").values.flatten()\n",
    "    coordinates = customInterpolator(localVol[\"LocalVolatility\"], strikePrice, maturityPrice)\n",
    " \n",
    "\n",
    "    return pd.Series(coordinates, index = priceGrid.index)\n",
    "\n",
    "trainingDataSet[\"locvol\"] = interpolatedLocalVolatility(localVolatility, trainingDataSet[\"Price\"])\n",
    "testingDataSet[\"locvol\"] = interpolatedLocalVolatility(localVolatility, testingDataSet[\"Price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wyOafgBuKk1d"
   },
   "outputs": [],
   "source": [
    "localVolatility[localVolatility.index.get_level_values(\"Maturity\") <= 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QUxTjD2zY706"
   },
   "outputs": [],
   "source": [
    "dataSet = trainingDataSet #Training set\n",
    "dataSetTest = testingDataSet #Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jAEIN8NVowfg"
   },
   "outputs": [],
   "source": [
    "#Data for gaussian processes\n",
    "tGrid = np.linspace(0, 1, 101)\n",
    "exportedRiskFreeIntegral = riskFreeIntegral(tGrid),\n",
    "exportedDivSpreadIntegral = divSpreadIntegral(tGrid)\n",
    "exportedRRiskCurvespline = riskCurvespline(tGrid),\n",
    "exportedDivSpline = divSpline(tGrid)\n",
    "dfCurve = pd.DataFrame(np.vstack([exportedRiskFreeIntegral, exportedDivSpreadIntegral, exportedRRiskCurvespline, exportedDivSpline]).T,\n",
    "                       columns=[\"riskFreeIntegral\",\"divSpreadIntegral\",\"riskCurvespline\",\"divSpline\"], \n",
    "                       index = tGrid)\n",
    "#Discount and dividend curve\n",
    "dfCurve.to_csv(\"dfCurve.csv\")\n",
    "#Training dataset\n",
    "dataSet.to_csv(\"trainingDataSet.csv\")\n",
    "#Testing dataset\n",
    "dataSetTest.to_csv(\"testingDataSet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XFKey5xg68I"
   },
   "outputs": [],
   "source": [
    "dataSetTest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u8HmA41Xg67P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vsQ55bK5g66T"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hHGmJOe2g63_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7v2I8qPzg6nc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RyOtl8iCziXl"
   },
   "source": [
    "# Neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gTX3rV8ka02z"
   },
   "source": [
    "## Scaling methods\n",
    "\n",
    "Use min-max of scaling strike between 0 et 1 for improving stability of neural network training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BLqa33lNSXTB"
   },
   "outputs": [],
   "source": [
    "def transformCustomMinMax(df, scaler):\n",
    "  return pd.DataFrame(scaler.transform(df),\n",
    "                      index = df.index, \n",
    "                      columns = df.columns)\n",
    "#Reverse operation min-max scaling\n",
    "def inverseTransformMinMax(df, scaler):\n",
    "  return pd.DataFrame(scaler.inverse_transform(df),\n",
    "                      index = df.index, \n",
    "                      columns = df.columns)\n",
    "#Same thing but for a particular column\n",
    "def inverseTransformColumnMinMax(originalDf, scaler, column):\n",
    "  colIndex = originalDf.columns.get_loc(column.name)\n",
    "  maxCol = scaler.data_max_[colIndex]\n",
    "  minCol = scaler.data_min_[colIndex]\n",
    "  return pd.Series(minCol + (maxCol - minCol) * column, index = column.index).rename(column.name)  \n",
    "#Reverse transform of min-max scaling but for greeks   \n",
    "def inverseTransformColumnGreeksMinMax(originalDf, \n",
    "                                       scaler,\n",
    "                                       columnDerivative,\n",
    "                                       columnFunctionName,\n",
    "                                       columnVariableName,\n",
    "                                       order = 1):\n",
    "  colFunctionIndex = originalDf.columns.get_loc(columnFunctionName)\n",
    "  maxColFunction = scaler.data_max_[colFunctionIndex]\n",
    "  minColFunction = scaler.data_min_[colFunctionIndex]\n",
    "  scaleFunction = (maxColFunction - minColFunction)\n",
    "  \n",
    "  colVariableIndex = originalDf.columns.get_loc(columnVariableName)\n",
    "  maxColVariable = scaler.data_max_[colVariableIndex]\n",
    "  minColVariable = scaler.data_min_[colVariableIndex]\n",
    "  scaleVariable = (maxColVariable - minColVariable) ** order\n",
    "\n",
    "  return pd.Series(scaleFunction * columnDerivative / scaleVariable , \n",
    "                   index = columnDerivative.index).rename(columnDerivative.name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZFGpjMuoaJOh"
   },
   "outputs": [],
   "source": [
    "#Tools functions for min-max scaling\n",
    "def transformCustomId(df, scaler):\n",
    "  return pd.DataFrame(df,\n",
    "                      index = df.index, \n",
    "                      columns = df.columns)\n",
    "def inverseTransformId(df, scaler):\n",
    "  return pd.DataFrame(df,\n",
    "                      index = df.index, \n",
    "                      columns = df.columns)\n",
    "def inverseTransformColumnId(originalDf, scaler, column):\n",
    "  return pd.Series(column, index = column.index).rename(column.name)  \n",
    "\n",
    "def inverseTransformColumnGreeksId(originalDf, scaler, \n",
    "                                 columnDerivative, \n",
    "                                 columnFunctionName, \n",
    "                                 columnVariableName,\n",
    "                                 order = 1):\n",
    "  return pd.Series(columnDerivative , index = columnDerivative.index).rename(columnDerivative.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XkPnYeFSbWKt"
   },
   "outputs": [],
   "source": [
    "activateScaling = False\n",
    "transformCustom = transformCustomMinMax if activateScaling else transformCustomId\n",
    "inverseTransform = inverseTransformMinMax if activateScaling else inverseTransformId\n",
    "inverseTransformColumn = inverseTransformColumnMinMax if activateScaling else inverseTransformColumnId\n",
    "inverseTransformColumnGreeks = inverseTransformColumnGreeksMinMax if activateScaling else inverseTransformColumnGreeksId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "83ZlEZ7Ys0tA"
   },
   "outputs": [],
   "source": [
    "scaler = skl.preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(dataSet)\n",
    "scaledDataSet = transformCustom(dataSet, scaler)\n",
    "scaledDataSetTest = transformCustom(dataSetTest, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sLjz7YjBw7o5"
   },
   "outputs": [],
   "source": [
    "scaledDataSet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lAUxbEzOnLMh"
   },
   "outputs": [],
   "source": [
    "#Search strike for ATM option\n",
    "midS0 = dataSet[dataSet.index.get_level_values(\"Strike\") >= S0[0]].index.get_level_values(\"Strike\").min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aVDBRx3BbcFW"
   },
   "source": [
    "## Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6McrN8zK6WL2"
   },
   "outputs": [],
   "source": [
    "#Plot loss for each epoch \n",
    "def plotEpochLoss(lossSerie):\n",
    "  fig = plt.figure(figsize=(20,10))\n",
    "  ax = fig.gca()\n",
    "  \n",
    "  ax.plot(lossSerie , \"-\", color=\"black\")\n",
    "  ax.set_xlabel(\"Epoch number\", fontsize=18, labelpad=20)\n",
    "  ax.set_ylabel(\"Logarithmic Loss\", fontsize=18, labelpad=20)\n",
    "  ax.set_title(\"Training Loss evolution\", fontsize=24)\n",
    "  ax.tick_params(labelsize=16)\n",
    "  ax.set_facecolor('white')\n",
    "  plt.show()\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DTXqhgHOSP7r"
   },
   "outputs": [],
   "source": [
    "KMin = 0.7 * S0[0]\n",
    "KMax = 1.3 * S0[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDz4FSbcprg7"
   },
   "outputs": [],
   "source": [
    "#Plot a surface as a superposition of curves\n",
    "def plotMultipleCurve(data,\n",
    "                      Title = 'True Price Surface',\n",
    "                      yMin = KMin,\n",
    "                      yMax = KMax,\n",
    "                      zAsPercent = False):\n",
    "  \n",
    "\n",
    "  dataCurve = data[(data.index.get_level_values(\"Strike\") <= yMax) * (data.index.get_level_values(\"Strike\") >= yMin)]\n",
    "\n",
    "  fig = plt.figure(figsize=(20,10))\n",
    "  ax = fig.gca()\n",
    "\n",
    "  for t in np.linspace(0,0.8,9) :\n",
    "    k = dataCurve[dataCurve.index.get_level_values(\"Maturity\") >= t].index.get_level_values(\"Maturity\").unique().min()\n",
    "    curveK = dataCurve[dataCurve.index.get_level_values(\"Maturity\")==k]\n",
    "    dataSerie = pd.Series(curveK.values * (100 if zAsPercent else 1) ,\n",
    "                          index = curveK.index.get_level_values(\"Strike\"))\n",
    "    ax.plot(dataSerie , \"--+\", label=str(k))\n",
    "  ax.legend()  \n",
    "  ax.set_xlabel(data.index.names[0], fontsize=18, labelpad=20)\n",
    "  ax.set_ylabel(data.name, fontsize=18, labelpad=20)\n",
    "  if zAsPercent :\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "  ax.set_title(Title, fontsize=24)\n",
    "  ax.tick_params(labelsize=16)\n",
    "  ax.set_facecolor('white')\n",
    "  plt.show()\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NrzucF2AQznx"
   },
   "outputs": [],
   "source": [
    "plotMultipleCurve(localVolatility[\"LocalVolatility\"][localVolatility.index.get_level_values(\"Maturity\")>0.01],\n",
    "                  Title = 'Local Volatility Surface',\n",
    "                  yMin=0.7*S0[0],\n",
    "                  yMax=1.4*S0[0], \n",
    "                  zAsPercent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZYhSZTjM2Lqp"
   },
   "outputs": [],
   "source": [
    "#Plotting function for surface\n",
    "#xTitle : title for x axis\n",
    "#yTitle : title for y axis\n",
    "#zTitle : title for z axis\n",
    "#Title : plot title\n",
    "#az : azimuth i.e. angle of view for surface\n",
    "#yMin : minimum value for y axis\n",
    "#yMax : maximum value for y axis\n",
    "#zAsPercent : boolean, if true format zaxis as percentage \n",
    "def plotGridCustom(coordinates, zValue,\n",
    "                   xTitle = \"Maturity\",\n",
    "                   yTitle = \"Strike\",\n",
    "                   zTitle = \"Price\",\n",
    "                   Title = 'True Price Surface', \n",
    "                   az=320, \n",
    "                   yMin = KMin,\n",
    "                   yMax = KMax,\n",
    "                   zAsPercent = False):\n",
    "  y = coordinates[:,0]\n",
    "  filteredValue = (y > yMin) & (y < yMax)\n",
    "  x = coordinates[:,1][filteredValue]\n",
    "  y = coordinates[:,0][filteredValue]\n",
    "  z = zValue[filteredValue].flatten()\n",
    "  \n",
    "  fig = plt.figure(figsize=(20,10))\n",
    "  ax = fig.gca(projection='3d')\n",
    "  \n",
    "  ax.set_xlabel(xTitle, fontsize=18, labelpad=20)\n",
    "  ax.set_ylabel(yTitle, fontsize=18, labelpad=20)\n",
    "  ax.set_zlabel(zTitle, fontsize=18, labelpad=10)\n",
    "  \n",
    "  cmap=plt.get_cmap(\"inferno\")\n",
    "  colors=cmap(z * 100 if zAsPercent else z)[np.newaxis, :, :3]\n",
    "  surf = ax.plot_trisurf(x, y,\n",
    "                         z * 100 if zAsPercent else z ,\n",
    "                         linewidth=1.0,\n",
    "                         antialiased=True, \n",
    "                         cmap = cmap,\n",
    "                         color=(0,0,0,0))\n",
    "  scaleEdgeValue = surf.to_rgba(surf.get_array())\n",
    "  surf.set_edgecolors(scaleEdgeValue) \n",
    "  surf.set_alpha(0)\n",
    "\n",
    "  if zAsPercent :\n",
    "    ax.zaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "  ax.view_init(elev=10., azim=az)\n",
    "  ax.set_title(Title, fontsize=24)\n",
    "  ax.set_facecolor('white')\n",
    "\n",
    "  plt.tick_params(labelsize=16)\n",
    "\n",
    "  \n",
    "  plt.show()\n",
    "\n",
    "\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2tGvs9RI0bC3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tE34jVfx0a2v"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cTNEnEsWbBOE"
   },
   "outputs": [],
   "source": [
    "#Plotting function from a dataframe\n",
    "def plotSurface(data, \n",
    "                zName, \n",
    "                Title = 'True Price Surface', \n",
    "                az=320,\n",
    "                yMin = KMin,\n",
    "                yMax = KMax,\n",
    "                zAsPercent = False):\n",
    "  plotGridCustom(data.index.to_frame().values, \n",
    "                 data[zName].values,\n",
    "                 xTitle = data.index.names[1],\n",
    "                 yTitle = data.index.names[0],\n",
    "                 zTitle = zName,\n",
    "                 Title = Title, \n",
    "                 az=az, \n",
    "                 yMin = yMin, \n",
    "                 yMax = yMax, \n",
    "                 zAsPercent=zAsPercent)\n",
    "  return\n",
    "\n",
    "#Plotting function from a pandas series\n",
    "def plotSerie(data,\n",
    "              Title = 'True Price Surface',\n",
    "              az=320,\n",
    "              yMin = KMin,\n",
    "              yMax = KMax, \n",
    "              zAsPercent = False):\n",
    "  \n",
    "\n",
    "  plotGridCustom(data.index.to_frame().values, \n",
    "                 data.values,\n",
    "                 xTitle = data.index.names[1],\n",
    "                 yTitle = data.index.names[0],\n",
    "                 zTitle = data.name,\n",
    "                 Title = Title, \n",
    "                 az=az, \n",
    "                 yMin = yMin, \n",
    "                 yMax = yMax, \n",
    "                 zAsPercent = zAsPercent)\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X1hknqmZHSKW"
   },
   "outputs": [],
   "source": [
    "#Plotting function for surface\n",
    "#xTitle : title for x axis\n",
    "#yTitle : title for y axis\n",
    "#zTitle : title for z axis\n",
    "#Title : plot title\n",
    "#az : azimuth i.e. angle of view for surface\n",
    "#yMin : minimum value for y axis\n",
    "#yMax : maximum value for y axis\n",
    "#zAsPercent : boolean, if true format zaxis as percentage \n",
    "def plot2GridCustom(coordinates, zValue,\n",
    "                    coordinates2, zValue2,\n",
    "                    xTitle = \"Maturity\",\n",
    "                    yTitle = \"Strike\",\n",
    "                    zTitle = \"Price\",\n",
    "                    Title = 'True Price Surface', \n",
    "                    az=320, \n",
    "                    yMin = KMin,\n",
    "                    yMax = KMax,\n",
    "                    zAsPercent = False):\n",
    "  y = coordinates[:,0]\n",
    "  filteredValue = (y > yMin) & (y < yMax)\n",
    "  x = coordinates[:,1][filteredValue]\n",
    "  y = coordinates[:,0][filteredValue]\n",
    "  z = zValue[filteredValue].flatten()\n",
    "  \n",
    "  y2 = coordinates2[:,0]\n",
    "  filteredValue2 = (y2 > yMin) & (y2 < yMax)\n",
    "  x2 = coordinates2[:,1][filteredValue2]\n",
    "  y2 = coordinates2[:,0][filteredValue2]\n",
    "  z2 = zValue2[filteredValue2].flatten()\n",
    "  \n",
    "  fig = plt.figure(figsize=(20,10))\n",
    "  ax = fig.gca(projection='3d')\n",
    "  \n",
    "  ax.set_xlabel(xTitle, fontsize=18, labelpad=20)\n",
    "  ax.set_ylabel(yTitle, fontsize=18, labelpad=20)\n",
    "  ax.set_zlabel(zTitle, fontsize=18, labelpad=10)\n",
    "  \n",
    "  cmap=plt.get_cmap(\"inferno\")\n",
    "  colors=cmap(z * 100 if zAsPercent else z)[np.newaxis, :, :3]\n",
    "  ax.scatter(x2, y2, z2, marker='o', color=\"r\", alpha=1, s=40)\n",
    "  ax.scatter(x, y, z, marker='o', color=\"b\", alpha=1, s=40)\n",
    "  #surf = ax.plot_trisurf(x, y,\n",
    "  #                       z * 100 if zAsPercent else z ,\n",
    "  #                       linewidth=1.0,\n",
    "  #                       antialiased=True, \n",
    "  #                       cmap = cmap,\n",
    "  #                       color=(0,0,0,0))\n",
    "  #scaleEdgeValue = surf.to_rgba(surf.get_array())\n",
    "  #surf.set_edgecolors(scaleEdgeValue) \n",
    "  #surf.set_alpha(0)\n",
    "\n",
    "\n",
    "  if zAsPercent :\n",
    "    ax.zaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "  ax.view_init(elev=10., azim=az)\n",
    "  #ax.set_title(Title, fontsize=24)\n",
    "  ax.set_facecolor('white')\n",
    "\n",
    "  plt.tick_params(labelsize=16)\n",
    "\n",
    "  \n",
    "  plt.show()\n",
    "\n",
    "\n",
    "  return\n",
    "\n",
    "#Plotting function from a pandas series\n",
    "def plot2Series(data, \n",
    "                data2,\n",
    "                Title = 'True Price Surface',\n",
    "                az=320,\n",
    "                yMin = KMin,\n",
    "                yMax = KMax, \n",
    "                zAsPercent = False):\n",
    "  \n",
    "\n",
    "  plot2GridCustom(data.index.to_frame().values, \n",
    "                  data.values,\n",
    "                  data2.index.to_frame().values, \n",
    "                  data2.values,\n",
    "                  xTitle = data.index.names[1],\n",
    "                  yTitle = data.index.names[0],\n",
    "                  zTitle = data.name,\n",
    "                  Title = Title, \n",
    "                  az=az, \n",
    "                  yMin = yMin, \n",
    "                  yMax = yMax, \n",
    "                  zAsPercent = zAsPercent)\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Za78IddfhsqS"
   },
   "outputs": [],
   "source": [
    "plt.get_cmap(\"plasma\")(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wUKTeMfH2IhD"
   },
   "outputs": [],
   "source": [
    "plotSurface(dataSet, \"Price\", Title = 'True Price Surface')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YmQpNeyis0iV"
   },
   "outputs": [],
   "source": [
    "inverseTransform(scaledDataSet, scaler).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "af5ddhHu6rhl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-zkCoxz7rvj"
   },
   "outputs": [],
   "source": [
    "def convertToLogMoneyness(formerSerie):\n",
    "  maturity = formerSerie.index.get_level_values(\"Maturity\")\n",
    "  logMoneyness = np.log(S0[0] / formerSerie.index.get_level_values(\"Strike\"))\n",
    "  newIndex = pd.MultiIndex.from_arrays([np.array(logMoneyness.values), np.array(maturity.values)], names=('LogMoneyness', 'Maturity'))\n",
    "  if type(formerSerie) == type(pd.Series()) :\n",
    "    return pd.Series(formerSerie.values , index=newIndex)\n",
    "  return pd.DataFrame(formerSerie.values, index = newIndex, columns= formerSerie.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mh8qUCd11Uyh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "REWz2Qinm4iZ"
   },
   "outputs": [],
   "source": [
    "#Plot predicted value, benchmark value, absoluate error and relative error\n",
    "#It also compute RMSE between predValue and refValue\n",
    "#predValue : approximated value \n",
    "#refValue : benchamrk value\n",
    "#quantityName : name for approximated quantity\n",
    "#az : azimuth i.e. angle of view for surface\n",
    "#yMin : minimum value for y axis\n",
    "#yMax : maximum value for y axis\n",
    "def predictionDiagnosis(predValue, \n",
    "                        refValue, \n",
    "                        quantityName, \n",
    "                        az=320,\n",
    "                        yMin = KMin,\n",
    "                        yMax = KMax):\n",
    "  \n",
    "  predValueFiltered = predValue[predValue.index.get_level_values(\"Maturity\") > 0.001]\n",
    "  refValueFiltered = refValue[refValue.index.get_level_values(\"Maturity\") > 0.001]\n",
    "  title = \"Predicted \" + quantityName + \" surface\"\n",
    "  plotSerie(predValueFiltered.rename(quantityName), \n",
    "            Title = title, \n",
    "            az=az,\n",
    "            yMin = yMin,\n",
    "            yMax = yMax)\n",
    "  \n",
    "  title = \"True \" + quantityName + \" surface\"\n",
    "  plotSerie(refValueFiltered.rename(quantityName), \n",
    "            Title = title, \n",
    "            az=az,\n",
    "            yMin = yMin,\n",
    "            yMax = yMax)\n",
    "  \n",
    "  title = quantityName + \" surface error\"\n",
    "  absoluteError = np.abs(predValueFiltered - refValueFiltered) \n",
    "  plotSerie(absoluteError.rename(quantityName + \" Absolute Error\"),\n",
    "            Title = title,\n",
    "            az=az,\n",
    "            yMin = yMin,\n",
    "            yMax = yMax)\n",
    "  \n",
    "  title = quantityName + \" surface error\"\n",
    "  relativeError = np.abs(predValueFiltered - refValueFiltered) / refValueFiltered\n",
    "  plotSerie(relativeError.rename(quantityName + \" Relative Error (%)\"),\n",
    "            Title = title,\n",
    "            az=az,\n",
    "            yMin = yMin,\n",
    "            yMax = yMax, \n",
    "            zAsPercent = True)\n",
    "  \n",
    "  print(\"RMSE : \", np.sqrt(np.mean(np.square(absoluteError))) )\n",
    "  \n",
    "  return\n",
    "\n",
    "#Diagnose Price, theta, gamma and local volatility\n",
    "def modelSummary(price, \n",
    "                 volLocale, \n",
    "                 delta_T, \n",
    "                 gamma_K, \n",
    "                 benchDataset,\n",
    "                 sigma=0.3, \n",
    "                 az=40,\n",
    "                 yMin = KMin,\n",
    "                 yMax = KMax,\n",
    "                 logMoneynessScale = False):\n",
    "  nbArbitrageViolations = ((delta_T<0) + (gamma_K<0)).sum()\n",
    "  print(\"Number of static arbitrage violations : \", nbArbitrageViolations)\n",
    "  if logMoneynessScale : \n",
    "    pricePred = convertToLogMoneyness(price)\n",
    "    volLocalePred = convertToLogMoneyness(volLocale)\n",
    "    delta_TPred = convertToLogMoneyness(delta_T)\n",
    "    gKRefPred = convertToLogMoneyness(gamma_K)\n",
    "    benchDatasetScaled = convertToLogMoneyness(benchDataset)\n",
    "    yMinScaled = np.log(S0[0]/yMax)\n",
    "    yMaxScaled = np.log(S0[0]/yMin)\n",
    "    azimutIncrement = 180\n",
    "  else : \n",
    "    pricePred = price\n",
    "    volLocalePred = volLocale\n",
    "    delta_TPred = delta_T\n",
    "    gKRefPred = gamma_K\n",
    "    benchDatasetScaled = benchDataset\n",
    "    yMinScaled = yMin\n",
    "    yMaxScaled = yMax\n",
    "    azimutIncrement = 0\n",
    "  \n",
    "  priceRef = benchDatasetScaled[\"Price\"]\n",
    "  predictionDiagnosis(pricePred, \n",
    "                      priceRef, \n",
    "                      \"Price\",\n",
    "                      az=320 + azimutIncrement,\n",
    "                      yMin = yMinScaled,\n",
    "                      yMax = yMaxScaled)\n",
    "  \n",
    "  \n",
    "  volLocaleRef = benchDatasetScaled[\"locvol\"]\n",
    "  predictionDiagnosis(volLocalePred, \n",
    "                      volLocaleRef, \n",
    "                      \"Local volatility\",\n",
    "                      az= az + azimutIncrement,\n",
    "                      yMin = yMinScaled,\n",
    "                      yMax = yMaxScaled)\n",
    "  \n",
    "  \n",
    "  dTRef = benchDatasetScaled[\"Theta\"]\n",
    "  predictionDiagnosis(delta_TPred, \n",
    "                      dTRef, \n",
    "                      \"Theta\",\n",
    "                      az=340 + azimutIncrement,\n",
    "                      yMin = yMinScaled,\n",
    "                      yMax = yMaxScaled)\n",
    "  \n",
    "  \n",
    "  gKRef = benchDatasetScaled[\"Gamma Strike\"]\n",
    "  predictionDiagnosis(gKRefPred, \n",
    "                      gKRef, \n",
    "                      \"Gamma Strike\",\n",
    "                      az=340 + azimutIncrement,\n",
    "                      yMin = yMinScaled,\n",
    "                      yMax = yMaxScaled)\n",
    "  return\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jQ8OwgHF1I-n"
   },
   "source": [
    "### Implied volatility function calibration by bissection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aL1OsFOh5h--"
   },
   "outputs": [],
   "source": [
    "def bs_price(cp, s, k, rf, t, v, div):\n",
    "        \"\"\" Price an option using the Black-Scholes model.\n",
    "        cp: +1/-1 for call/put\n",
    "        s: initial stock price\n",
    "        k: strike price\n",
    "        t: expiration time\n",
    "        v: volatility\n",
    "        rf: risk-free rate\n",
    "        div: dividend\n",
    "        \"\"\"\n",
    "    \n",
    "        d1 = (np.log(s/k)+(rf-div+0.5*v*v)*t)/(v*np.sqrt(t))\n",
    "        d2 = d1 - v*np.sqrt(t)\n",
    "\n",
    "        optprice = (cp*s*np.exp(-div*t)*st.norm.cdf(cp*d1)) - (cp*k*np.exp(-rf*t)*st.norm.cdf(cp*d2))\n",
    "        \n",
    "        return optprice\n",
    "\n",
    "def bissectionMethod(S_0, r, q, implied_vol0, maturity, Strike, refPrice, epsilon):\n",
    "    calibratedSigma = implied_vol0\n",
    "    #Call black-scholes price function for initial value\n",
    "    priceBS = bs_price(-1 ,S0, Strike, r, maturity, calibratedSigma, q)\n",
    "    sigmaUp = 2.0\n",
    "    sigmaInf = epsilon\n",
    "    lossSerie = []\n",
    "    \n",
    "    priceMax = bs_price(-1 ,S0, Strike, r, maturity, sigmaUp, q)\n",
    "    if priceMax < refPrice:\n",
    "        return priceMax, sigmaUp, pd.Series(lossSerie)\n",
    "    \n",
    "    priceMin = bs_price(-1 ,S0, Strike, r, maturity, sigmaInf, q)\n",
    "    if priceMin > refPrice:\n",
    "        return priceMin, sigmaInf, pd.Series(lossSerie) \n",
    "\n",
    "    #Stop the optimization when the error is less than epsilon\n",
    "    while(abs(priceBS - refPrice) > epsilon):\n",
    "        #Update the upper bound or the lower bound \n",
    "        #by comparing calibrated price and the target price \n",
    "        if priceBS < refPrice : \n",
    "            sigmaInf = calibratedSigma\n",
    "        else :\n",
    "            sigmaUp = calibratedSigma\n",
    "        #Update calibratedSigma\n",
    "        calibratedSigma = (sigmaUp + sigmaInf) / 2\n",
    "        #Update calibrated price\n",
    "        priceBS = bs_price(-1 ,S0, Strike, r, maturity, calibratedSigma, q)\n",
    "        #Record the calibration error for this step\n",
    "        lossSerie.append(abs(priceBS - refPrice)) \n",
    "        \n",
    "    return priceBS, calibratedSigma, pd.Series(lossSerie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qGu1Iz5I1DoF"
   },
   "outputs": [],
   "source": [
    "#Execute calibration of implied volatility from estimated price and benchmark price\n",
    "#Then plot esitmated implied vol, absolute and relative error\n",
    "def plotImpliedVol(priceSurface, \n",
    "                   refImpliedVol, \n",
    "                   rIntegralSpline = None, \n",
    "                   qIntegralSpline = None, \n",
    "                   az=40,\n",
    "                   yMin = KMin,\n",
    "                   yMax = KMax,\n",
    "                   relativeErrorVolMax = 1000,\n",
    "                   logMoneynessScale = False):\n",
    "    return plotImpliedVolConcrete(priceSurface[priceSurface.index.get_level_values(\"Maturity\") > 0.001],\n",
    "                                  refImpliedVol[refImpliedVol.index.get_level_values(\"Maturity\") > 0.001],\n",
    "                                  rIntegralSpline = rIntegralSpline,\n",
    "                                  qIntegralSpline = qIntegralSpline,\n",
    "                                  az=az,\n",
    "                                  yMin = yMin,\n",
    "                                  yMax = yMax,\n",
    "                                  relativeErrorVolMax = relativeErrorVolMax, \n",
    "                                  logMoneynessScale = logMoneynessScale)\n",
    "\n",
    "def plotImpliedVolConcrete(priceSurface,\n",
    "                           refImpliedVol,\n",
    "                           rIntegralSpline = None,\n",
    "                           qIntegralSpline = None,\n",
    "                           az=40,\n",
    "                           yMin = KMin,\n",
    "                           yMax = KMax,\n",
    "                           relativeErrorVolMax = 10,\n",
    "                           logMoneynessScale = False):\n",
    "    priceSurfaceScaled = convertToLogMoneyness(priceSurface) if logMoneynessScale else priceSurface\n",
    "    refImpliedVolScaled = convertToLogMoneyness(refImpliedVol) if logMoneynessScale else refImpliedVol\n",
    "    df = priceSurfaceScaled.index.to_frame()\n",
    "    df[\"Price\"] = priceSurfaceScaled\n",
    "    df[\"Strike\"] = convertToLogMoneyness(priceSurface.index.to_frame()[\"Strike\"]) if logMoneynessScale else priceSurface.index.to_frame()[\"Strike\"]\n",
    "    scaledYMin = np.log(S0[0]/yMax) if logMoneynessScale else yMin\n",
    "    scaledYMax = np.log(S0[0]/yMin) if logMoneynessScale else yMax\n",
    "    azimutIncrement = 180 if logMoneynessScale else 0\n",
    "\n",
    "\n",
    "    epsilon = 1e-9\n",
    "    calibrationFunction = lambda x : bissectionMethod(S0, \n",
    "                                                      rIntegralSpline(x[\"Maturity\"])/x[\"Maturity\"] if (rIntegralSpline is not None) else r, \n",
    "                                                      qIntegralSpline(x[\"Maturity\"])/x[\"Maturity\"] if (qIntegralSpline is not None) else q, \n",
    "                                                      0.2, \n",
    "                                                      x[\"Maturity\"], \n",
    "                                                      x[\"Strike\"], \n",
    "                                                      x[\"Price\"], \n",
    "                                                      epsilon)[1]\n",
    "\n",
    "    impliedVol = df.apply(calibrationFunction, axis = 1).rename(\"Implied Volatility\")\n",
    "    impliedVolError = np.abs(impliedVol-refImpliedVolScaled).rename('Absolute Error')\n",
    "    relativeImpliedVolError = (impliedVolError / refImpliedVolScaled).rename(\"Relative error (%)\")\n",
    "    \n",
    "    plotSerie(impliedVol, \n",
    "              Title = 'Implied volatility surface', \n",
    "              az=az + azimutIncrement,\n",
    "              yMin = scaledYMin,\n",
    "              yMax = scaledYMax)\n",
    "\n",
    "    plotSerie(impliedVolError, \n",
    "              Title = 'Implied volatility error', \n",
    "              az=az + azimutIncrement,\n",
    "              yMin = scaledYMin,\n",
    "              yMax = scaledYMax)\n",
    "    \n",
    "    plotSerie(relativeImpliedVolError.clip(0,relativeErrorVolMax / 100.0), \n",
    "              Title = 'Implied volatility relative error', \n",
    "              az=az + azimutIncrement,\n",
    "              yMin = scaledYMin,\n",
    "              yMax = scaledYMax,\n",
    "              zAsPercent = True)\n",
    "  \n",
    "    print(\"Implied volalitity RMSE : \", np.sqrt(np.mean(np.square(impliedVolError))) )\n",
    "\n",
    "    return impliedVol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0m2TzPY7TL_m"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jl7xMrJRRiVk"
   },
   "outputs": [],
   "source": [
    "plotSerie(localVolatility[\"LocalVolatility\"],\n",
    "          Title = 'Local Volatility Surface',\n",
    "          az=30,\n",
    "          yMin=0.7*S0,\n",
    "          yMax=1.4*S0, zAsPercent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OJ6EELvlQJJI"
   },
   "outputs": [],
   "source": [
    "plotSerie(dataSet[\"locvol\"],\n",
    "          Title = 'Local Volatility Surface',\n",
    "          az=30,\n",
    "          yMin=0.7*S0,\n",
    "          yMax=1.4*S0, zAsPercent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MhLrX-8Y1DeM"
   },
   "outputs": [],
   "source": [
    "dataSet[\"locvol\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UpF5ELRnoS73"
   },
   "source": [
    "## Learning Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rwa7gLppaEdi"
   },
   "outputs": [],
   "source": [
    "#Import tensorflow for 1.x version \n",
    "from keras.layers import Dense, Input\n",
    "from keras import Model\n",
    "import keras.backend as K\n",
    "import keras.activations as Act\n",
    "from functools import partial\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vGkkvShljABd"
   },
   "outputs": [],
   "source": [
    "#Deactivate warning messages\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KqtRsQxRZ9bT"
   },
   "outputs": [],
   "source": [
    "hyperparameters = {}\n",
    "#penalization coefficient\n",
    "hyperparameters[\"lambdaLocVol\"] = 100\n",
    "hyperparameters[\"lambdaSoft\"] = 100 \n",
    "hyperparameters[\"lambdaGamma\"] = 10000\n",
    "\n",
    "#Derivative soft constraints parameters\n",
    "hyperparameters[\"lowerBoundTheta\"] = 0.01\n",
    "hyperparameters[\"lowerBoundGamma\"] = 0.00001\n",
    "\n",
    "#Local variance parameters\n",
    "hyperparameters[\"DupireVarCap\"] = 10\n",
    "hyperparameters[\"DupireVolLowerBound\"] = 0.05\n",
    "hyperparameters[\"DupireVolUpperBound\"] = 0.40\n",
    "\n",
    "#Learning scheduler coefficient\n",
    "hyperparameters[\"LearningRateStart\"] = 0.1\n",
    "hyperparameters[\"Patience\"] = 100\n",
    "hyperparameters[\"batchSize\"] = 50\n",
    "hyperparameters[\"FinalLearningRate\"] = 1e-6\n",
    "hyperparameters[\"FixedLearningRate\"] = False\n",
    "\n",
    "#Training parameters\n",
    "hyperparameters[\"nbUnits\"] = 200 #number of units for hidden layers\n",
    "hyperparameters[\"maxEpoch\"] = 10000 #maximum number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vNDkFJlYdi7h"
   },
   "source": [
    "### Learning scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4ZrujaHeD0L"
   },
   "outputs": [],
   "source": [
    "#Format result from training step\n",
    "def evalAndFormatResult(price, loss, dataSet):\n",
    "\n",
    "    scaledPredPrice = pd.Series(price.flatten(), index = dataSet.index).rename(\"Price\")\n",
    "    predPrice = inverseTransformColumn(dataSet, scaler, scaledPredPrice)\n",
    "    \n",
    "    return predPrice, pd.Series(loss)\n",
    "\n",
    "#Format result from training step when local volatility is computed\n",
    "def evalAndFormatDupireResult(price, volDupire, theta, gamma, dupireVar, loss, dataSet):\n",
    "    predPrice, lossEpoch = evalAndFormatResult(price, loss, dataSet)\n",
    "\n",
    "    predDupire = pd.Series(volDupire.flatten(), index = dataSet.index).rename(\"Dupire\")\n",
    "    \n",
    "    scaledTheta = pd.Series(theta.flatten(), index = dataSet.index).rename(\"Theta\")\n",
    "    predTheta = inverseTransformColumnGreeks(dataSet, scaler, scaledTheta, \n",
    "                                             \"Price\", \"Maturity\")\n",
    "    \n",
    "    scaledGammaK = pd.Series(gamma.flatten(), index = dataSet.index).rename(\"GammaK\")\n",
    "    predGammaK = inverseTransformColumnGreeks(dataSet, scaler, scaledGammaK, \n",
    "                                              \"Price\", \"ChangedStrike\", order = 2)\n",
    "    \n",
    "    return predPrice, predDupire, predTheta, predGammaK, lossEpoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B1iXQCMZRIGV"
   },
   "outputs": [],
   "source": [
    "#Penalization for pseudo local volatility\n",
    "def intervalRegularization(localVariance, vegaRef, hyperParameters):\n",
    "  lowerVolBound = hyperParameters[\"DupireVolLowerBound\"]\n",
    "  upperVolBound = hyperParameters[\"DupireVolUpperBound\"]\n",
    "  no_nans = tf.clip_by_value(localVariance, 0, hyperParameters[\"DupireVarCap\"])\n",
    "  reg = tf.nn.relu(tf.square(lowerVolBound) - no_nans) + tf.nn.relu(no_nans - tf.square(upperVolBound))\n",
    "  lambdas = hyperParameters[\"lambdaLocVol\"] / tf.reduce_mean(vegaRef)\n",
    "  return lambdas * tf.reduce_mean(tf.boolean_mask(reg, tf.is_finite(reg)))\n",
    "\n",
    "#Add above regularization to the list of penalization\n",
    "def addDupireRegularisation(priceTensor, tensorList, penalizationList, formattingResultFunction, vegaRef, hyperParameters):\n",
    "    updatedPenalizationList = penalizationList + [intervalRegularization(tensorList[-1], vegaRef, hyperParameters)]\n",
    "    return priceTensor, tensorList, updatedPenalizationList, formattingResultFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZUe6Ixrf8Nk-"
   },
   "outputs": [],
   "source": [
    "#Mini-batch sampling methods for large datasets\n",
    "def selectMiniBatchWithoutReplacement(dataSet, batch_size):\n",
    "    nbObs = dataSet.shape[0]\n",
    "    idx = np.arange(nbObs) \n",
    "    np.random.shuffle(idx) \n",
    "    nbBatches = int(np.ceil(nbObs/batch_size))\n",
    "    xBatchList = []\n",
    "    lastBatchIndex = 0\n",
    "    for i in range(nbBatches):\n",
    "        firstBatchIndex = i*batch_size\n",
    "        lastBatchIndex = (i+1)*batch_size\n",
    "        xBatchList.append(dataSet.iloc[idx[firstBatchIndex:lastBatchIndex],:])\n",
    "    xBatchList.append(dataSet.iloc[idx[lastBatchIndex:],:])\n",
    "    return xBatchList\n",
    "\n",
    "def selectMiniBatchWithReplacement(dataSet, batch_size):\n",
    "    nbObs = dataSet.shape[0] \n",
    "    nbBatches = int(np.ceil(nbObs/batch_size)) + 1\n",
    "    xBatchList = []\n",
    "    lastBatchIndex = 0\n",
    "    for i in range(nbBatches):\n",
    "        idx = np.random.randint(nbObs, size = batch_size)\n",
    "        xBatchList.append(dataSet.iloc[idx,:])\n",
    "    return xBatchList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y4oKCKdCL2fI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZXP0y5kveFPg"
   },
   "outputs": [],
   "source": [
    "#Train neural network with a decreasing rule for learning rate\n",
    "#NNFactory :  function creating the architecture\n",
    "#dataSet : training data\n",
    "#activateRegularization : boolean, if true add bound penalization to dupire variance\n",
    "#hyperparameters : dictionnary containing various hyperparameters\n",
    "#modelName : name under which tensorflow model is saved\n",
    "def create_train_model(NNFactory, \n",
    "                       dataSet, \n",
    "                       activateRegularization, \n",
    "                       hyperparameters,\n",
    "                       modelName = \"bestModel\"):\n",
    "    hidden_nodes = hyperparameters[\"nbUnits\"] \n",
    "    nbEpoch = hyperparameters[\"maxEpoch\"] \n",
    "    fixedLearningRate = (None if hyperparameters[\"FixedLearningRate\"] else hyperparameters[\"LearningRateStart\"])\n",
    "    patience = hyperparameters[\"Patience\"]\n",
    "    \n",
    "    # Go through num_iters iterations (ignoring mini-batching)\n",
    "    activateLearningDecrease = (~ hyperparameters[\"FixedLearningRate\"])\n",
    "    learningRate = hyperparameters[\"LearningRateStart\"]\n",
    "    learningRateEpoch = 0\n",
    "    finalLearningRate = hyperparameters[\"FinalLearningRate\"]\n",
    "\n",
    "    batch_size = hyperparameters[\"batchSize\"]\n",
    "\n",
    "    start = time.time()\n",
    "    # Reset the graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Placeholders for input and output data   \n",
    "    Strike = tf.placeholder(tf.float32,[None,1])\n",
    "    Maturity = tf.placeholder(tf.float32,[None,1])\n",
    "    factorPrice = tf.placeholder(tf.float32,[None,1])\n",
    "    y = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='y')\n",
    "    vegaRef = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='vegaRef')\n",
    "    learningRateTensor = tf.placeholder(tf.float32,[])\n",
    "    \n",
    "    #Get scaling for strike\n",
    "    colStrikeIndex = dataSet.columns.get_loc(\"ChangedStrike\")\n",
    "    maxColFunction = scaler.data_max_[colStrikeIndex]\n",
    "    minColFunction = scaler.data_min_[colStrikeIndex]\n",
    "    scF = (maxColFunction - minColFunction) \n",
    "    scaleTensor = tf.constant(scF, dtype=tf.float32)\n",
    "    strikeMinTensor = tf.constant(minColFunction, dtype=tf.float32)\n",
    "\n",
    "    price_pred_tensor = None\n",
    "    TensorList = None\n",
    "    penalizationList = None \n",
    "    formattingFunction = None\n",
    "    if activateRegularization : #Add pseudo local volatility regularisation\n",
    "        price_pred_tensor, TensorList, penalizationList, formattingFunction = addDupireRegularisation( *NNFactory(hidden_nodes,\n",
    "                                                                                                                  Strike,\n",
    "                                                                                                                  Maturity, \n",
    "                                                                                                                  scaleTensor, \n",
    "                                                                                                                  strikeMinTensor, \n",
    "                                                                                                                  vegaRef, \n",
    "                                                                                                                  hyperparameters) ,\n",
    "                                                                                                      vegaRef, \n",
    "                                                                                                      hyperparameters)\n",
    "    else :\n",
    "        price_pred_tensor, TensorList, penalizationList, formattingFunction = NNFactory(hidden_nodes,\n",
    "                                                                                        Strike, \n",
    "                                                                                        Maturity, \n",
    "                                                                                        scaleTensor, \n",
    "                                                                                        strikeMinTensor, \n",
    "                                                                                        vegaRef, \n",
    "                                                                                        hyperparameters)\n",
    "\n",
    "    price_pred_tensor_sc= tf.multiply( factorPrice, price_pred_tensor)\n",
    "    TensorList[0] = price_pred_tensor_sc\n",
    "    \n",
    "    # Define a loss function\n",
    "    pointwiseError = tf.reduce_mean(tf.abs(price_pred_tensor_sc - y) / vegaRef)\n",
    "    errors = tf.add_n([pointwiseError] + penalizationList) \n",
    "    loss = tf.log(tf.reduce_mean(errors))\n",
    "\n",
    "\n",
    "\n",
    "    # Define a train operation to minimize the loss\n",
    "    lr = learningRate\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRateTensor)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    # Initialize variables and run session\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    n = dataSet.shape[0]\n",
    "    scaledInput = transformCustomMinMax(dataSet, scaler)\n",
    "\n",
    "    \n",
    "    maturity = dataSet[\"Maturity\"].values.reshape(n,1)\n",
    "    loss_serie = []\n",
    "\n",
    "    def createFeedDict(batch):\n",
    "        batchSize = batch.shape[0]\n",
    "        feedDict = {Strike : scaledInput[\"ChangedStrike\"].loc[batch.index].values.reshape(batchSize,1),\n",
    "                    Maturity : batch[\"Maturity\"].values.reshape(batchSize,1), \n",
    "                    y : batch[\"Price\"].values.reshape(batchSize,1),\n",
    "                    factorPrice : batch[\"DividendFactor\"].values.reshape(batchSize,1), \n",
    "                    learningRateTensor : learningRate,\n",
    "                    vegaRef : np.ones_like(batch[\"VegaRef\"].values.reshape(batchSize,1))}\n",
    "        return feedDict\n",
    "\n",
    "    #Learning rate is divided by 10 if no imporvement is observed for training loss after \"patience\" epochs\n",
    "    def updateLearningRate(iterNumber, lr, lrEpoch):\n",
    "        if not activateLearningDecrease :\n",
    "            print(\"Constant learning rate, stop training\")\n",
    "            return False, lr, lrEpoch\n",
    "        if learningRate > finalLearningRate :\n",
    "            lr *= 0.1\n",
    "            lrEpoch = iterNumber\n",
    "            saver.restore(sess, modelName)\n",
    "            print(\"Iteration : \", lrEpoch, \"new learning rate : \", lr)\n",
    "        else :\n",
    "          print(\"Last Iteration : \", lrEpoch, \"final learning rate : \", lr)\n",
    "          return False, lr, lrEpoch\n",
    "        return True, lr, lrEpoch\n",
    "    \n",
    "    epochFeedDict = createFeedDict(dataSet)\n",
    "\n",
    "    def evalBestModel():\n",
    "        if not activateLearningDecrease :\n",
    "            print(\"Learning rate : \", learningRate, \" final loss : \", min(loss_serie))\n",
    "        currentBestLoss = sess.run(loss, feed_dict=epochFeedDict)\n",
    "        currentBestPenalizations = sess.run([pointwiseError, penalizationList], feed_dict=epochFeedDict)\n",
    "        print(\"Best loss (hidden nodes: %d, iterations: %d): %.2f\" % (hidden_nodes, len(loss_serie), currentBestLoss))\n",
    "        print(\"Best Penalization : \", currentBestPenalizations)\n",
    "        return\n",
    "    \n",
    "    for i in range(nbEpoch):\n",
    "        miniBatchList = [dataSet]\n",
    "        penalizationResult = sess.run(penalizationList, feed_dict=epochFeedDict)\n",
    "        lossResult = sess.run(pointwiseError, feed_dict=epochFeedDict)\n",
    "\n",
    "        #miniBatchList = selectMiniBatchWithoutReplacement(dataSet, batch_size)\n",
    "        for k in range(len(miniBatchList)) :\n",
    "            batchFeedDict = createFeedDict(miniBatchList[k])\n",
    "            sess.run(train, feed_dict=batchFeedDict)\n",
    "        \n",
    "        \n",
    "        loss_serie.append(sess.run(loss, feed_dict=epochFeedDict))\n",
    "\n",
    "        if (len(loss_serie) < 2) or (loss_serie[-1] <= min(loss_serie)):\n",
    "          #Save model as model is improved\n",
    "          saver.save(sess, modelName)\n",
    "        if (np.isnan(loss_serie[-1]) or  #Unstable model\n",
    "            ( (i-learningRateEpoch >= patience) and (min(loss_serie[-patience:]) > min(loss_serie)) ) ) : #No improvement for training loss during the latest 100 iterations\n",
    "          continueTraining, learningRate, learningRateEpoch = updateLearningRate(i, learningRate, learningRateEpoch)\n",
    "          if continueTraining :\n",
    "            evalBestModel()\n",
    "          else :\n",
    "            break\n",
    "    saver.restore(sess, modelName)  \n",
    "    \n",
    "    evalBestModel()\n",
    "\n",
    "    evalList  = sess.run(TensorList, feed_dict=epochFeedDict)\n",
    "    \n",
    "    sess.close()\n",
    "    end = time.time()\n",
    "    print(\"Training Time : \", end - start)\n",
    "    \n",
    "    return formattingFunction(*evalList, loss_serie, dataSet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vn3rCCVOeCyX"
   },
   "outputs": [],
   "source": [
    "#Evaluate neural network without training, it restores parameters obtained from a pretrained model \n",
    "#NNFactory :  function creating the neural architecture\n",
    "#dataSet : dataset on which neural network is evaluated \n",
    "#activateRegularization : boolean, if true add bound penalization for dupire variance\n",
    "#hyperparameters : dictionnary containing various hyperparameters\n",
    "#modelName : name of tensorflow model to restore\n",
    "def create_eval_model(NNFactory, \n",
    "                      dataSet, \n",
    "                      activateRegularization, \n",
    "                      hyperparameters,\n",
    "                      modelName = \"bestModel\"):\n",
    "    hidden_nodes = hyperparameters[\"nbUnits\"] \n",
    "    \n",
    "    # Go through num_iters iterations (ignoring mini-batching)\n",
    "    activateLearningDecrease = (~ hyperparameters[\"FixedLearningRate\"])\n",
    "    learningRate = hyperparameters[\"LearningRateStart\"]\n",
    "\n",
    "    # Reset the graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Placeholders for input and output data   \n",
    "    Strike = tf.placeholder(tf.float32,[None,1])\n",
    "    Maturity = tf.placeholder(tf.float32,[None,1])\n",
    "    factorPrice = tf.placeholder(tf.float32,[None,1])\n",
    "    y = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='y')\n",
    "    vegaRef = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='vegaRef')\n",
    "    learningRateTensor = tf.placeholder(tf.float32,[])\n",
    "    \n",
    "    #Get scaling for strike\n",
    "    colStrikeIndex = dataSet.columns.get_loc(\"ChangedStrike\")\n",
    "    maxColFunction = scaler.data_max_[colStrikeIndex]\n",
    "    minColFunction = scaler.data_min_[colStrikeIndex]\n",
    "    scF = (maxColFunction - minColFunction) \n",
    "    scaleTensor = tf.constant(scF, dtype=tf.float32)\n",
    "    strikeMinTensor = tf.constant(minColFunction, dtype=tf.float32)\n",
    "\n",
    "    price_pred_tensor = None\n",
    "    TensorList = None\n",
    "    penalizationList = None \n",
    "    formattingFunction = None\n",
    "    if activateRegularization : \n",
    "        price_pred_tensor, TensorList, penalizationList, formattingFunction = addDupireRegularisation( *NNFactory(hidden_nodes,\n",
    "                                                                                                                  Strike,\n",
    "                                                                                                                  Maturity, \n",
    "                                                                                                                  scaleTensor, \n",
    "                                                                                                                  strikeMinTensor, \n",
    "                                                                                                                  vegaRef,\n",
    "                                                                                                                  hyperparameters,\n",
    "                                                                                                                  IsTraining=False), \n",
    "                                                                                                      vegaRef,\n",
    "                                                                                                      hyperparameters )\n",
    "    else :\n",
    "        price_pred_tensor, TensorList, penalizationList, formattingFunction = NNFactory(hidden_nodes,\n",
    "                                                                                        Strike, \n",
    "                                                                                        Maturity, \n",
    "                                                                                        scaleTensor, \n",
    "                                                                                        strikeMinTensor,\n",
    "                                                                                        vegaRef,\n",
    "                                                                                        hyperparameters,\n",
    "                                                                                        IsTraining=False)\n",
    "\n",
    "    price_pred_tensor_sc= tf.multiply(factorPrice,price_pred_tensor)\n",
    "    TensorList[0] = price_pred_tensor_sc\n",
    "    \n",
    "    # Define a loss function\n",
    "    pointwiseError = tf.reduce_mean(tf.abs(price_pred_tensor_sc - y) / vegaRef)\n",
    "    errors = tf.add_n([pointwiseError] + penalizationList)\n",
    "    loss = tf.log(tf.reduce_mean(errors))\n",
    "\n",
    "\n",
    "    # Define a train operation to minimize the loss\n",
    "    lr = learningRate \n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRateTensor)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    # Initialize variables and run session\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    n = dataSet.shape[0]\n",
    "    scaledInput = transformCustomMinMax(dataSet, scaler)\n",
    "\n",
    "    \n",
    "    maturity = dataSet[\"Maturity\"].values.reshape(n,1)\n",
    "    loss_serie = []\n",
    "\n",
    "    def createFeedDict(batch):\n",
    "        batchSize = batch.shape[0]\n",
    "        feedDict = {Strike : scaledInput[\"ChangedStrike\"].loc[batch.index].values.reshape(batchSize,1),\n",
    "                    Maturity : batch[\"Maturity\"].values.reshape(batchSize,1), \n",
    "                    y : batch[\"Price\"].values.reshape(batchSize,1),\n",
    "                    factorPrice : batch[\"DividendFactor\"].values.reshape(batchSize,1), \n",
    "                    learningRateTensor : learningRate,\n",
    "                    vegaRef : np.ones_like(batch[\"VegaRef\"].values.reshape(batchSize,1))}\n",
    "        return feedDict\n",
    "    \n",
    "    epochFeedDict = createFeedDict(dataSet)\n",
    "\n",
    "    def evalBestModel():\n",
    "        if not activateLearningDecrease :\n",
    "            print(\"Learning rate : \", learningRate, \" final loss : \", min(loss_serie))\n",
    "        currentBestLoss = sess.run(loss, feed_dict=epochFeedDict)\n",
    "        currentBestPenalizations = sess.run([pointwiseError, penalizationList], feed_dict=epochFeedDict)\n",
    "        print(\"Best loss (hidden nodes: %d, iterations: %d): %.2f\" % (hidden_nodes, len(loss_serie), currentBestLoss))\n",
    "        print(\"Best Penalization : \", currentBestPenalizations)\n",
    "        return\n",
    "    \n",
    "    saver.restore(sess, modelName)  \n",
    "    \n",
    "    evalBestModel()\n",
    "\n",
    "    evalList  = sess.run(TensorList, feed_dict=epochFeedDict)\n",
    "    \n",
    "    sess.close()\n",
    "    \n",
    "    return formattingFunction(*evalList, [0], dataSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-qgxVPguB_6P"
   },
   "source": [
    "### Convex architecture (Price only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JKjTFU6ioS76"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Soft constraints for strike convexity and strike/maturity monotonicity  \n",
    "def arbitragePenalties(priceTensor, strikeTensor, maturityTensor, scaleTensor, vegaRef, hyperparameters):\n",
    "\n",
    "    dK = tf.gradients(priceTensor, strikeTensor, name=\"dK\")\n",
    "    hK = tf.gradients(dK[0], strikeTensor, name=\"hK\") / tf.square(scaleTensor)\n",
    "    theta = tf.gradients(priceTensor,maturityTensor,name=\"dT\")\n",
    "    \n",
    "    lambdas = hyperparameters[\"lambdaSoft\"]  / tf.reduce_mean(vegaRef) \n",
    "    lowerBoundTheta = tf.constant(hyperparameters[\"lowerBoundTheta\"])\n",
    "    lowerBoundGamma = tf.constant(hyperparameters[\"lowerBoundGamma\"])\n",
    "    grad_penalty = lambdas * tf.reduce_mean(tf.nn.relu(-theta[0] + lowerBoundTheta ))\n",
    "    hessian_penalty = lambdas * hyperparameters[\"lowerBoundGamma\"] * tf.reduce_mean(tf.nn.relu(-hK[0] + lowerBoundGamma ))\n",
    "    \n",
    "    return [grad_penalty, hessian_penalty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "njypUjlPLFeK"
   },
   "outputs": [],
   "source": [
    "#Tools function for Neural network architecture\n",
    "\n",
    "#Initilize weights as positive\n",
    "def positiveKernelInitializer(shape, \n",
    "                              dtype=None, \n",
    "                              partition_info=None):\n",
    "  return tf.abs(tf.keras.initializers.normal()(shape,dtype=dtype, partition_info=partition_info))\n",
    "\n",
    "#Soft convex layer\n",
    "def convexLayer(n_units, tensor, isTraining, name, isNonDecreasing = True):\n",
    "  with tf.name_scope(name):\n",
    "    layer = tf.layers.dense(tensor if isNonDecreasing else (- tensor), \n",
    "                            units=n_units,\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_normal())\n",
    "    \n",
    "    \n",
    "    return tf.nn.softplus(layer)\n",
    "\n",
    "#Soft monotonic layer\n",
    "def monotonicLayer(n_units,  tensor, isTraining, name):\n",
    "  with tf.name_scope(name):\n",
    "    layer = tf.layers.dense(tensor, \n",
    "                            units=n_units,\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_normal())\n",
    "    \n",
    "    \n",
    "    \n",
    "    return tf.nn.sigmoid(layer)\n",
    "\n",
    "#Soft convex layer followed by output layer for regression \n",
    "def convexOutputLayer(n_units, tensor, isTraining, name, isNonDecreasing = True):\n",
    "  with tf.name_scope(name):\n",
    "    layer = tf.layers.dense(tensor if isNonDecreasing else (- tensor), \n",
    "                            units=2*n_units,\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_normal(),\n",
    "                            activation = 'softplus')\n",
    "    \n",
    "     \n",
    "    layer = tf.layers.dense(layer, \n",
    "                            units=1,\n",
    "                            kernel_initializer=positiveKernelInitializer,\n",
    "                            activation = 'softplus')\n",
    "    \n",
    "    return layer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Neural network factory for Hybrid approach : splitted network with soft contraints\n",
    "def NNArchitectureConstrained(n_units, \n",
    "                              strikeTensor,\n",
    "                              maturityTensor, \n",
    "                              scaleTensor, \n",
    "                              strikeMinTensor, \n",
    "                              vegaRef, \n",
    "                              hyperparameters,\n",
    "                              IsTraining=True):\n",
    "  #First splitted layer\n",
    "  hidden1S = convexLayer(n_units = n_units,\n",
    "                         tensor = strikeTensor,\n",
    "                         isTraining=IsTraining, \n",
    "                         name = \"Hidden1S\")\n",
    "  \n",
    "  hidden1M = monotonicLayer(n_units = n_units,\n",
    "                            tensor = maturityTensor, \n",
    "                            isTraining = IsTraining, \n",
    "                            name = \"Hidden1M\")\n",
    "  \n",
    "  hidden1 = tf.concat([hidden1S, hidden1M], axis=-1)\n",
    "  \n",
    "  #Second and output layer\n",
    "  out = convexOutputLayer(n_units = n_units,\n",
    "                          tensor = hidden1,\n",
    "                          isTraining = IsTraining,\n",
    "                          name = \"Output\")\n",
    "  #Soft constraints\n",
    "  penaltyList = arbitragePenalties(out, strikeTensor, \n",
    "                                   maturityTensor, \n",
    "                                   scaleTensor, \n",
    "                                   vegaRef, \n",
    "                                   hyperparameters)\n",
    "  \n",
    "  return out, [out], penaltyList, evalAndFormatResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6i9lEOpGoS79"
   },
   "outputs": [],
   "source": [
    "plt.plot(dataSet.index.get_level_values(\"Strike\"), dataSet[\"Price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uT7gq2LRSXTI"
   },
   "outputs": [],
   "source": [
    "y_pred0, lossSerie0 = create_train_model(NNArchitectureConstrained, scaledDataSet, False, hyperparameters, modelName = \"softConvexHybridModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-fv_bsyPSXTK"
   },
   "outputs": [],
   "source": [
    "print(\"Minimum error : \",lossSerie0.min())\n",
    "plotEpochLoss(lossSerie0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kwi7u3dELnws"
   },
   "outputs": [],
   "source": [
    "lossSerie0.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MUEXO111oS8G"
   },
   "outputs": [],
   "source": [
    "y_pred0, lossSerie0 = create_eval_model(NNArchitectureConstrained, \n",
    "                                        scaledDataSet, \n",
    "                                        False, \n",
    "                                        hyperparameters, \n",
    "                                        modelName = \"softConvexHybridModel\")\n",
    "predictionDiagnosis(y_pred0, dataSet[\"Price\"], \" Price \")\n",
    "impV0 = plotImpliedVol(y_pred0, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L-mNSLXHhRfm"
   },
   "outputs": [],
   "source": [
    "y_pred0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2STQwalmhoyR"
   },
   "outputs": [],
   "source": [
    "y_pred0.loc[(midS0,slice(None))].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V9tKlu5Ji1YF"
   },
   "outputs": [],
   "source": [
    "dataSet[\"Price\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oRI-8R9ezLLa"
   },
   "outputs": [],
   "source": [
    "y_pred0Test, lossSerie0Test = create_eval_model(NNArchitectureConstrained, \n",
    "                                                scaledDataSetTest, \n",
    "                                                False, \n",
    "                                                hyperparameters, \n",
    "                                                modelName = \"softConvexHybridModel\")\n",
    "predictionDiagnosis(y_pred0Test, dataSetTest[\"Price\"], \" Price \")\n",
    "impV0Test = plotImpliedVol(y_pred0Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nes-RItOzLIM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bQT4PIYWzK81"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KWdzg9DEv7eH"
   },
   "source": [
    "### Unconstrained neural network (Price only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HnG0q3QisrMt"
   },
   "outputs": [],
   "source": [
    "#Unconstrained dense layer\n",
    "def unconstrainedLayer(n_units,  tensor, isTraining, name, activation = K.softplus):\n",
    "  with tf.name_scope(name):\n",
    "    layer = tf.layers.dense(tensor, \n",
    "                            units=n_units,\n",
    "                            activation=activation,  \n",
    "                            kernel_initializer=tf.keras.initializers.he_normal())\n",
    "    return layer\n",
    "\n",
    "#Factory for unconstrained network\n",
    "def NNArchitectureUnconstrained(n_units, \n",
    "                                strikeTensor,\n",
    "                                maturityTensor, \n",
    "                                scaleTensor, \n",
    "                                strikeMinTensor, \n",
    "                                vegaRef,\n",
    "                                hyperparameters,\n",
    "                                IsTraining=True):\n",
    "  \n",
    "  inputLayer = tf.concat([strikeTensor,maturityTensor], axis=-1)\n",
    "  \n",
    "  #First layer\n",
    "  hidden1 = unconstrainedLayer(n_units = n_units,\n",
    "                               tensor = inputLayer,\n",
    "                               isTraining=IsTraining, \n",
    "                               name = \"Hidden1\")\n",
    "  \n",
    "  #Second layer\n",
    "  hidden2 = unconstrainedLayer(n_units = n_units,\n",
    "                               tensor = hidden1,\n",
    "                               isTraining=IsTraining, \n",
    "                               name = \"Hidden2\")\n",
    "  #Output layer \n",
    "  out = unconstrainedLayer(n_units = 1,\n",
    "                           tensor = hidden2,\n",
    "                           isTraining=IsTraining, \n",
    "                           name = \"Output\",\n",
    "                           activation = None)\n",
    "  \n",
    "  return out, [out], [], evalAndFormatResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_YZGyKcpsrMv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WdWudyC2srMw"
   },
   "outputs": [],
   "source": [
    "y_pred1, lossSerie1 = create_train_model(NNArchitectureUnconstrained, \n",
    "                                         scaledDataSet, \n",
    "                                         False, \n",
    "                                         hyperparameters,\n",
    "                                         modelName = \"unconstrainedModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AzjHIzRUsrMx"
   },
   "outputs": [],
   "source": [
    "print(\"Minimum error : \",lossSerie1.min())\n",
    "plotEpochLoss(lossSerie1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TArgJP5fsrMy"
   },
   "outputs": [],
   "source": [
    "lossSerie1.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IzlURNdisrM0"
   },
   "outputs": [],
   "source": [
    "y_pred1, lossSerie1 = create_eval_model(NNArchitectureUnconstrained, \n",
    "                                        scaledDataSet, \n",
    "                                        False, \n",
    "                                        hyperparameters,\n",
    "                                        modelName = \"unconstrainedModel\")\n",
    "predictionDiagnosis(y_pred1, dataSet[\"Price\"], \" Price \")\n",
    "impV1 = plotImpliedVol(y_pred1, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J5fP6NJ4srM1"
   },
   "outputs": [],
   "source": [
    "y_pred1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_pMZlMbdsrM5"
   },
   "outputs": [],
   "source": [
    "y_pred1.loc[(midS0,slice(None))].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lU1LIqnPsrM9"
   },
   "outputs": [],
   "source": [
    "dataSet[\"Price\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bPrMgqTtsrNB"
   },
   "outputs": [],
   "source": [
    "y_pred1Test, lossSerie1Test = create_eval_model(NNArchitectureUnconstrained, \n",
    "                                                scaledDataSetTest, \n",
    "                                                False, \n",
    "                                                hyperparameters,\n",
    "                                                modelName = \"unconstrainedModel\")\n",
    "predictionDiagnosis(y_pred1Test, dataSetTest[\"Price\"], \" Price \")\n",
    "impV1Test = plotImpliedVol(y_pred1Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O7JCdZib-C7g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SfnzM06b--q0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zFoYMDiI_DHi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ONoMCWOCXWO-"
   },
   "source": [
    "## Dupire formula implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QWZzojhoDJRn"
   },
   "outputs": [],
   "source": [
    "#Dupire formula from exact derivative computation\n",
    "def dupireFormula(HessianStrike, \n",
    "                  GradMaturity, \n",
    "                  Strike,\n",
    "                  scaleTensor,\n",
    "                  strikeMinTensor,\n",
    "                  IsTraining=True):\n",
    "  twoConstant = tf.constant(2.0)\n",
    "  dupireVar = tf.math.divide(tf.math.divide(tf.math.scalar_mul(twoConstant,GradMaturity), \n",
    "                                            HessianStrike), \n",
    "                             tf.square(Strike + strikeMinTensor / scaleTensor))\n",
    "  #Initial weights of neural network can be random which lead to negative dupireVar\n",
    "  dupireVolTensor = tf.sqrt(dupireVar) \n",
    "  return dupireVolTensor, dupireVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o59zKQXO8LKU"
   },
   "outputs": [],
   "source": [
    "#Dupire formula with derivative obtained from native tensorflow algorithmic differentiation\n",
    "def rawDupireFormula(priceTensor, \n",
    "                     adjustedStrikeTensor, \n",
    "                     maturityTensor,\n",
    "                     scaleTensor,\n",
    "                     strikeMinTensor,\n",
    "                     IsTraining=True):\n",
    "  batchSize = tf.shape(adjustedStrikeTensor)[0]\n",
    "  dK = tf.reshape(tf.gradients(priceTensor, adjustedStrikeTensor, name=\"dK\")[0], shape=[batchSize,-1])\n",
    "  hK = tf.reshape(tf.gradients(dK, adjustedStrikeTensor, name=\"hK\")[0], shape=[batchSize,-1])\n",
    "  dupireDenominator = tf.square(adjustedStrikeTensor + strikeMinTensor / scaleTensor) * hK\n",
    "\n",
    "  dT = tf.reshape(tf.gradients(priceTensor,maturityTensor,name=\"dT\")[0], shape=[batchSize,-1])\n",
    "\n",
    "  #Initial weights of neural network can be random which lead to negative dupireVar\n",
    "  dupireVar = 2 * dT / dupireDenominator\n",
    "  dupireVol = tf.sqrt(dupireVar) \n",
    "  return  dupireVol, dT, hK / tf.square(scaleTensor), dupireVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B3YTL72Tw_FV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s192l-r-B6lD"
   },
   "source": [
    "### Hybrid architecture (Exact derivatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1mtTLFrQC36u"
   },
   "outputs": [],
   "source": [
    "def exact_derivatives(Strike, Maturity):\n",
    "    w1K = tf.get_default_graph().get_tensor_by_name( 'dense/kernel:0')\n",
    "    w1T = tf.get_default_graph().get_tensor_by_name( 'dense_1/kernel:0')\n",
    "    w2 = tf.get_default_graph().get_tensor_by_name( 'dense_2/kernel:0')\n",
    "    w3 = tf.get_default_graph().get_tensor_by_name( 'dense_3/kernel:0')\n",
    "\n",
    "    b1K = tf.get_default_graph().get_tensor_by_name( 'dense/bias:0')\n",
    "    b1T = tf.get_default_graph().get_tensor_by_name( 'dense_1/bias:0')\n",
    "    b2 = tf.get_default_graph().get_tensor_by_name( 'dense_2/bias:0')\n",
    "    b3 = tf.get_default_graph().get_tensor_by_name( 'dense_3/bias:0')\n",
    "\n",
    "    Z1K= tf.nn.softplus(tf.matmul(Strike, w1K) + b1K)\n",
    "    Z1T= tf.nn.sigmoid(tf.matmul(Maturity, w1T) + b1T)\n",
    "\n",
    "    Z= tf.concat([Z1K, Z1T], axis=-1)\n",
    "    I2=tf.matmul(Z, w2) + b2\n",
    "    Z2=tf.nn.softplus(I2)\n",
    "    I3=tf.matmul(Z2, w3) + b3\n",
    "    F=tf.nn.softplus(I3)\n",
    "\n",
    "    D1K= tf.nn.sigmoid(tf.matmul(Strike, w1K) + b1K)\n",
    "    I2K=tf.multiply(D1K, w1K)\n",
    "    Z2K = tf.concat([I2K, tf.scalar_mul(tf.constant(0.0),I2K)],axis=-1)\n",
    "   \n",
    "    dI2dK=tf.matmul(Z2K, w2)\n",
    "    Z2w3=tf.multiply(tf.nn.sigmoid(I2),dI2dK)\n",
    "    dI3dK=tf.matmul(Z2w3, w3)\n",
    "    dF_dK=tf.multiply(tf.nn.sigmoid(I3),dI3dK)\n",
    "    \n",
    "    D1T= sigmoidGradient(tf.matmul(Maturity,w1T) + b1T)\n",
    "    I2T=tf.multiply(D1T, w1T)\n",
    "    Z2T = tf.concat([tf.scalar_mul(tf.constant(0.0),I2T), I2T],axis=-1)\n",
    "   \n",
    "    dI2dT=tf.matmul(Z2T, w2)\n",
    "    Z2w3=tf.multiply(tf.nn.sigmoid(I2),dI2dT)\n",
    "    dI3dT=tf.matmul(Z2w3, w3)\n",
    "    dF_dT=tf.multiply(tf.nn.sigmoid(I3),dI3dT)\n",
    "    \n",
    "    \n",
    "    d2F_dK2=tf.multiply(sigmoidGradient(I3),tf.square(dI3dK))\n",
    "    DD1K=sigmoidGradient(tf.matmul(Strike, w1K) + b1K)\n",
    "    w1K2=tf.multiply(w1K,w1K)\n",
    "    ID2K=tf.multiply(DD1K,w1K2)\n",
    "    ZD2K = tf.concat([ID2K, tf.scalar_mul(tf.constant(0.0),ID2K)],axis=-1)\n",
    "   \n",
    "    d2I2_dK2=tf.matmul(ZD2K, w2)\n",
    "    \n",
    "    ZD2=tf.multiply(sigmoidGradient(I2), tf.square(dI2dK)) \n",
    "    ZD2+=tf.multiply(tf.nn.sigmoid(I2),d2I2_dK2)\n",
    "    d2I3dK2=tf.matmul(ZD2, w3)\n",
    "    \n",
    "    d2F_dK2+=tf.multiply(tf.nn.sigmoid(I3),d2I3dK2)\n",
    "    \n",
    "    return dF_dT, dF_dK, d2F_dK2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_fkoZMVXWO-"
   },
   "outputs": [],
   "source": [
    "#Tools functions for neural architecture\n",
    "def positiveKernelInitializer(shape, \n",
    "                              dtype=None, \n",
    "                              partition_info=None):\n",
    "  return tf.abs(tf.keras.initializers.normal()(shape,dtype=dtype, partition_info=partition_info))\n",
    "\n",
    "\n",
    "#Neural network architecture\n",
    "def convexLayer1(n_units, tensor, isTraining, name, isNonDecreasing = True):\n",
    "  with tf.name_scope(name):\n",
    "    layer = tf.layers.dense(tensor if isNonDecreasing else (- tensor), \n",
    "                            units=n_units,\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_normal())\n",
    "    \n",
    "    \n",
    "    return tf.nn.softplus(layer), layer\n",
    "\n",
    "def monotonicLayer1(n_units,  tensor, isTraining, name):\n",
    "  with tf.name_scope(name):\n",
    "    layer = tf.layers.dense(tensor, \n",
    "                            units=n_units,\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_normal())\n",
    "    \n",
    "    \n",
    "    \n",
    "    return tf.nn.sigmoid(layer),layer\n",
    "\n",
    "def convexOutputLayer1(n_units, tensor, isTraining, name, isNonDecreasing = True):\n",
    "  with tf.name_scope(name):\n",
    "    layer = tf.layers.dense(tensor if isNonDecreasing else (- tensor), \n",
    "                            units=2*n_units,\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_normal(),\n",
    "                            activation = 'softplus') \n",
    "    \n",
    "     \n",
    "    layer = tf.layers.dense(layer, \n",
    "                            units=1,\n",
    "                            kernel_initializer=positiveKernelInitializer, \n",
    "                            activation = 'softplus')\n",
    "    \n",
    "    return layer, layer \n",
    "  \n",
    "\n",
    "def convexLayerHybrid1(n_units, \n",
    "                      tensor, \n",
    "                      isTraining, \n",
    "                      name, \n",
    "                      activationFunction2 = Act.softplus,\n",
    "                      activationFunction1 = Act.exponential,\n",
    "                      isNonDecreasing = True):\n",
    "  with tf.name_scope(name):\n",
    "    layer = tf.layers.dense(tensor if isNonDecreasing else (- tensor), \n",
    "                            units=n_units,\n",
    "                            kernel_initializer=positiveKernelInitializer)\n",
    "    l1,l2 = tf.split(layer,2,1)\n",
    "    output = tf.concat([activationFunction1(l1),activationFunction2(l2)],axis=-1)\n",
    "    return output , layer\n",
    "\n",
    "def sigmoidGradient(inputTensor):\n",
    "  return tf.nn.sigmoid(inputTensor) * ( 1 - tf.nn.sigmoid(inputTensor) )\n",
    "\n",
    "def sigmoidHessian(inputTensor) :\n",
    "  return (tf.square(1 - tf.nn.sigmoid(inputTensor)) -\n",
    "          tf.nn.sigmoid(inputTensor) * (1 - tf.nn.sigmoid(inputTensor)))\n",
    "\n",
    "  \n",
    "def NNArchitectureConstrainedDupire(n_units, \n",
    "                                    strikeTensor,\n",
    "                                    maturityTensor, \n",
    "                                    scaleTensor, \n",
    "                                    strikeMinTensor,\n",
    "                                    vegaRef, \n",
    "                                    hyperparameters,\n",
    "                                    IsTraining=True):\n",
    "  #First splitted layer\n",
    "  hidden1S, layer1S = convexLayer1(n_units = n_units,\n",
    "                                   tensor = strikeTensor,\n",
    "                                   isTraining=IsTraining,\n",
    "                                   name = \"Hidden1S\")\n",
    "  \n",
    "  hidden1M,layer1M = monotonicLayer1(n_units = n_units,\n",
    "                                     tensor = maturityTensor,\n",
    "                                     isTraining = IsTraining,\n",
    "                                     name = \"Hidden1M\")\n",
    "  \n",
    "  hidden1 = tf.concat([hidden1S, hidden1M], axis=-1)\n",
    "  \n",
    "  #Second layer and output layer\n",
    "  out, layer = convexOutputLayer1(n_units = n_units,\n",
    "                                  tensor = hidden1,\n",
    "                                  isTraining = IsTraining,\n",
    "                                  name = \"Output\")\n",
    "  \n",
    "  \n",
    "  dT, dS, HS = exact_derivatives(strikeTensor, maturityTensor)\n",
    "  \n",
    "  \n",
    "  \n",
    "  #Local volatility\n",
    "  dupireVol, dupireVar = dupireFormula(HS, dT, \n",
    "                                       strikeTensor,\n",
    "                                       scaleTensor,\n",
    "                                       strikeMinTensor, \n",
    "                                       IsTraining=IsTraining)\n",
    "  \n",
    "  #Soft constraints on price\n",
    "  lambdas = hyperparameters[\"lambdaSoft\"]\n",
    "  lowerBoundTheta = tf.constant(hyperparameters[\"lowerBoundTheta\"])\n",
    "  lowerBoundGamma = tf.constant(hyperparameters[\"lowerBoundGamma\"])\n",
    "  grad_penalty = lambdas * tf.reduce_mean(tf.nn.relu(-dT + lowerBoundTheta) / vegaRef)\n",
    "  HSScaled = HS / tf.square(scaleTensor)\n",
    "  hessian_penalty = lambdas * hyperparameters[\"lambdaGamma\"] * tf.reduce_mean(tf.nn.relu(- HSScaled + lowerBoundGamma) / vegaRef)\n",
    "  \n",
    "  return out, [out, dupireVol, dT, HSScaled, dupireVar], [grad_penalty, hessian_penalty], evalAndFormatDupireResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SHfggURehdvS"
   },
   "outputs": [],
   "source": [
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yRIe-lSD8A0H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NX751W-BNad"
   },
   "outputs": [],
   "source": [
    "y_pred2, volLocale2, dNN_T2, gNN_K2, lossSerie2 = create_train_model(NNArchitectureConstrainedDupire,\n",
    "                                                                     scaledDataSet,\n",
    "                                                                     False, \n",
    "                                                                     hyperparameters,\n",
    "                                                                     modelName = \"convexHybridMatthewDupireVolModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XnX-RKpgBO0y"
   },
   "outputs": [],
   "source": [
    "plotEpochLoss(lossSerie2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "puVhFTMTBcQ5"
   },
   "outputs": [],
   "source": [
    "lossSerie2.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5k1RAfyearf"
   },
   "outputs": [],
   "source": [
    "y_pred2, volLocale2, dNN_T2, gNN_K2, lossSerie2 = create_eval_model(NNArchitectureConstrainedDupire, \n",
    "                                                                    scaledDataSet, \n",
    "                                                                    False, \n",
    "                                                                    hyperparameters,\n",
    "                                                                    modelName = \"convexHybridMatthewDupireVolModel\")\n",
    "modelSummary(y_pred2, volLocale2, dNN_T2, gNN_K2, dataSet)\n",
    "impV2 = plotImpliedVol(y_pred2, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xj1pPdXLZHrk"
   },
   "outputs": [],
   "source": [
    "volLocale2.loc[(midS0,slice(None))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R0jZy5U7JqVt"
   },
   "outputs": [],
   "source": [
    "y_pred2Test, volLocale2Test, dNN_T2Test, gNN_K2Test, lossSerie2Test = create_eval_model(NNArchitectureConstrainedDupire, \n",
    "                                                                                        scaledDataSetTest, \n",
    "                                                                                        False, \n",
    "                                                                                        hyperparameters,\n",
    "                                                                                        modelName = \"convexHybridMatthewDupireVolModel\")\n",
    "modelSummary(y_pred2Test, volLocale2Test, dNN_T2Test, gNN_K2Test, dataSetTest)\n",
    "impV2Test = plotImpliedVol(y_pred2Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "de9dKmAAHI2m"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hUMtwNuguMRu"
   },
   "source": [
    "### Hybrid Network (Derivatives from algorithmic differentiation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YOiA45eZJqSb"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j8m0kMz_udFn"
   },
   "outputs": [],
   "source": [
    "\n",
    "def NNArchitectureConstrainedRawDupire(n_units, \n",
    "                                       strikeTensor,\n",
    "                                       maturityTensor,\n",
    "                                       scaleTensor,\n",
    "                                       strikeMinTensor, \n",
    "                                       vegaRef, \n",
    "                                       hyperparameters,\n",
    "                                       IsTraining=True):\n",
    "  #First splitted layer\n",
    "  hidden1S = convexLayer(n_units = n_units,\n",
    "                         tensor = strikeTensor,\n",
    "                         isTraining=IsTraining, \n",
    "                         name = \"Hidden1S\")\n",
    "  \n",
    "  hidden1M = monotonicLayer(n_units = n_units,\n",
    "                            tensor = maturityTensor, \n",
    "                            isTraining = IsTraining, \n",
    "                            name = \"Hidden1M\")\n",
    "  \n",
    "  hidden1 = tf.concat([hidden1S, hidden1M], axis=-1)\n",
    "  \n",
    "  #Second hidden layer and output layer\n",
    "  out = convexOutputLayer(n_units = n_units,\n",
    "                          tensor = hidden1,\n",
    "                          isTraining = IsTraining,\n",
    "                          name = \"Output\")\n",
    "  \n",
    "  #Compute local volatility\n",
    "  dupireVol, theta, hK, dupireVar = rawDupireFormula(out, strikeTensor, \n",
    "                                                     maturityTensor, \n",
    "                                                     scaleTensor, \n",
    "                                                     strikeMinTensor,\n",
    "                                                     IsTraining=IsTraining)\n",
    "\n",
    "  #Soft constraints for no-arbitrage\n",
    "  lambdas = hyperparameters[\"lambdaSoft\"] \n",
    "  lowerBoundTheta = tf.constant(hyperparameters[\"lowerBoundTheta\"])\n",
    "  lowerBoundGamma = tf.constant(hyperparameters[\"lowerBoundGamma\"])\n",
    "  grad_penalty = lambdas * tf.reduce_mean(tf.nn.relu(-theta + lowerBoundTheta) / vegaRef)\n",
    "  hessian_penalty = lambdas * hyperparameters[\"lambdaGamma\"] * tf.reduce_mean(tf.nn.relu(-hK + lowerBoundGamma) / vegaRef)\n",
    "  \n",
    "  return out, [out, dupireVol, theta, hK, dupireVar], [grad_penalty, hessian_penalty], evalAndFormatDupireResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GntRmtJkudCz"
   },
   "outputs": [],
   "source": [
    "y_pred3, volLocale3, dNN_T3, gNN_K3, lossSerie3 = create_train_model(NNArchitectureConstrainedRawDupire,\n",
    "                                                                     scaledDataSet,\n",
    "                                                                     False, \n",
    "                                                                     hyperparameters,\n",
    "                                                                     modelName = \"convexHybridDupireVolModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2lkQVBdnuc_-"
   },
   "outputs": [],
   "source": [
    "plotEpochLoss(lossSerie3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bfEKwrHCuc9B"
   },
   "outputs": [],
   "source": [
    "lossSerie3.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MA24IOZoucqj"
   },
   "outputs": [],
   "source": [
    "y_pred3, volLocale3, dNN_T3, gNN_K3, lossSerie3 = create_eval_model(NNArchitectureConstrainedRawDupire, \n",
    "                                                                    scaledDataSet, \n",
    "                                                                    False,\n",
    "                                                                    hyperparameters,\n",
    "                                                                    modelName = \"convexHybridDupireVolModel\")\n",
    "modelSummary(y_pred3, volLocale3, dNN_T3, gNN_K3, dataSet)\n",
    "impV3 = plotImpliedVol(y_pred3, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R5BCRxbMuc0p"
   },
   "outputs": [],
   "source": [
    "volLocale3.loc[(midS0,slice(None))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7bS4fYI7ucjO"
   },
   "outputs": [],
   "source": [
    "y_pred3Test, volLocale3Test, dNN_T3Test, gNN_K3Test, lossSerie3Test = create_eval_model(NNArchitectureConstrainedRawDupire, \n",
    "                                                                                        scaledDataSetTest, \n",
    "                                                                                        False, \n",
    "                                                                                        hyperparameters,\n",
    "                                                                                        modelName = \"convexHybridDupireVolModel\")\n",
    "modelSummary(y_pred3Test, volLocale3Test, dNN_T3Test, gNN_K3Test, dataSetTest)\n",
    "impV3Test = plotImpliedVol(y_pred3Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y02WGdGVucZ5"
   },
   "outputs": [],
   "source": [
    "\n",
    "dNN_T3Test[dNN_T3Test<=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XIki8Q3nfygr"
   },
   "outputs": [],
   "source": [
    "modelSummary(y_pred3, \n",
    "             volLocale3, \n",
    "             dNN_T3, \n",
    "             gNN_K3, \n",
    "             dataSet,\n",
    "             logMoneynessScale = True)\n",
    "impV3 = plotImpliedVol(y_pred3, \n",
    "                       dataSet[\"ImpliedVol\"], \n",
    "                       rIntegralSpline=riskFreeIntegral, \n",
    "                       qIntegralSpline=divSpreadIntegral,\n",
    "                       logMoneynessScale = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sixvBhaGfyYE"
   },
   "outputs": [],
   "source": [
    "modelSummary(y_pred3Test, \n",
    "             volLocale3Test, \n",
    "             dNN_T3Test, \n",
    "             gNN_K3Test, \n",
    "             dataSetTest,\n",
    "             logMoneynessScale = True)\n",
    "impV3Test = plotImpliedVol(y_pred3Test, \n",
    "                           dataSetTest[\"ImpliedVol\"], \n",
    "                           rIntegralSpline=riskFreeIntegral, \n",
    "                           qIntegralSpline=divSpreadIntegral,\n",
    "                           logMoneynessScale = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r9EvOdg0kU2P"
   },
   "source": [
    "### Standard network with soft constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O1UXinUrkUm_"
   },
   "outputs": [],
   "source": [
    "def NNArchitectureVanillaSoftDupire(n_units, strikeTensor,\n",
    "                                    maturityTensor,\n",
    "                                    scaleTensor,\n",
    "                                    strikeMinTensor,\n",
    "                                    vegaRef,\n",
    "                                    hyperparameters,\n",
    "                                    IsTraining=True):\n",
    "  \n",
    "  inputLayer = tf.concat([strikeTensor,maturityTensor], axis=-1)\n",
    "  #First layer\n",
    "  hidden1 = unconstrainedLayer(n_units = n_units,\n",
    "                               tensor = inputLayer,\n",
    "                               isTraining=IsTraining, \n",
    "                               name = \"Hidden1\")\n",
    "  #Second layer\n",
    "  hidden2 = unconstrainedLayer(n_units = n_units,\n",
    "                               tensor = hidden1,\n",
    "                               isTraining=IsTraining, \n",
    "                               name = \"Hidden2\")\n",
    "  #Output layer\n",
    "  out = unconstrainedLayer(n_units = 1,\n",
    "                           tensor = hidden2,\n",
    "                           isTraining=IsTraining, \n",
    "                           name = \"Output\",\n",
    "                           activation = None)\n",
    "  #Local volatility \n",
    "  dupireVol, theta, hK, dupireVar = rawDupireFormula(out, strikeTensor,\n",
    "                                                     maturityTensor,\n",
    "                                                     scaleTensor,\n",
    "                                                     strikeMinTensor,\n",
    "                                                     IsTraining=IsTraining)\n",
    "  #Soft constraints for no arbitrage\n",
    "  lambdas = hyperparameters[\"lambdaSoft\"] \n",
    "  lowerBoundTheta = tf.constant(hyperparameters[\"lowerBoundTheta\"])\n",
    "  lowerBoundGamma = tf.constant(hyperparameters[\"lowerBoundGamma\"])\n",
    "  grad_penalty = lambdas * tf.reduce_mean(tf.nn.relu(-theta + lowerBoundTheta) / vegaRef)\n",
    "  hessian_penalty = lambdas * hyperparameters[\"lambdaGamma\"] * tf.reduce_mean(tf.nn.relu(-hK + lowerBoundGamma) / vegaRef)\n",
    "  \n",
    "  return out, [out, dupireVol, theta, hK, dupireVar], [grad_penalty, hessian_penalty], evalAndFormatDupireResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k-FDViM6keCA"
   },
   "outputs": [],
   "source": [
    "y_pred4, volLocale4, dNN_T4, gNN_K4, lossSerie4 = create_train_model(NNArchitectureVanillaSoftDupire,\n",
    "                                                                     scaledDataSet,\n",
    "                                                                     False, \n",
    "                                                                     hyperparameters,\n",
    "                                                                     modelName = \"convexSoftDupireVolModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YqrPGUcHkd_a"
   },
   "outputs": [],
   "source": [
    "plotEpochLoss(lossSerie4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PBoUrx83kd9W"
   },
   "outputs": [],
   "source": [
    "lossSerie4.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hI7jsuqPYvI1"
   },
   "outputs": [],
   "source": [
    "y_pred4, volLocale4, dNN_T4, gNN_K4, lossSerie4 = create_eval_model(NNArchitectureVanillaSoftDupire,\n",
    "                                                                    scaledDataSet, \n",
    "                                                                    False, \n",
    "                                                                    hyperparameters,\n",
    "                                                                    modelName = \"convexSoftDupireVolModel\")\n",
    "modelSummary(y_pred4, volLocale4, dNN_T4, gNN_K4, dataSet)\n",
    "impV4 = plotImpliedVol(y_pred4, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rwpPXhszkd6Z"
   },
   "outputs": [],
   "source": [
    "volLocale4.loc[(midS0,slice(None))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xAfSOpREkdzc"
   },
   "outputs": [],
   "source": [
    "y_pred4Test, volLocale4Test, dNN_T4Test, gNN_K4Test, lossSerie4Test = create_eval_model(NNArchitectureVanillaSoftDupire, \n",
    "                                                                                        scaledDataSetTest, \n",
    "                                                                                        False, \n",
    "                                                                                        hyperparameters,\n",
    "                                                                                        modelName = \"convexSoftDupireVolModel\")\n",
    "modelSummary(y_pred4Test, volLocale4Test, dNN_T4Test, gNN_K4Test, dataSetTest)\n",
    "impV4Test = plotImpliedVol(y_pred4Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OCZXZWCIkdxR"
   },
   "outputs": [],
   "source": [
    "modelSummary(y_pred4, \n",
    "             volLocale4, \n",
    "             dNN_T4, \n",
    "             gNN_K4, \n",
    "             dataSet,\n",
    "             logMoneynessScale = True)\n",
    "impV4 = plotImpliedVol(y_pred4, \n",
    "                       dataSet[\"ImpliedVol\"], \n",
    "                       rIntegralSpline=riskFreeIntegral, \n",
    "                       qIntegralSpline=divSpreadIntegral,\n",
    "                       logMoneynessScale = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0SsMFMnokdu-"
   },
   "outputs": [],
   "source": [
    "modelSummary(y_pred4Test, \n",
    "             volLocale4Test, \n",
    "             dNN_T4Test, \n",
    "             gNN_K4Test, \n",
    "             dataSetTest,\n",
    "             logMoneynessScale = True)\n",
    "impV4Test = plotImpliedVol(y_pred4Test, \n",
    "                           dataSetTest[\"ImpliedVol\"], \n",
    "                           rIntegralSpline=riskFreeIntegral, \n",
    "                           qIntegralSpline=divSpreadIntegral,\n",
    "                           logMoneynessScale = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9LHmCnZikdr1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Atqa9I8Gkdn-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_eJG7_513hNC"
   },
   "source": [
    "### Unconstrained standard network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6skBo-i-JqOx"
   },
   "outputs": [],
   "source": [
    "def NNArchitectureUnconstrainedDupire(n_units, strikeTensor,\n",
    "                                      maturityTensor,\n",
    "                                      scaleTensor,\n",
    "                                      strikeMinTensor, \n",
    "                                      vegaRef,\n",
    "                                      hyperparameters,\n",
    "                                      IsTraining=True):\n",
    "  \n",
    "  inputLayer = tf.concat([strikeTensor,maturityTensor], axis=-1)\n",
    "  \n",
    "  #First layer\n",
    "  hidden1 = unconstrainedLayer(n_units = n_units,\n",
    "                               tensor = inputLayer,\n",
    "                               isTraining=IsTraining, \n",
    "                               name = \"Hidden1\")\n",
    "  #Second layer\n",
    "  hidden2 = unconstrainedLayer(n_units = n_units,\n",
    "                               tensor = hidden1,\n",
    "                               isTraining=IsTraining, \n",
    "                               name = \"Hidden2\")\n",
    "  #Ouput layer\n",
    "  out = unconstrainedLayer(n_units = 1,\n",
    "                           tensor = hidden2,\n",
    "                           isTraining=IsTraining, \n",
    "                           name = \"Output\",\n",
    "                           activation = None)\n",
    "  #Local volatility\n",
    "  dupireVol, theta, hK, dupireVar = rawDupireFormula(out, strikeTensor,\n",
    "                                                     maturityTensor,\n",
    "                                                     scaleTensor,\n",
    "                                                     strikeMinTensor,\n",
    "                                                     IsTraining=IsTraining)\n",
    "  \n",
    "  return out, [out, dupireVol, theta, hK, dupireVar], [], evalAndFormatDupireResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cIEIgBTp3mFn"
   },
   "outputs": [],
   "source": [
    "y_pred5, volLocale5, dNN_T5, gNN_K5, lossSerie5 = create_train_model(NNArchitectureUnconstrainedDupire,\n",
    "                                                                     scaledDataSet,\n",
    "                                                                     False, \n",
    "                                                                     hyperparameters,\n",
    "                                                                     modelName = \"unconstrainedDupireVolModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "evKBtgfB3mBj"
   },
   "outputs": [],
   "source": [
    "plotEpochLoss(lossSerie5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X8qTeMJm3l-i"
   },
   "outputs": [],
   "source": [
    "lossSerie5.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "odwXtVHNYvJH"
   },
   "outputs": [],
   "source": [
    "y_pred5, volLocale5, dNN_T5, gNN_K5, lossSerie5 = create_eval_model(NNArchitectureUnconstrainedDupire,\n",
    "                                                                    scaledDataSet,\n",
    "                                                                    False,\n",
    "                                                                    hyperparameters,\n",
    "                                                                    modelName = \"unconstrainedDupireVolModel\")\n",
    "modelSummary(y_pred5, volLocale5, dNN_T5, gNN_K5, dataSet)\n",
    "impV5 = plotImpliedVol(y_pred5, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a306_WrY3l7R"
   },
   "outputs": [],
   "source": [
    "volLocale5.loc[(midS0,slice(None))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1vsHhlAW3lpu"
   },
   "outputs": [],
   "source": [
    "y_pred5Test, volLocale5Test, dNN_T5Test, gNN_K5Test, lossSerie5Test = create_eval_model(NNArchitectureUnconstrainedDupire, \n",
    "                                                                                        scaledDataSetTest, \n",
    "                                                                                        False, \n",
    "                                                                                        hyperparameters,\n",
    "                                                                                        modelName = \"unconstrainedDupireVolModel\")\n",
    "modelSummary(y_pred5Test, volLocale5Test, dNN_T5Test, gNN_K5Test, dataSetTest)\n",
    "impV5Test = plotImpliedVol(y_pred5Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cUB3Y62YJpwo"
   },
   "outputs": [],
   "source": [
    "modelSummary(y_pred5, \n",
    "             volLocale5, \n",
    "             dNN_T5, \n",
    "             gNN_K5, \n",
    "             dataSet,\n",
    "             logMoneynessScale = True)\n",
    "impV5 = plotImpliedVol(y_pred5, \n",
    "                       dataSet[\"ImpliedVol\"], \n",
    "                       rIntegralSpline=riskFreeIntegral, \n",
    "                       qIntegralSpline=divSpreadIntegral,\n",
    "                       logMoneynessScale = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kzo0lHB9e8pN"
   },
   "outputs": [],
   "source": [
    "modelSummary(y_pred5Test, \n",
    "             volLocale5Test, \n",
    "             dNN_T5Test, \n",
    "             gNN_K5Test, \n",
    "             dataSetTest,\n",
    "             logMoneynessScale = True)\n",
    "impV5Test = plotImpliedVol(y_pred5Test, \n",
    "                           dataSetTest[\"ImpliedVol\"], \n",
    "                           rIntegralSpline=riskFreeIntegral, \n",
    "                           qIntegralSpline=divSpreadIntegral,\n",
    "                           logMoneynessScale = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MrvDySFQWK-D"
   },
   "source": [
    "### Hard constrained architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1R2iPlotWQoH"
   },
   "outputs": [],
   "source": [
    "#Tools functions for hard constrained neural architecture\n",
    "\n",
    "def convexLayerHard(n_units, tensor, isTraining, name, isNonDecreasing = True):\n",
    "  with tf.name_scope(name):\n",
    "    layer = tf.layers.dense(tensor if isNonDecreasing else (- tensor), \n",
    "                            units=n_units,\n",
    "                            kernel_constraint = tf.keras.constraints.NonNeg(), \n",
    "                            kernel_initializer=tf.keras.initializers.glorot_normal())\n",
    "    \n",
    "    \n",
    "    return tf.nn.softplus(layer), layer \n",
    "\n",
    "def monotonicLayerHard(n_units,  tensor, isTraining, name):\n",
    "  with tf.name_scope(name):\n",
    "    layer = tf.layers.dense(tensor, \n",
    "                            units=n_units,\n",
    "                            kernel_constraint = tf.keras.constraints.NonNeg(), \n",
    "                            kernel_initializer=tf.keras.initializers.glorot_normal())\n",
    "    \n",
    "    \n",
    "    \n",
    "    return tf.nn.sigmoid(layer),layer\n",
    "\n",
    "def convexOutputLayerHard(n_units, tensor, isTraining, name, isNonDecreasing = True):\n",
    "  with tf.name_scope(name):\n",
    "    layer = tf.layers.dense(tensor if isNonDecreasing else (- tensor), \n",
    "                            units=2*n_units,\n",
    "                            kernel_constraint = tf.keras.constraints.NonNeg(), \n",
    "                            kernel_initializer=tf.keras.initializers.glorot_normal(),\n",
    "                            activation = 'softplus') \n",
    "    \n",
    "     \n",
    "    layer = tf.layers.dense(layer, \n",
    "                            units=1,\n",
    "                            kernel_constraint = tf.keras.constraints.NonNeg(), \n",
    "                            kernel_initializer=positiveKernelInitializer, \n",
    "                            activation = 'softplus')\n",
    "    \n",
    "    return layer, layer \n",
    "  \n",
    "\n",
    "def convexLayerHybridHard(n_units,\n",
    "                          tensor,\n",
    "                          isTraining,\n",
    "                          name,\n",
    "                          activationFunction2 = Act.softplus,\n",
    "                          activationFunction1 = Act.exponential,\n",
    "                          isNonDecreasing = True):\n",
    "  with tf.name_scope(name):\n",
    "    layer = tf.layers.dense(tensor if isNonDecreasing else (- tensor), \n",
    "                            units=n_units,\n",
    "                            kernel_constraint = tf.keras.constraints.NonNeg(), \n",
    "                            kernel_initializer=positiveKernelInitializer)\n",
    "    l1,l2 = tf.split(layer,2,1)\n",
    "    output = tf.concat([activationFunction1(l1),activationFunction2(l2)],axis=-1)\n",
    "    return output , layer\n",
    "\n",
    "def sigmoidGradientHard(inputTensor):\n",
    "  return tf.nn.sigmoid(inputTensor) * ( 1 - tf.nn.sigmoid(inputTensor) )\n",
    "\n",
    "def sigmoidHessianHard(inputTensor) :\n",
    "  return (tf.square(1 - tf.nn.sigmoid(inputTensor)) -\n",
    "          tf.nn.sigmoid(inputTensor) * (1 - tf.nn.sigmoid(inputTensor)))\n",
    "  \n",
    "\n",
    "\n",
    "  \n",
    "def NNArchitectureHardConstrainedDupire(n_units, strikeTensor, \n",
    "                                        maturityTensor,\n",
    "                                        scaleTensor,\n",
    "                                        strikeMinTensor, \n",
    "                                        vegaRef,\n",
    "                                        hyperparameters,\n",
    "                                        IsTraining=True):\n",
    "  #First layer\n",
    "  hidden1S, layer1S = convexLayerHard(n_units = n_units,\n",
    "                                      tensor = strikeTensor,\n",
    "                                      isTraining=IsTraining,\n",
    "                                      name = \"Hidden1S\")\n",
    "  \n",
    "  hidden1M,layer1M = monotonicLayerHard(n_units = n_units,\n",
    "                                        tensor = maturityTensor,\n",
    "                                        isTraining = IsTraining,\n",
    "                                        name = \"Hidden1M\")\n",
    "  \n",
    "  hidden1 = tf.concat([hidden1S, hidden1M], axis=-1)\n",
    "  \n",
    "  #Second layer and output layer\n",
    "  out, layer = convexOutputLayerHard(n_units = n_units,\n",
    "                                     tensor = hidden1,\n",
    "                                     isTraining = IsTraining,\n",
    "                                     name = \"Output\")\n",
    "  #Local volatility\n",
    "  dupireVol, theta, hK, dupireVar = rawDupireFormula(out, strikeTensor,\n",
    "                                                     maturityTensor,\n",
    "                                                     scaleTensor,\n",
    "                                                     strikeMinTensor,\n",
    "                                                     IsTraining=IsTraining)\n",
    "  \n",
    "  return out, [out, dupireVol, theta, hK, dupireVar], [], evalAndFormatDupireResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ruwRhj_-WQlh"
   },
   "outputs": [],
   "source": [
    "y_pred6, volLocale6, dNN_T6, gNN_K6, lossSerie6 = create_train_model(NNArchitectureHardConstrainedDupire,\n",
    "                                                                     scaledDataSet,\n",
    "                                                                     False, \n",
    "                                                                     hyperparameters,\n",
    "                                                                     modelName = \"convexHardDupireVolModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W0GD3xYRWQif"
   },
   "outputs": [],
   "source": [
    "plotEpochLoss(lossSerie6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XF5Iq9ZYWQf6"
   },
   "outputs": [],
   "source": [
    "lossSerie6.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tRpdkOKMYvJQ"
   },
   "outputs": [],
   "source": [
    "y_pred6, volLocale6, dNN_T6, gNN_K6, lossSerie6 = create_eval_model(NNArchitectureHardConstrainedDupire, \n",
    "                                                                    scaledDataSet, \n",
    "                                                                    False, \n",
    "                                                                    hyperparameters,\n",
    "                                                                    modelName = \"convexHardDupireVolModel\")\n",
    "modelSummary(y_pred6, volLocale6, dNN_T6, gNN_K6, dataSet)\n",
    "impV6 = plotImpliedVol(y_pred6, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-oZmufhXWQdk"
   },
   "outputs": [],
   "source": [
    "volLocale6.loc[(midS0,slice(None))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_NKwHrxdWQWG"
   },
   "outputs": [],
   "source": [
    "y_pred6Test, volLocale6Test, dNN_T6Test, gNN_K6Test, lossSerie6Test = create_eval_model(NNArchitectureHardConstrainedDupire, \n",
    "                                                                                        scaledDataSetTest, \n",
    "                                                                                        False, \n",
    "                                                                                        hyperparameters,\n",
    "                                                                                        modelName = \"convexHardDupireVolModel\")\n",
    "modelSummary(y_pred6Test, volLocale6Test, dNN_T6Test, gNN_K6Test, dataSetTest)\n",
    "impV6Test = plotImpliedVol(y_pred6Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8W6gaoaKWQTS"
   },
   "outputs": [],
   "source": [
    "modelSummary(y_pred6, \n",
    "             volLocale6, \n",
    "             dNN_T6, \n",
    "             gNN_K6, \n",
    "             dataSet)\n",
    "impV6 = plotImpliedVol(y_pred6, \n",
    "                       dataSet[\"ImpliedVol\"], \n",
    "                       rIntegralSpline=riskFreeIntegral, \n",
    "                       qIntegralSpline=divSpreadIntegral,\n",
    "                       logMoneynessScale = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UTPL26K4dYLL"
   },
   "outputs": [],
   "source": [
    "modelSummary(y_pred6Test, \n",
    "             volLocale6Test, \n",
    "             dNN_T6Test, \n",
    "             gNN_K6Test, \n",
    "             dataSetTest)\n",
    "impV6Test = plotImpliedVol(y_pred6Test, \n",
    "                           dataSetTest[\"ImpliedVol\"], \n",
    "                           rIntegralSpline=riskFreeIntegral, \n",
    "                           qIntegralSpline=divSpreadIntegral,\n",
    "                           logMoneynessScale = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n_iWhfZjZJt7"
   },
   "source": [
    "## Dupire regularization \n",
    "\n",
    "Same lines as above except that dupire regularization is now activated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NZBPdHPuB0CX"
   },
   "source": [
    "### Hybrid architecture (Exact derivatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hmbg_HciZNaW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CMQDARTbZTM8"
   },
   "outputs": [],
   "source": [
    "y_pred8, volLocale8, dNN_T8, gNN_K8, lossSerie8 = create_train_model(NNArchitectureConstrainedDupire,\n",
    "                                                                     scaledDataSet,\n",
    "                                                                     True, \n",
    "                                                                     hyperparameters,\n",
    "                                                                     modelName = \"regularizedConvexHybridMatthewDupireVolModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "phELMC43ZS-4"
   },
   "outputs": [],
   "source": [
    "plotEpochLoss(lossSerie8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elgx3ip3ZS24"
   },
   "outputs": [],
   "source": [
    "lossSerie8.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5kIlaICuZSzV"
   },
   "outputs": [],
   "source": [
    "y_pred8, volLocale8, dNN_T8, gNN_K8, lossSerie8 = create_eval_model(NNArchitectureConstrainedDupire, \n",
    "                                                                    scaledDataSet, \n",
    "                                                                    True, \n",
    "                                                                    hyperparameters,\n",
    "                                                                    modelName = \"regularizedConvexHybridMatthewDupireVolModel\")\n",
    "modelSummary(y_pred8, volLocale8, dNN_T8, gNN_K8, dataSet)\n",
    "impV8 = plotImpliedVol(y_pred8, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "njoK6xVa1ReM"
   },
   "outputs": [],
   "source": [
    "y_pred8Test, volLocale8Test, dNN_T8Test, gNN_K8Test, lossSerie8Test = create_eval_model(NNArchitectureConstrainedDupire, \n",
    "                                                                                        scaledDataSetTest, \n",
    "                                                                                        True, \n",
    "                                                                                        hyperparameters,\n",
    "                                                                                        modelName = \"regularizedConvexHybridMatthewDupireVolModel\")\n",
    "modelSummary(y_pred8Test, volLocale8Test, dNN_T8Test, gNN_K8Test, dataSetTest)\n",
    "impV8Test = plotImpliedVol(y_pred8Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NB3_-zutBt2y"
   },
   "source": [
    "### Unconstrained standard network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xYrYMhOZZ3Pd"
   },
   "outputs": [],
   "source": [
    "y_pred9, volLocale9, dNN_T9, gNN_K9, lossSerie9 = create_train_model(NNArchitectureUnconstrainedDupire,\n",
    "                                                                     scaledDataSet,\n",
    "                                                                     True, \n",
    "                                                                     hyperparameters,\n",
    "                                                                     modelName = \"regularizedUnconstrainedDupireVolModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wHi2doJQBbZO"
   },
   "outputs": [],
   "source": [
    "plotEpochLoss(lossSerie9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "76G0tLeSBdeD"
   },
   "outputs": [],
   "source": [
    "lossSerie9.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AYzkOXUABdXV"
   },
   "outputs": [],
   "source": [
    "y_pred9, volLocale9, dNN_T9, gNN_K9, lossSerie9 = create_eval_model(NNArchitectureUnconstrainedDupire, \n",
    "                                                                    scaledDataSet, \n",
    "                                                                    True, \n",
    "                                                                    hyperparameters,\n",
    "                                                                    modelName = \"regularizedUnconstrainedDupireVolModel\")\n",
    "modelSummary(y_pred9, volLocale9, dNN_T9, gNN_K9, dataSet)\n",
    "impV9 = plotImpliedVol(y_pred9, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t9Zvk6DAbAUn"
   },
   "outputs": [],
   "source": [
    "y_pred9Test, volLocale9Test, dNN_T9Test, gNN_K9Test, lossSerie9Test = create_eval_model(NNArchitectureUnconstrainedDupire, \n",
    "                                                                                        scaledDataSetTest, \n",
    "                                                                                        True, \n",
    "                                                                                        hyperparameters,\n",
    "                                                                                        modelName = \"regularizedUnconstrainedDupireVolModel\")\n",
    "modelSummary(y_pred9Test, volLocale9Test, dNN_T9Test, gNN_K9Test, dataSetTest)\n",
    "impV9Test = plotImpliedVol(y_pred9Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "--dUCOKu4c1v"
   },
   "outputs": [],
   "source": [
    "modelSummary(y_pred9, \n",
    "             volLocale9, \n",
    "             dNN_T9, \n",
    "             gNN_K9, \n",
    "             dataSet,\n",
    "             logMoneynessScale = True)\n",
    "impV9 = plotImpliedVol(y_pred9, \n",
    "                       dataSet[\"ImpliedVol\"], \n",
    "                       rIntegralSpline=riskFreeIntegral, \n",
    "                       qIntegralSpline=divSpreadIntegral,\n",
    "                       logMoneynessScale = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BNkdNccRcanb"
   },
   "outputs": [],
   "source": [
    "modelSummary(y_pred9Test, \n",
    "             volLocale9Test, \n",
    "             dNN_T9Test, \n",
    "             gNN_K9Test, \n",
    "             dataSetTest,\n",
    "             logMoneynessScale = True)\n",
    "impV9Test = plotImpliedVol(y_pred9Test, \n",
    "                           dataSetTest[\"ImpliedVol\"], \n",
    "                           rIntegralSpline=riskFreeIntegral, \n",
    "                           qIntegralSpline=divSpreadIntegral,\n",
    "                           logMoneynessScale = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5yP7BP6avj5q"
   },
   "source": [
    "### Hybrid Network (Derivatives from algorithmic differentiation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9u0zPfA6vpA2"
   },
   "outputs": [],
   "source": [
    "y_pred10, volLocale10, dNN_T10, gNN_K10, lossSerie10 = create_train_model(NNArchitectureConstrainedRawDupire,\n",
    "                                                                          scaledDataSet,\n",
    "                                                                          True,\n",
    "                                                                          hyperparameters,\n",
    "                                                                          modelName = \"regularizedConvexHybridDupireVolModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9t_BmpjTvpnU"
   },
   "outputs": [],
   "source": [
    "plotEpochLoss(lossSerie10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sKSBvFrzvqe8"
   },
   "outputs": [],
   "source": [
    "lossSerie10.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kh00YJyVYvJm"
   },
   "outputs": [],
   "source": [
    "y_pred10, volLocale10, dNN_T10, gNN_K10, lossSerie10 = create_eval_model(NNArchitectureConstrainedRawDupire,\n",
    "                                                                         scaledDataSet,\n",
    "                                                                         True,\n",
    "                                                                         hyperparameters,\n",
    "                                                                         modelName = \"regularizedConvexHybridDupireVolModel\")\n",
    "modelSummary(y_pred10, volLocale10, dNN_T10, gNN_K10, dataSet)\n",
    "impV10 = plotImpliedVol(y_pred10, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VADXbcv2vqBp"
   },
   "outputs": [],
   "source": [
    "y_pred10Test, volLocale10Test, dNN_T10Test, gNN_K10Test, lossSerie10Test = create_eval_model(NNArchitectureConstrainedRawDupire,\n",
    "                                                                                             scaledDataSetTest,\n",
    "                                                                                             True,\n",
    "                                                                                             hyperparameters,\n",
    "                                                                                             modelName = \"regularizedConvexHybridDupireVolModel\")\n",
    "modelSummary(y_pred10Test, volLocale10Test, dNN_T10Test, gNN_K10Test, dataSetTest)\n",
    "impV10Test = plotImpliedVol(y_pred10Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2bBB7MRVXmli"
   },
   "outputs": [],
   "source": [
    "modelSummary(y_pred10, \n",
    "             volLocale10, \n",
    "             dNN_T10, \n",
    "             gNN_K10, \n",
    "             dataSet,\n",
    "             logMoneynessScale = True)\n",
    "impV10 = plotImpliedVol(y_pred10, \n",
    "                        dataSet[\"ImpliedVol\"], \n",
    "                        rIntegralSpline=riskFreeIntegral, \n",
    "                        qIntegralSpline=divSpreadIntegral,\n",
    "                        logMoneynessScale = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bjfVGDKab29-"
   },
   "outputs": [],
   "source": [
    "modelSummary(y_pred10Test, \n",
    "             volLocale10Test, \n",
    "             dNN_T10Test, \n",
    "             gNN_K10Test, \n",
    "             dataSetTest,\n",
    "             logMoneynessScale = True)\n",
    "impV10Test = plotImpliedVol(y_pred10Test, \n",
    "                            dataSetTest[\"ImpliedVol\"], \n",
    "                            rIntegralSpline=riskFreeIntegral, \n",
    "                            qIntegralSpline=divSpreadIntegral,\n",
    "                            logMoneynessScale = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nl9Gq8s4o_DH"
   },
   "source": [
    "### Standard network with soft constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ck5is8QXicGb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "soE7FV4GpEFV"
   },
   "outputs": [],
   "source": [
    "y_pred11, volLocale11, dNN_T11, gNN_K11, lossSerie11 = create_train_model(NNArchitectureVanillaSoftDupire,\n",
    "                                                                          scaledDataSet,\n",
    "                                                                          True,\n",
    "                                                                          hyperparameters,\n",
    "                                                                          modelName = \"regularizedConvexSoftDupireVolModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "999cJ67npECl"
   },
   "outputs": [],
   "source": [
    "plotEpochLoss(lossSerie11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KRVyq4jRpEAJ"
   },
   "outputs": [],
   "source": [
    "lossSerie11.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkCAmIkx9eSd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fbu9gPcdYvJs"
   },
   "outputs": [],
   "source": [
    "y_pred11, volLocale11, dNN_T11, gNN_K11, lossSerie11 = create_eval_model(NNArchitectureVanillaSoftDupire,\n",
    "                                                                         scaledDataSet,\n",
    "                                                                         True,\n",
    "                                                                         hyperparameters,\n",
    "                                                                         modelName = \"regularizedConvexSoftDupireVolModel\")\n",
    "modelSummary(y_pred11, volLocale11, dNN_T11, gNN_K11, dataSet)\n",
    "impV11 = plotImpliedVol(y_pred11, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vLcb8QJdFdMy"
   },
   "outputs": [],
   "source": [
    "y_pred11Test, volLocale11Test, dNN_T11Test, gNN_K11Test, lossSerie11Test = create_eval_model(NNArchitectureVanillaSoftDupire,\n",
    "                                                                                             scaledDataSetTest,\n",
    "                                                                                             True,\n",
    "                                                                                             hyperparameters,\n",
    "                                                                                             modelName = \"regularizedConvexSoftDupireVolModel\")\n",
    "modelSummary(y_pred11Test, volLocale11Test, dNN_T11Test, gNN_K11Test, dataSetTest)\n",
    "impV11Test = plotImpliedVol(y_pred11Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaHXPun1ibbS"
   },
   "outputs": [],
   "source": [
    "modelSummary(y_pred11, \n",
    "             volLocale11, \n",
    "             dNN_T11, \n",
    "             gNN_K11, \n",
    "             dataSet,\n",
    "             logMoneynessScale = True)\n",
    "impV11 = plotImpliedVol(y_pred11, \n",
    "                        dataSet[\"ImpliedVol\"], \n",
    "                        rIntegralSpline=riskFreeIntegral, \n",
    "                        qIntegralSpline=divSpreadIntegral,\n",
    "                        logMoneynessScale = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dbXvzI1BXlRT"
   },
   "outputs": [],
   "source": [
    "modelSummary(y_pred11Test, \n",
    "             volLocale11Test, \n",
    "             dNN_T11Test, \n",
    "             gNN_K11Test, \n",
    "             dataSetTest,\n",
    "             logMoneynessScale = True)\n",
    "impV11Test = plotImpliedVol(y_pred11Test, \n",
    "                            dataSetTest[\"ImpliedVol\"], \n",
    "                            rIntegralSpline=riskFreeIntegral, \n",
    "                            qIntegralSpline=divSpreadIntegral,\n",
    "                            logMoneynessScale = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MO8Iv9_cH6fS"
   },
   "outputs": [],
   "source": [
    "priceTrain = convertToLogMoneyness(dataSet[dataSet.Maturity > 0])[\"Price\"]\n",
    "plot2Series(convertToLogMoneyness(dataSetTest[dataSetTest.Maturity > 0])[\"Price\"], \n",
    "            priceTrain[priceTrain < 1500],\n",
    "            Title = \"Reference Price Surfaces\",\n",
    "            yMin = -1000,\n",
    "            az=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BkukSDCmqTjh"
   },
   "outputs": [],
   "source": [
    "priceTrain = convertToLogMoneyness(dataSet[dataSet.Maturity > 0])[\"Price\"]\n",
    "plotSerie(convertToLogMoneyness(dataSet[dataSet.Maturity > 0])[\"Price\"],\n",
    "          Title = \"Reference Price Surfaces\",\n",
    "          yMin = -1000,\n",
    "          az=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a8BwviLCrfMt"
   },
   "outputs": [],
   "source": [
    "priceTrain = convertToLogMoneyness(dataSet[dataSet.Maturity > 0])[\"Price\"]\n",
    "plot2Series(convertToLogMoneyness(dataSetTest[dataSetTest.Maturity > 0])[\"Price\"], \n",
    "            priceTrain,\n",
    "            Title = \"\",\n",
    "            yMin = -1000,\n",
    "            az=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3OV7a-tVI2fK"
   },
   "outputs": [],
   "source": [
    "priceTrain = convertToLogMoneyness(y_pred11[y_pred11.index.get_level_values(\"Maturity\") > 0]) \n",
    "plot2Series(convertToLogMoneyness(y_pred11Test[y_pred11Test.index.get_level_values(\"Maturity\") > 0]), \n",
    "            priceTrain,\n",
    "            Title = '',\n",
    "            yMin = -1000,\n",
    "            az=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lnFQih4ZaYZ7"
   },
   "outputs": [],
   "source": [
    "priceTrain = convertToLogMoneyness(dataSet)[\"Price\"]\n",
    "plot2Series(convertToLogMoneyness(y_pred11Test), \n",
    "            priceTrain,\n",
    "            Title = '',\n",
    "            yMin = -1000,\n",
    "            az=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jAhDVOHRJDum"
   },
   "outputs": [],
   "source": [
    "volTrain = (impV11[impV11.index.get_level_values(\"Maturity\") > 0])\n",
    "plot2Series((impV11Test[impV11Test.index.get_level_values(\"Maturity\") > 0])[impV11Test > 0.05], \n",
    "            volTrain[volTrain < 0.3][volTrain > 0.05],\n",
    "            Title = \"Dense Soft Implied volatility Surfaces\",\n",
    "            yMin = -1000,\n",
    "            az=230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MoFYYHsKJDlf"
   },
   "outputs": [],
   "source": [
    "volTrain = convertToLogMoneyness((dataSet[\"ImpliedVol\"][dataSet[\"ImpliedVol\"].index.get_level_values(\"Maturity\") > 0]))\n",
    "plot2Series((impV11Test[impV11Test.index.get_level_values(\"Maturity\") > 0])[impV11Test > 0.05], #convertToLogMoneyness((dataSetTest[\"ImpliedVol\"][dataSetTest[\"ImpliedVol\"].index.get_level_values(\"Maturity\") > 0])), \n",
    "            volTrain[volTrain < 0.3],\n",
    "            Title = \"Dense Soft Implied volatility Surfaces\",\n",
    "            yMin = -1000,\n",
    "            az=230)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pMVAzDEKXe2Z"
   },
   "source": [
    "### Hard constrained architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hLkTjQUOXkOj"
   },
   "outputs": [],
   "source": [
    "y_pred12, volLocale12, dNN_T12, gNN_K12, lossSerie12 = create_train_model(NNArchitectureHardConstrainedDupire,\n",
    "                                                                          scaledDataSet,\n",
    "                                                                          True,\n",
    "                                                                          hyperparameters,\n",
    "                                                                          modelName = \"regularizedConvexHardDupireVolModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n9EwiocWXkLX"
   },
   "outputs": [],
   "source": [
    "plotEpochLoss(lossSerie12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQzB6KPmXkJB"
   },
   "outputs": [],
   "source": [
    "lossSerie12.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IDI9ydh3YvJz"
   },
   "outputs": [],
   "source": [
    "y_pred12, volLocale12, dNN_T12, gNN_K12, lossSerie12 = create_eval_model(NNArchitectureHardConstrainedDupire,\n",
    "                                                                         scaledDataSet,\n",
    "                                                                         True,\n",
    "                                                                         hyperparameters,\n",
    "                                                                         modelName = \"regularizedConvexHardDupireVolModel\")\n",
    "modelSummary(y_pred12, volLocale12, dNN_T12, gNN_K12, dataSet)\n",
    "impV12 = plotImpliedVol(y_pred12, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2548YwGSXj68"
   },
   "outputs": [],
   "source": [
    "y_pred12Test, volLocale12Test, dNN_T12Test, gNN_K12Test, lossSerie12Test = create_eval_model(NNArchitectureHardConstrainedDupire,\n",
    "                                                                                             scaledDataSetTest,\n",
    "                                                                                             True,\n",
    "                                                                                             hyperparameters,\n",
    "                                                                                             modelName = \"regularizedConvexHardDupireVolModel\")\n",
    "modelSummary(y_pred12Test, volLocale12Test, dNN_T12Test, gNN_K12Test, dataSetTest)\n",
    "impV12Test = plotImpliedVol(y_pred12Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xccdFPbTXj3n"
   },
   "outputs": [],
   "source": [
    "modelSummary(y_pred12, \n",
    "             volLocale12, \n",
    "             dNN_T12, \n",
    "             gNN_K12, \n",
    "             dataSet,\n",
    "             logMoneynessScale = True)\n",
    "impV12 = plotImpliedVol(y_pred12, \n",
    "                        dataSet[\"ImpliedVol\"], \n",
    "                        rIntegralSpline=riskFreeIntegral, \n",
    "                        qIntegralSpline=divSpreadIntegral,\n",
    "                        logMoneynessScale = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nj3ve-crXj1B"
   },
   "outputs": [],
   "source": [
    "modelSummary(y_pred12Test, \n",
    "             volLocale12Test, \n",
    "             dNN_T12Test, \n",
    "             gNN_K12Test, \n",
    "             dataSetTest,\n",
    "             logMoneynessScale = True)\n",
    "impV12Test = plotImpliedVol(y_pred12Test, \n",
    "                            dataSetTest[\"ImpliedVol\"], \n",
    "                            rIntegralSpline=riskFreeIntegral, \n",
    "                            qIntegralSpline=divSpreadIntegral,\n",
    "                            logMoneynessScale = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mu8QA9CmXjyg"
   },
   "outputs": [],
   "source": [
    "priceTrain = convertToLogMoneyness(y_pred12[y_pred12.index.get_level_values(\"Maturity\") > 0]) \n",
    "plot2Series(convertToLogMoneyness(y_pred12Test[y_pred12Test.index.get_level_values(\"Maturity\") > 0]), \n",
    "            priceTrain,\n",
    "            Title = '',\n",
    "            yMin = -1000,\n",
    "            az=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gr7G5r7JXjrJ"
   },
   "outputs": [],
   "source": [
    "volTrain = (impV12[impV12.index.get_level_values(\"Maturity\") > 0])\n",
    "plot2Series((impV12Test[impV12Test.index.get_level_values(\"Maturity\") > 0]), \n",
    "            volTrain[volTrain < 0.3],\n",
    "            Title = \"Hard Implied volatility Surfaces\",\n",
    "            yMin = -1000,\n",
    "            az=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TCvO25GfA2op"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DvLBkdlWjwzg"
   },
   "source": [
    "## Monte Carlo pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s8feoYmarxmj"
   },
   "source": [
    "### Monte Carlo with implied vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N9KS6Vkro1Rg"
   },
   "outputs": [],
   "source": [
    "nbTimeStep = 100\n",
    "nbPaths = 100000\n",
    "def MonteCarloPricerImplicit(S,\n",
    "                             Strike,\n",
    "                             Maturity,\n",
    "                             rSpline,\n",
    "                             divSpline,\n",
    "                             nbPaths,\n",
    "                             nbTimeStep,\n",
    "                             impliedVol):\n",
    "  time_grid = np.linspace(0, Maturity, int(nbTimeStep + 1))\n",
    "  timeStep = Maturity / nbTimeStep\n",
    "  gaussianNoise = np.random.normal(scale = np.sqrt(timeStep), size=(nbTimeStep, nbPaths))\n",
    "\n",
    "  logReturn = np.zeros((nbTimeStep + 1, nbPaths))\n",
    "  logReturn[0,:] = 0\n",
    "\n",
    "  for i in range(nbTimeStep) :\n",
    "      t = time_grid[i]\n",
    "\n",
    "      St = S0 * np.exp(logReturn[i,:])\n",
    "      volLocale = impliedVol\n",
    "\n",
    "      mu = rSpline(t) - divSpline(t)\n",
    "      drift = np.ones(nbPaths) * (mu - np.square(volLocale) / 2.0) \n",
    "      logReturn[i + 1, :] = logReturn[i,:] + drift * timeStep + gaussianNoise[i,:] * volLocale\n",
    "  SFinal = S0 * np.exp(logReturn[-1, :])\n",
    "  return np.mean(np.maximum(Strike - SFinal, 0))\n",
    "\n",
    "def MonteCarloPricerVectorizedImplicit(S,\n",
    "                                       dataSet,\n",
    "                                       rSpline,\n",
    "                                       divSpline,\n",
    "                                       nbPaths,\n",
    "                                       nbTimeStep):\n",
    "  func = lambda x : MonteCarloPricerImplicit(S, x[\"Strike\"], x[\"Maturity\"], riskCurvespline, divSpline, nbPaths, nbTimeStep, x[\"ImpliedVol\"])\n",
    "  return dataSet.apply(func, axis=1) * np.exp(-riskFreeIntegral(dataSet.index.get_level_values(\"Maturity\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q1E6lHkPpYXD"
   },
   "outputs": [],
   "source": [
    "mcResRef = MonteCarloPricerVectorizedImplicit(S0[0],\n",
    "                                              dataSet,\n",
    "                                              riskCurvespline,\n",
    "                                              divSpline,\n",
    "                                              nbPaths,\n",
    "                                              nbTimeStep)\n",
    "mcResRef.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iKWRzDyTpYTh"
   },
   "outputs": [],
   "source": [
    "predictionDiagnosis(mcResRef, dataSet[\"Price\"], \" Price \", yMin=4100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H8MJ9XlYr2My"
   },
   "source": [
    "### Monte Carlo local volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3n8evyIxsB2W"
   },
   "source": [
    "#### Constant local volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnZBKekABhBm"
   },
   "outputs": [],
   "source": [
    "def volLocaleTest(S, T):\n",
    "  return np.ones_like(S) * 0.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JdkQpONK4k7H"
   },
   "outputs": [],
   "source": [
    "nbTimeStep = 100\n",
    "nbPaths = 100000\n",
    "def MonteCarloPricer(S, \n",
    "                     Strike, \n",
    "                     Maturity, \n",
    "                     rSpline, \n",
    "                     divSpline, \n",
    "                     nbPaths, \n",
    "                     nbTimeStep, \n",
    "                     volLocaleFunction):\n",
    "  time_grid = np.linspace(0, Maturity, int(nbTimeStep + 1))\n",
    "  timeStep = Maturity / nbTimeStep\n",
    "  gaussianNoise = np.random.normal(scale = np.sqrt(timeStep), size=(nbTimeStep, nbPaths))\n",
    "\n",
    "  logReturn = np.zeros((nbTimeStep + 1, nbPaths))\n",
    "  logReturn[0,:] = 0\n",
    "\n",
    "  for i in range(nbTimeStep) :\n",
    "      t = time_grid[i]\n",
    "\n",
    "      St = S0 * np.exp(logReturn[i,:])\n",
    "      volLocale = volLocaleFunction(St, np.ones(nbPaths) * t)\n",
    "\n",
    "      mu = rSpline(t) - divSpline(t)\n",
    "      drift = np.ones(nbPaths) * (mu - np.square(volLocale) / 2.0) \n",
    "      logReturn[i + 1, :] = logReturn[i,:] + drift * timeStep + gaussianNoise[i,:] * volLocale\n",
    "  SFinal = S0 * np.exp(logReturn[-1, :])\n",
    "  return np.mean(np.maximum(Strike - SFinal, 0))\n",
    "\n",
    "def MonteCarloPricerVectorized(S, \n",
    "                               dataSet,\n",
    "                               rSpline, \n",
    "                               divSpline, \n",
    "                               nbPaths, \n",
    "                               nbTimeStep, \n",
    "                               volLocaleFunction):\n",
    "  func = lambda x : MonteCarloPricer(S, x[\"Strike\"], x[\"Maturity\"], riskCurvespline, divSpline, nbPaths, nbTimeStep, volLocaleFunction)\n",
    "  return dataSet.apply(func, axis=1) * np.exp(-riskFreeIntegral(dataSet.index.get_level_values(\"Maturity\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mYHvsquue2JO"
   },
   "outputs": [],
   "source": [
    "mcResSigmaRef = MonteCarloPricerVectorized(S0[0],\n",
    "                                           dataSetTest,\n",
    "                                           riskCurvespline,\n",
    "                                           divSpline,\n",
    "                                           nbPaths,\n",
    "                                           nbTimeStep,\n",
    "                                           volLocaleTest)\n",
    "mcResSigmaRef.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jJR0sXsUmCm2"
   },
   "outputs": [],
   "source": [
    "predictionDiagnosis(mcResSigmaRef, dataSetTest[\"Price\"], \" Price \", yMin=4100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ltaanlj5-uTA"
   },
   "outputs": [],
   "source": [
    "mcResSigmaRef.to_csv(\"mcResSigmaRef.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ysLo9wxsTOj"
   },
   "source": [
    "#### Extracting neural local volatility\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MV4SmwctjbjS"
   },
   "outputs": [],
   "source": [
    "def evalVolLocale(NNFactory,\n",
    "                  strikes,\n",
    "                  maturities,\n",
    "                  dataSet,\n",
    "                  hyperParameters,\n",
    "                  modelName = \"bestModel\"):\n",
    "    \n",
    "    hidden_nodes = hyperParameters[\"nbUnits\"] \n",
    "\n",
    "    # Reset the graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Placeholders for input and output data   \n",
    "    Strike = tf.placeholder(tf.float32,[None,1])\n",
    "    Maturity = tf.placeholder(tf.float32,[None,1])\n",
    "    factorPrice = tf.placeholder(tf.float32,[None,1])\n",
    "    y = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='y')\n",
    "    vegaRef = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='vegaRef')\n",
    "    learningRateTensor = tf.placeholder(tf.float32,[])\n",
    "    \n",
    "    #Get scaling for strike\n",
    "    colStrikeIndex = dataSet.columns.get_loc(\"ChangedStrike\")\n",
    "    maxColFunction = scaler.data_max_[colStrikeIndex]\n",
    "    minColFunction = scaler.data_min_[colStrikeIndex]\n",
    "    scF = (maxColFunction - minColFunction) \n",
    "    scaleTensor = tf.constant(scF, dtype=tf.float32)\n",
    "    strikeMinTensor = tf.constant(minColFunction, dtype=tf.float32)\n",
    "\n",
    "    price_pred_tensor = None\n",
    "    TensorList = None\n",
    "    penalizationList = None \n",
    "    formattingFunction = None\n",
    "    price_pred_tensor, TensorList, penalizationList, formattingFunction = NNFactory(hidden_nodes,\n",
    "                                                                                    Strike,\n",
    "                                                                                    Maturity,\n",
    "                                                                                    scaleTensor,\n",
    "                                                                                    strikeMinTensor,\n",
    "                                                                                    vegaRef,\n",
    "                                                                                    hyperParameters,\n",
    "                                                                                    IsTraining=False)# one hidden layer\n",
    "\n",
    "\n",
    "    price_pred_tensor_sc= tf.multiply(factorPrice,price_pred_tensor)\n",
    "    TensorList[0] = price_pred_tensor_sc\n",
    "    \n",
    "    # Define a loss function\n",
    "    pointwiseError = tf.reduce_mean(tf.abs(price_pred_tensor_sc - y) / vegaRef)\n",
    "    errors = tf.add_n([pointwiseError] + penalizationList) \n",
    "    loss = tf.log(tf.reduce_mean(errors))\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRateTensor)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    # Initialize variables and run session\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    n = strikes.shape[0]\n",
    "    changedVar = changeOfVariable(strikes, maturities)\n",
    "    scaledStrike = (changedVar[0]-minColFunction)/scF\n",
    "    dividendFactor = changedVar[1]\n",
    "\n",
    "    def createFeedDict(s, t, d):\n",
    "        batchSize = s.shape[0]\n",
    "        feedDict = {Strike : np.reshape(s, (batchSize,1)), \n",
    "                    Maturity : np.reshape(t, (batchSize,1)) ,  \n",
    "                    factorPrice : np.reshape(d, (batchSize,1)), \n",
    "                    vegaRef : np.ones((batchSize,1))}\n",
    "        return feedDict\n",
    "    \n",
    "    epochFeedDict = createFeedDict(scaledStrike, maturities, dividendFactor)\n",
    "    \n",
    "    saver.restore(sess, modelName)  \n",
    "\n",
    "    evalList = sess.run(TensorList, feed_dict=epochFeedDict)\n",
    "    \n",
    "    sess.close()\n",
    "    \n",
    "    return pd.Series(evalList[1].flatten(), index = pd.MultiIndex.from_arrays([strikes, maturities], names=('Strike', 'Maturity')))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bG_opQ7DynZC"
   },
   "outputs": [],
   "source": [
    "strikeLow = dataSet[\"Strike\"].min()#min(dataSet[\"Strike\"].min(),dataSetTest[\"Strike\"].min())\n",
    "strikeUp = dataSet[\"Strike\"].max()#max(dataSet[\"Strike\"].max(),dataSetTest[\"Strike\"].max())\n",
    "strikeGrid = np.linspace(strikeLow, strikeUp, 100)\n",
    "matLow = dataSet[\"Maturity\"].min()#min(dataSet[\"Maturity\"].min(),dataSetTest[\"Maturity\"].min())\n",
    "matUp = dataSet[\"Maturity\"].max()#max(dataSet[\"Maturity\"].max(),dataSetTest[\"Maturity\"].max())\n",
    "matGrid = np.linspace(matLow, matUp, 100)\n",
    "volLocaleGrid = np.meshgrid(strikeGrid, matGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4Yd7wL-ftbr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oawxG7p4oGUA"
   },
   "outputs": [],
   "source": [
    "def interpolatedMCLocalVolatility(localVol, strikes, maturities):\n",
    "    coordinates =  np.array( customInterpolator(localVol, strikes, maturities) ).flatten()  \n",
    "    return pd.Series(coordinates, index = pd.MultiIndex.from_arrays([strikes, maturities], names=('Strike', 'Maturity')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rRwNc1rgM28W"
   },
   "source": [
    "##### Standard network, soft constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QBw5bnz6QKTQ"
   },
   "outputs": [],
   "source": [
    "def neuralVolLocale(s,t):\n",
    "  vLoc = evalVolLocale(NNArchitectureVanillaSoftDupire,\n",
    "                       s, t,\n",
    "                       dataSetTest,\n",
    "                       hyperparameters,\n",
    "                       modelName = \"convexSoftDupireVolModel\")\n",
    "  return vLoc.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UZwCXR8lzaQa"
   },
   "outputs": [],
   "source": [
    "volLocalInterp = neuralVolLocale(volLocaleGrid[0].flatten(), \n",
    "                                 volLocaleGrid[1].flatten())\n",
    "volLocalInterp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9q0UQuuH3OHn"
   },
   "outputs": [],
   "source": [
    "volLocalInterp.to_csv(\"Dense08082001VolLocalGrid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OUVTy-tl9HBE"
   },
   "outputs": [],
   "source": [
    "volLocalInterp2 = neuralVolLocale(dataSetTest.index.get_level_values(\"Strike\").values.flatten(), \n",
    "                                  dataSetTest.index.get_level_values(\"Maturity\").values.flatten())\n",
    "volLocalInterp2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-8dutn-w3b0Y"
   },
   "outputs": [],
   "source": [
    "volLocalInterp.to_csv(\"Dense08082001dataSetTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m5rPNAOtNLGn"
   },
   "outputs": [],
   "source": [
    "nnVolLocale = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Amwp9SY-D1o"
   },
   "outputs": [],
   "source": [
    "nnVolLocale2 = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp2, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPeQZ53f0L46"
   },
   "outputs": [],
   "source": [
    "plotSerie(volLocalInterp,\n",
    "          Title = 'Interpolated Local Volatility Surface',\n",
    "          az=30,\n",
    "          yMin=0.0*S0,\n",
    "          yMax=2.0*S0, \n",
    "          zAsPercent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k8OZjFTHezNT"
   },
   "outputs": [],
   "source": [
    "\n",
    "plotSerie(nnVolLocale2(volLocaleGrid[0].flatten(), volLocaleGrid[1].flatten()),\n",
    "          Title = 'Interpolated Local Volatility Surface',\n",
    "          az=30,\n",
    "          yMin=0.0*S0,\n",
    "          yMax=2.0*S0, \n",
    "          zAsPercent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-VTQGuG49sv_"
   },
   "outputs": [],
   "source": [
    "plotSerie(volLocalInterp2,\n",
    "          Title = 'Testing Interpolated Local Volatility Surface',\n",
    "          az=30,\n",
    "          yMin=0.0*S0,\n",
    "          yMax=2.0*S0, \n",
    "          zAsPercent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_osDqp2mJbz"
   },
   "outputs": [],
   "source": [
    "plotSerie(dataSetTest[\"locvol\"],\n",
    "          Title = 'Testing Tikhonov Local Volatility Surface',\n",
    "          az=30,\n",
    "          yMin=0.0*S0,\n",
    "          yMax=2.0*S0, \n",
    "          zAsPercent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9zAPjVEiEJk7"
   },
   "outputs": [],
   "source": [
    "plotSerie(dataSet[\"locvol\"],\n",
    "          Title = 'Tikohnov Train Local Volatility Surface',\n",
    "          az=30,\n",
    "          yMin=0.0*S0,\n",
    "          yMax=2.0*S0, \n",
    "          zAsPercent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_xd_f6kpj6R"
   },
   "outputs": [],
   "source": [
    "plotSerie(neuralVolLocale(dataSet.index.get_level_values(\"Strike\").values.flatten(), dataSet.index.get_level_values(\"Maturity\").values.flatten()),\n",
    "          Title = 'Training Local Volatility Surface',\n",
    "          az=30,\n",
    "          yMin=0.0*S0,\n",
    "          yMax=2.0*S0, \n",
    "          zAsPercent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cd5HBrfIgdR7"
   },
   "outputs": [],
   "source": [
    "plotSerie(localVolatility[\"LocalVolatility\"],\n",
    "          Title = 'Complete tikhonov Local Volatility Surface',\n",
    "          az=30,\n",
    "          yMin=0.0*S0,\n",
    "          yMax=2.0*S0, \n",
    "          zAsPercent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dB--pSuegkg6"
   },
   "outputs": [],
   "source": [
    "localVolatility.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_sdUIUb_MwVJ"
   },
   "source": [
    "##### Hard constraint Regularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4WESqBRzRGyt"
   },
   "outputs": [],
   "source": [
    "def neuralVolLocaleHardReg(s,t):\n",
    "  vLoc = evalVolLocale(NNArchitectureHardConstrainedDupire,\n",
    "                       s, t,\n",
    "                       dataSet,\n",
    "                       hyperparameters,\n",
    "                       modelName = \"regularizedConvexHardDupireVolModel\")\n",
    "  return vLoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "coZ0NmLduoR7"
   },
   "outputs": [],
   "source": [
    "volLocalInterp3 = neuralVolLocaleHardReg(volLocaleGrid[0].flatten(),\n",
    "                                         volLocaleGrid[1].flatten())\n",
    "volLocalInterp3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rlLZq1p6RXc-"
   },
   "outputs": [],
   "source": [
    "volLocalInterp4 = neuralVolLocaleHardReg(dataSetTest.index.get_level_values(\"Strike\").values.flatten(),\n",
    "                                         dataSetTest.index.get_level_values(\"Maturity\").values.flatten())\n",
    "volLocalInterp4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bHQgKjjMRXZe"
   },
   "outputs": [],
   "source": [
    "nnVolLocale3 = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp3, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMy6YZzmRXXX"
   },
   "outputs": [],
   "source": [
    "nnVolLocale4 = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp4, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bl72KtNERXT_"
   },
   "outputs": [],
   "source": [
    "plotSerie(volLocalInterp3,\n",
    "          Title = 'Interpolated Local Volatility Surface',\n",
    "          az=30,\n",
    "          yMin=0.0*S0,\n",
    "          yMax=2.0*S0, \n",
    "          zAsPercent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ILzOqJ0SRXOE"
   },
   "outputs": [],
   "source": [
    "plotSerie(volLocalInterp4,\n",
    "          Title = 'Interpolated Local Volatility Surface',\n",
    "          az=30,\n",
    "          yMin=0.0*S0,\n",
    "          yMax=2.0*S0, \n",
    "          zAsPercent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S9167YBhRpIY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_RU2VfS9UcAN"
   },
   "source": [
    "##### Hard constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_fdam7WpUcAS"
   },
   "outputs": [],
   "source": [
    "def neuralVolLocaleHard(s,t):\n",
    "  vLoc = evalVolLocale(NNArchitectureHardConstrainedDupire,\n",
    "                       s, t,\n",
    "                       dataSet,\n",
    "                       hyperparameters,\n",
    "                       modelName = \"convexHardDupireVolModel\")\n",
    "  return vLoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42NAiQ7OUcAd"
   },
   "outputs": [],
   "source": [
    "volLocalInterp5 = neuralVolLocaleHard(volLocaleGrid[0].flatten(),\n",
    "                                      volLocaleGrid[1].flatten())\n",
    "volLocalInterp5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1NZ9rNwyUcAj"
   },
   "outputs": [],
   "source": [
    "volLocalInterp6 = neuralVolLocaleHard(dataSetTest.index.get_level_values(\"Strike\").values.flatten(),\n",
    "                                      dataSetTest.index.get_level_values(\"Maturity\").values.flatten())\n",
    "volLocalInterp6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UjY8_upSUcAn"
   },
   "outputs": [],
   "source": [
    "nnVolLocale5 = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp5, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A2uShCIhUcAr"
   },
   "outputs": [],
   "source": [
    "nnVolLocale6 = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp6, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yDLVxVCsUcAu"
   },
   "outputs": [],
   "source": [
    "plotSerie(volLocalInterp5,\n",
    "          Title = 'Interpolated Local Volatility Surface',\n",
    "          az=30,\n",
    "          yMin=0.0*S0,\n",
    "          yMax=2.0*S0, \n",
    "          zAsPercent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4F5jBRtdUcAw"
   },
   "outputs": [],
   "source": [
    "plotSerie(volLocalInterp6,\n",
    "          Title = 'Interpolated Local Volatility Surface',\n",
    "          az=30,\n",
    "          yMin=0.0*S0,\n",
    "          yMax=2.0*S0, \n",
    "          zAsPercent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZojUS1lnUcAz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CslgrmvEUcA1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w96mzVNxRpFw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LIWdTVy8tf59"
   },
   "source": [
    "#### Tikhonov local volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "odpuxjDbtlV0"
   },
   "outputs": [],
   "source": [
    "nnTikhonov = lambda x,y : interpolatedMCLocalVolatility(localVolatility[\"LocalVolatility\"], x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cP0LG52vtlQ5"
   },
   "outputs": [],
   "source": [
    "mcResTikhonov = MonteCarloPricerVectorized(S0[0],\n",
    "                                           dataSetTest,\n",
    "                                           riskCurvespline,\n",
    "                                           divSpline,\n",
    "                                           nbPaths,\n",
    "                                           nbTimeStep,\n",
    "                                           nnTikhonov)\n",
    "mcResTikhonov.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uH2OxQ0OtlNd"
   },
   "outputs": [],
   "source": [
    "predictionDiagnosis(mcResTikhonov, dataSetTest[\"Price\"], \" Price \", yMin=4100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t65wX7-xtlKU"
   },
   "outputs": [],
   "source": [
    "mcResTikhonov.to_csv(\"mcResTikhonov.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BSe4AmUJsLo-"
   },
   "source": [
    "#### Neural local Volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z3Hrt5KOVt9S"
   },
   "source": [
    "##### Standard Network soft constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T0DdvwGtdKPZ"
   },
   "outputs": [],
   "source": [
    "mcResVolLocale = MonteCarloPricerVectorized(S0[0],\n",
    "                                            dataSetTest,\n",
    "                                            riskCurvespline,\n",
    "                                            divSpline,\n",
    "                                            nbPaths,\n",
    "                                            nbTimeStep,\n",
    "                                            nnVolLocale)\n",
    "mcResVolLocale.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b2NqMe9KEb4r"
   },
   "outputs": [],
   "source": [
    "mcResVolLocale.to_csv(\"mcResVolLocale.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aAslenyljg_w"
   },
   "outputs": [],
   "source": [
    "predictionDiagnosis(mcResVolLocale, dataSetTet[\"Price\"], \" Price \", yMin=4100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "38OJqpkCfYaI"
   },
   "outputs": [],
   "source": [
    "dataSetTest.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0eW1ICq6-RZ3"
   },
   "outputs": [],
   "source": [
    "mcResVolLocale2 = MonteCarloPricerVectorized(S0[0],\n",
    "                                            dataSetTest,\n",
    "                                            riskCurvespline,\n",
    "                                            divSpline,\n",
    "                                            nbPaths,\n",
    "                                            nbTimeStep,\n",
    "                                            nnVolLocale2)\n",
    "mcResVolLocale2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jNmkLtZE-RPA"
   },
   "outputs": [],
   "source": [
    "predictionDiagnosis(mcResVolLocale2, dataSetTest[\"Price\"], \" Price \", yMin=4100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "demJXGlD6V3U"
   },
   "outputs": [],
   "source": [
    "mcResVolLocale2.to_csv(\"mcResVolLocale2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pwupw4shWcqI"
   },
   "source": [
    "##### Hard constraint Regularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HtFZpARdWcqN"
   },
   "outputs": [],
   "source": [
    "mcResVolLocale3 = MonteCarloPricerVectorized(S0[0],\n",
    "                                             dataSetTest,\n",
    "                                             riskCurvespline,\n",
    "                                             divSpline,\n",
    "                                             nbPaths,\n",
    "                                             nbTimeStep,\n",
    "                                             nnVolLocale3)\n",
    "mcResVolLocale3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4JQvapLCWcqS"
   },
   "outputs": [],
   "source": [
    "predictionDiagnosis(mcResVolLocale3, dataSetTest[\"Price\"], \" Price \", yMin=4100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FoR4evV3Wcqd"
   },
   "outputs": [],
   "source": [
    "mcResVolLocale3.to_csv(\"mcResVolLocale3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RbgkiRPeWcqY"
   },
   "outputs": [],
   "source": [
    "mcResVolLocale4 = MonteCarloPricerVectorized(S0[0],\n",
    "                                             dataSetTest,\n",
    "                                             riskCurvespline,\n",
    "                                             divSpline,\n",
    "                                             nbPaths,\n",
    "                                             nbTimeStep,\n",
    "                                             nnVolLocale4)\n",
    "mcResVolLocale4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sb6dt-iJWcqa"
   },
   "outputs": [],
   "source": [
    "predictionDiagnosis(mcResVolLocale4, dataSetTest[\"Price\"], \" Price \", yMin=4100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JTIQMwhREWsz"
   },
   "outputs": [],
   "source": [
    "mcResVolLocale4.to_csv(\"mcResVolLocale4.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z45IZtNnWfek"
   },
   "source": [
    "##### Hard constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2vCtUGRNWfel"
   },
   "outputs": [],
   "source": [
    "mcResVolLocale5 = MonteCarloPricerVectorized(S0[0],\n",
    "                                             dataSetTest,\n",
    "                                             riskCurvespline,\n",
    "                                             divSpline,\n",
    "                                             nbPaths,\n",
    "                                             nbTimeStep,\n",
    "                                             nnVolLocale5)\n",
    "mcResVolLocale5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1h3Ee3CcWfen"
   },
   "outputs": [],
   "source": [
    "predictionDiagnosis(mcResVolLocale5, dataSetTest[\"Price\"], \" Price \", yMin=4100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fFrj8iUVBVZx"
   },
   "outputs": [],
   "source": [
    "mcResVolLocale5.to_csv(\"mcResVolLocale5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XudXGmowWffz"
   },
   "outputs": [],
   "source": [
    "mcResVolLocale6 = MonteCarloPricerVectorized(S0[0],\n",
    "                                             dataSetTest,\n",
    "                                             riskCurvespline,\n",
    "                                             divSpline,\n",
    "                                             nbPaths,\n",
    "                                             nbTimeStep,\n",
    "                                             nnVolLocale6)\n",
    "mcResVolLocale6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oz5Rx-INWff1"
   },
   "outputs": [],
   "source": [
    "predictionDiagnosis(mcResVolLocale6, dataSetTest[\"Price\"], \" Price \", yMin=4100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N0HxLb3apD4l"
   },
   "outputs": [],
   "source": [
    "mcResVolLocale6.to_csv(\"mcResVolLocale6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V4XXpLmjWff3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "39dhPnBxW-tI"
   },
   "source": [
    "## Gatheral transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LtubjBVY3vGh"
   },
   "source": [
    "#### Select Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DvG6upPF3yo0"
   },
   "outputs": [],
   "source": [
    "trainingDataSet = generateData(formattedTrainingData[\"ImpliedVol\"], S0, riskFreeIntegral, divSpreadIntegral, riskCurvespline, divSpline)\n",
    "trainingDataSet.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ezNbsoYYP1II"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p5mydcuC3-uA"
   },
   "outputs": [],
   "source": [
    "trainingDataSet[\"locvol\"] = interpolatedLocalVolatility(localVolatility, trainingDataSet[\"Price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1FVxFB8Y4Dma"
   },
   "outputs": [],
   "source": [
    "dataSet = trainingDataSet #Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kC9cAGL14QjO"
   },
   "outputs": [],
   "source": [
    "scaler = skl.preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(dataSet)\n",
    "scaledDataSet = transformCustom(dataSet, scaler)\n",
    "scaledDataSetTest = transformCustom(dataSetTest, scaler)\n",
    "scaledDataSet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-wR4X6rz4TY1"
   },
   "outputs": [],
   "source": [
    "#Search strike for ATM option\n",
    "midS0 = dataSet[dataSet.index.get_level_values(\"Strike\") >= S0[0]].index.get_level_values(\"Strike\").min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hwG8y4zzVlLj"
   },
   "source": [
    "#### Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CD9EbLyF0Eyw"
   },
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "#Diagnose Price, theta, gamma and local volatility\n",
    "def modelSummaryGatheral(totalVariance,\n",
    "                         volLocale,\n",
    "                         delta_T,\n",
    "                         gamma_K,\n",
    "                         benchDataset,\n",
    "                         sigma=0.3,\n",
    "                         az=40,\n",
    "                         yMin = KMin,\n",
    "                         yMax = KMax,\n",
    "                         logMoneynessScale = False):\n",
    "  refDataset = benchDataset.loc[totalVariance.index]\n",
    "  if logMoneynessScale : \n",
    "    totalVariancePred = convertToLogMoneyness(totalVariance)\n",
    "    volLocalePred = convertToLogMoneyness(volLocale)\n",
    "    delta_TPred = convertToLogMoneyness(delta_T)\n",
    "    gKRefPred = convertToLogMoneyness(gamma_K)\n",
    "    benchDatasetScaled = convertToLogMoneyness(refDataset)\n",
    "    yMinScaled = np.log(S0[0]/yMax)\n",
    "    yMaxScaled = np.log(S0[0]/yMin)\n",
    "    azimutIncrement = 180\n",
    "  else : \n",
    "    totalVariancePred = totalVariance\n",
    "    volLocalePred = volLocale\n",
    "    delta_TPred = delta_T\n",
    "    gKRefPred = gamma_K\n",
    "    benchDatasetScaled = refDataset\n",
    "    yMinScaled = yMin\n",
    "    yMaxScaled = yMax\n",
    "    azimutIncrement = 0\n",
    "    \n",
    "  priceRef = benchDatasetScaled[\"impliedTotalVariance\"]\n",
    "  predictionDiagnosis(totalVariancePred, \n",
    "                      priceRef, \n",
    "                      \"Implied Variance\",\n",
    "                      az=320 + azimutIncrement,\n",
    "                      yMin = yMinScaled,\n",
    "                      yMax = yMaxScaled)\n",
    "  \n",
    "  volLocaleRef = benchDatasetScaled[\"locvol\"]\n",
    "  predictionDiagnosis(volLocalePred, \n",
    "                      volLocaleRef, \n",
    "                      \"Local volatility\",\n",
    "                      az=az + azimutIncrement,\n",
    "                      yMin = yMinScaled,\n",
    "                      yMax = yMaxScaled)\n",
    "  \n",
    "  impliedVolPred = np.sqrt(totalVariancePred) #np.sqrt(totalVariance / refDataset[\"Maturity\"])\n",
    "  predictionDiagnosis(impliedVolPred, \n",
    "                      benchDatasetScaled[\"ImpliedVol\"], \n",
    "                      \"Implied volatility\",\n",
    "                      az=az + azimutIncrement,\n",
    "                      yMin = yMinScaled,\n",
    "                      yMax = yMaxScaled)\n",
    "  \n",
    "  dTRef = benchDatasetScaled[\"Theta\"]\n",
    "  predictionDiagnosis(delta_TPred, \n",
    "                      dTRef, \n",
    "                      \"Theta\",\n",
    "                      az=340 + azimutIncrement,\n",
    "                      yMin = yMinScaled,\n",
    "                      yMax = yMaxScaled)\n",
    "  \n",
    "  gKRef = benchDatasetScaled[\"Gamma Strike\"]\n",
    "  predictionDiagnosis(gKRefPred, \n",
    "                      gKRef, \n",
    "                      \"Gamma Strike\",\n",
    "                      az=340 + azimutIncrement,\n",
    "                      yMin = yMinScaled,\n",
    "                      yMax = yMaxScaled)\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3sie49ZXVqJA"
   },
   "source": [
    "#### Execution functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KDEyyFW0XEjg"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Train neural network with a decreasing rule for learning rate\n",
    "#NNFactory :  function creating the architecture\n",
    "#dataSet : training data\n",
    "#activateRegularization : boolean, if true add bound penalization to dupire variance\n",
    "#hyperparameters : dictionnary containing various hyperparameters\n",
    "#modelName : name under which tensorflow model is saved\n",
    "def create_train_model_gatheral(NNFactory, \n",
    "                                dataSet, \n",
    "                                activateRegularization, \n",
    "                                hyperparameters,\n",
    "                                modelName = \"bestModel\"):\n",
    "    hidden_nodes = hyperparameters[\"nbUnits\"] \n",
    "    nbEpoch = hyperparameters[\"maxEpoch\"] \n",
    "    fixedLearningRate = (None if hyperparameters[\"FixedLearningRate\"] else hyperparameters[\"LearningRateStart\"])\n",
    "    patience = hyperparameters[\"Patience\"]\n",
    "    \n",
    "    # Go through num_iters iterations (ignoring mini-batching)\n",
    "    activateLearningDecrease = (~ hyperparameters[\"FixedLearningRate\"])\n",
    "    learningRate = hyperparameters[\"LearningRateStart\"]\n",
    "    learningRateEpoch = 0\n",
    "    finalLearningRate = hyperparameters[\"FinalLearningRate\"]\n",
    "\n",
    "    batch_size = hyperparameters[\"batchSize\"]\n",
    "\n",
    "    start = time.time()\n",
    "    # Reset the graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Placeholders for input and output data   \n",
    "    Moneyness = tf.placeholder(tf.float32,[None,1])\n",
    "    Maturity = tf.placeholder(tf.float32,[None,1])\n",
    "    y = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='y')\n",
    "    vegaRef = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='vegaRef')\n",
    "    learningRateTensor = tf.placeholder(tf.float32,[])\n",
    "    \n",
    "    #Get scaling for strike\n",
    "    colMoneynessIndex = dataSet.columns.get_loc(\"logMoneyness\")\n",
    "    maxColFunction = scaler.data_max_[colMoneynessIndex]\n",
    "    minColFunction = scaler.data_min_[colMoneynessIndex]\n",
    "    scF = (maxColFunction - minColFunction) \n",
    "    scaleTensor = tf.constant(scF, dtype=tf.float32)\n",
    "    moneynessMinTensor = tf.constant(minColFunction, dtype=tf.float32)\n",
    "\n",
    "    price_pred_tensor = None\n",
    "    TensorList = None\n",
    "    penalizationList = None \n",
    "    formattingFunction = None\n",
    "    if activateRegularization : #Add pseudo local volatility regularisation\n",
    "        vol_pred_tensor, TensorList, penalizationList, formattingFunction = addDupireRegularisation( *NNFactory(hidden_nodes,\n",
    "                                                                                                                Moneyness,\n",
    "                                                                                                                Maturity, \n",
    "                                                                                                                scaleTensor, \n",
    "                                                                                                                moneynessMinTensor, \n",
    "                                                                                                                vegaRef, \n",
    "                                                                                                                hyperparameters) ,\n",
    "                                                                                                    vegaRef, \n",
    "                                                                                                    hyperparameters)\n",
    "    else :\n",
    "        vol_pred_tensor, TensorList, penalizationList, formattingFunction = NNFactory(hidden_nodes,\n",
    "                                                                                      Moneyness, \n",
    "                                                                                      Maturity, \n",
    "                                                                                      scaleTensor, \n",
    "                                                                                      moneynessMinTensor, \n",
    "                                                                                      vegaRef, \n",
    "                                                                                      hyperparameters)\n",
    "\n",
    "    vol_pred_tensor_sc= vol_pred_tensor\n",
    "    TensorList[0] = vol_pred_tensor_sc\n",
    "    \n",
    "    # Define a loss function\n",
    "    pointwiseError = tf.reduce_mean(tf.abs(vol_pred_tensor_sc - y) / vegaRef)\n",
    "    errors = tf.add_n([pointwiseError] + penalizationList) \n",
    "    loss = tf.log(tf.reduce_mean(errors))\n",
    "\n",
    "\n",
    "\n",
    "    # Define a train operation to minimize the loss\n",
    "    lr = learningRate\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRateTensor)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    # Initialize variables and run session\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    n = dataSet.shape[0]\n",
    "    scaledInput = transformCustomMinMax(dataSet, scaler)\n",
    "\n",
    "    \n",
    "    maturity = dataSet[\"Maturity\"].values.reshape(n,1)\n",
    "    loss_serie = []\n",
    "\n",
    "    def createFeedDict(batch):\n",
    "        batchSize = batch.shape[0]\n",
    "        feedDict = {Moneyness : scaledInput[\"logMoneyness\"].loc[batch.index].values.reshape(batchSize,1),\n",
    "                    Maturity : batch[\"Maturity\"].values.reshape(batchSize,1), \n",
    "                    y : batch[\"impliedTotalVariance\"].values.reshape(batchSize,1),\n",
    "                    learningRateTensor : learningRate,\n",
    "                    vegaRef : np.ones_like(batch[\"VegaRef\"].values.reshape(batchSize,1))}\n",
    "        return feedDict\n",
    "\n",
    "    #Learning rate is divided by 10 if no imporvement is observed for training loss after \"patience\" epochs\n",
    "    def updateLearningRate(iterNumber, lr, lrEpoch):\n",
    "        if not activateLearningDecrease :\n",
    "            print(\"Constant learning rate, stop training\")\n",
    "            return False, lr, lrEpoch\n",
    "        if learningRate > finalLearningRate :\n",
    "            lr *= 0.1\n",
    "            lrEpoch = iterNumber\n",
    "            saver.restore(sess, modelName)\n",
    "            print(\"Iteration : \", lrEpoch, \"new learning rate : \", lr)\n",
    "        else :\n",
    "          print(\"Last Iteration : \", lrEpoch, \"final learning rate : \", lr)\n",
    "          return False, lr, lrEpoch\n",
    "        return True, lr, lrEpoch\n",
    "    \n",
    "    epochFeedDict = createFeedDict(dataSet)\n",
    "\n",
    "    def evalBestModel():\n",
    "        if not activateLearningDecrease :\n",
    "            print(\"Learning rate : \", learningRate, \" final loss : \", min(loss_serie))\n",
    "        currentBestLoss = sess.run(loss, feed_dict=epochFeedDict)\n",
    "        currentBestPenalizations = sess.run([pointwiseError, penalizationList], feed_dict=epochFeedDict)\n",
    "        print(\"Best loss (hidden nodes: %d, iterations: %d): %.2f\" % (hidden_nodes, len(loss_serie), currentBestLoss))\n",
    "        print(\"Best Penalization : \", currentBestPenalizations)\n",
    "        return\n",
    "    \n",
    "    for i in range(nbEpoch):\n",
    "        miniBatchList = [dataSet]\n",
    "        penalizationResult = sess.run(penalizationList, feed_dict=epochFeedDict)\n",
    "        lossResult = sess.run(pointwiseError, feed_dict=epochFeedDict)\n",
    "\n",
    "        #miniBatchList = selectMiniBatchWithoutReplacement(dataSet, batch_size)\n",
    "        for k in range(len(miniBatchList)) :\n",
    "            batchFeedDict = createFeedDict(miniBatchList[k])\n",
    "            sess.run(train, feed_dict=batchFeedDict)\n",
    "        \n",
    "        \n",
    "        loss_serie.append(sess.run(loss, feed_dict=epochFeedDict))\n",
    "\n",
    "        if (len(loss_serie) < 2) or (loss_serie[-1] <= min(loss_serie)):\n",
    "          #Save model as model is improved\n",
    "          saver.save(sess, modelName)\n",
    "        if (np.isnan(loss_serie[-1]) or  #Unstable model\n",
    "            ( (i-learningRateEpoch >= patience) and (min(loss_serie[-patience:]) > min(loss_serie)) ) ) : #No improvement for training loss during the latest 100 iterations\n",
    "          continueTraining, learningRate, learningRateEpoch = updateLearningRate(i, learningRate, learningRateEpoch)\n",
    "          if continueTraining :\n",
    "            evalBestModel()\n",
    "          else :\n",
    "            break\n",
    "    saver.restore(sess, modelName)  \n",
    "    \n",
    "    evalBestModel()\n",
    "\n",
    "    evalList  = sess.run(TensorList, feed_dict=epochFeedDict)\n",
    "    \n",
    "    sess.close()\n",
    "    end = time.time()\n",
    "    print(\"Training Time : \", end - start)\n",
    "    \n",
    "    return formattingFunction(*evalList, loss_serie, dataSet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHytoxyBXEhA"
   },
   "outputs": [],
   "source": [
    "#Evaluate neural network without training, it restores parameters obtained from a pretrained model \n",
    "#NNFactory :  function creating the neural architecture\n",
    "#dataSet : dataset on which neural network is evaluated \n",
    "#activateRegularization : boolean, if true add bound penalization for dupire variance\n",
    "#hyperparameters : dictionnary containing various hyperparameters\n",
    "#modelName : name of tensorflow model to restore\n",
    "def create_eval_model_gatheral(NNFactory, \n",
    "                               dataSet, \n",
    "                               activateRegularization, \n",
    "                               hyperparameters,\n",
    "                               modelName = \"bestModel\"):\n",
    "    hidden_nodes = hyperparameters[\"nbUnits\"] \n",
    "    \n",
    "    # Go through num_iters iterations (ignoring mini-batching)\n",
    "    activateLearningDecrease = (~ hyperparameters[\"FixedLearningRate\"])\n",
    "    learningRate = hyperparameters[\"LearningRateStart\"]\n",
    "\n",
    "    # Reset the graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Placeholders for input and output data   \n",
    "    Moneyness = tf.placeholder(tf.float32,[None,1])\n",
    "    Maturity = tf.placeholder(tf.float32,[None,1])\n",
    "    y = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='y')\n",
    "    vegaRef = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='vegaRef')\n",
    "    learningRateTensor = tf.placeholder(tf.float32,[])\n",
    "    \n",
    "    #Get scaling for strike\n",
    "    colMoneynessIndex = dataSet.columns.get_loc(\"logMoneyness\")\n",
    "    maxColFunction = scaler.data_max_[colMoneynessIndex]\n",
    "    minColFunction = scaler.data_min_[colMoneynessIndex]\n",
    "    scF = (maxColFunction - minColFunction) \n",
    "    scaleTensor = tf.constant(scF, dtype=tf.float32)\n",
    "    moneynessMinTensor = tf.constant(minColFunction, dtype=tf.float32)\n",
    "\n",
    "    price_pred_tensor = None\n",
    "    TensorList = None\n",
    "    penalizationList = None \n",
    "    formattingFunction = None\n",
    "    if activateRegularization : #Add pseudo local volatility regularisation\n",
    "        vol_pred_tensor, TensorList, penalizationList, formattingFunction = addDupireRegularisation( *NNFactory(hidden_nodes,\n",
    "                                                                                                                Moneyness,\n",
    "                                                                                                                Maturity, \n",
    "                                                                                                                scaleTensor, \n",
    "                                                                                                                moneynessMinTensor, \n",
    "                                                                                                                vegaRef, \n",
    "                                                                                                                hyperparameters) ,\n",
    "                                                                                                    vegaRef, \n",
    "                                                                                                    hyperparameters)\n",
    "    else :\n",
    "        vol_pred_tensor, TensorList, penalizationList, formattingFunction = NNFactory(hidden_nodes,\n",
    "                                                                                      Moneyness, \n",
    "                                                                                      Maturity, \n",
    "                                                                                      scaleTensor, \n",
    "                                                                                      moneynessMinTensor, \n",
    "                                                                                      vegaRef, \n",
    "                                                                                      hyperparameters)\n",
    "\n",
    "    vol_pred_tensor_sc= vol_pred_tensor\n",
    "    TensorList[0] = vol_pred_tensor_sc\n",
    "    \n",
    "    # Define a loss function\n",
    "    pointwiseError = tf.reduce_mean(tf.abs(vol_pred_tensor_sc - y) / vegaRef)\n",
    "    errors = tf.add_n([pointwiseError] + penalizationList)\n",
    "    loss = tf.log(tf.reduce_mean(errors))\n",
    "\n",
    "\n",
    "    # Define a train operation to minimize the loss\n",
    "    lr = learningRate \n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRateTensor)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    # Initialize variables and run session\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    n = dataSet.shape[0]\n",
    "    scaledInput = transformCustomMinMax(dataSet, scaler)\n",
    "\n",
    "    \n",
    "    maturity = dataSet[\"Maturity\"].values.reshape(n,1)\n",
    "    loss_serie = []\n",
    "\n",
    "    def createFeedDict(batch):\n",
    "        batchSize = batch.shape[0]\n",
    "        feedDict = {Moneyness : scaledInput[\"logMoneyness\"].loc[batch.index].values.reshape(batchSize,1),\n",
    "                    Maturity : batch[\"Maturity\"].values.reshape(batchSize,1), \n",
    "                    y : batch[\"impliedTotalVariance\"].values.reshape(batchSize,1),\n",
    "                    learningRateTensor : learningRate,\n",
    "                    vegaRef : np.ones_like(batch[\"VegaRef\"].values.reshape(batchSize,1))}\n",
    "        return feedDict\n",
    "    \n",
    "    epochFeedDict = createFeedDict(dataSet)\n",
    "\n",
    "    def evalBestModel():\n",
    "        if not activateLearningDecrease :\n",
    "            print(\"Learning rate : \", learningRate, \" final loss : \", min(loss_serie))\n",
    "        currentBestLoss = sess.run(loss, feed_dict=epochFeedDict)\n",
    "        currentBestPenalizations = sess.run([pointwiseError, penalizationList], feed_dict=epochFeedDict)\n",
    "        print(\"Best loss (hidden nodes: %d, iterations: %d): %.2f\" % (hidden_nodes, len(loss_serie), currentBestLoss))\n",
    "        print(\"Best Penalization : \", currentBestPenalizations)\n",
    "        return\n",
    "    \n",
    "    saver.restore(sess, modelName)  \n",
    "    \n",
    "    evalBestModel()\n",
    "\n",
    "    evalList  = sess.run(TensorList, feed_dict=epochFeedDict)\n",
    "    \n",
    "    sess.close()\n",
    "    \n",
    "    return formattingFunction(*evalList, [0], dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5yz8_0VgRrXL"
   },
   "outputs": [],
   "source": [
    "def evalVolLocaleGatheral(NNFactory,\n",
    "                          strikes,\n",
    "                          maturities,\n",
    "                          dataSet,\n",
    "                          hyperParameters,\n",
    "                          modelName = \"bestModel\"):\n",
    "    \n",
    "    hidden_nodes = hyperParameters[\"nbUnits\"] \n",
    "\n",
    "    # Reset the graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Placeholders for input and output data   \n",
    "    Moneyness = tf.placeholder(tf.float32,[None,1])\n",
    "    Maturity = tf.placeholder(tf.float32,[None,1])\n",
    "    y = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='y')\n",
    "    vegaRef = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='vegaRef')\n",
    "    learningRateTensor = tf.placeholder(tf.float32,[])\n",
    "    \n",
    "    #Get scaling for strike\n",
    "    colMoneynessIndex = dataSet.columns.get_loc(\"logMoneyness\")\n",
    "    maxColFunction = scaler.data_max_[colMoneynessIndex]\n",
    "    minColFunction = scaler.data_min_[colMoneynessIndex]\n",
    "    scF = (maxColFunction - minColFunction) \n",
    "    scaleTensor = tf.constant(scF, dtype=tf.float32)\n",
    "    moneynessMinTensor = tf.constant(minColFunction, dtype=tf.float32)\n",
    "\n",
    "    price_pred_tensor = None\n",
    "    TensorList = None\n",
    "    penalizationList = None \n",
    "    formattingFunction = None\n",
    "    vol_pred_tensor, TensorList, penalizationList, formattingFunction = NNFactory(hidden_nodes,\n",
    "                                                                                  Moneyness,\n",
    "                                                                                  Maturity,\n",
    "                                                                                  scaleTensor,\n",
    "                                                                                  moneynessMinTensor,\n",
    "                                                                                  vegaRef,\n",
    "                                                                                  hyperparameters)\n",
    "\n",
    "    vol_pred_tensor_sc= vol_pred_tensor\n",
    "    TensorList[0] = vol_pred_tensor_sc\n",
    "    \n",
    "    # Define a loss function\n",
    "    pointwiseError = tf.reduce_mean(tf.abs(vol_pred_tensor_sc - y) / vegaRef)\n",
    "    errors = tf.add_n([pointwiseError] + penalizationList)\n",
    "    loss = tf.log(tf.reduce_mean(errors))\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRateTensor)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    # Initialize variables and run session\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    n = strikes.shape[0]\n",
    "    changedVar = changeOfVariable(strikes, maturities)\n",
    "\n",
    "    moneyness = np.log(changedVar[0] / S0[0]) \n",
    "    scaledMoneyness = (moneyness-minColFunction)/scF\n",
    "\n",
    "    def createFeedDict(m, t):\n",
    "        batchSize = m.shape[0]\n",
    "        feedDict = {Moneyness : np.reshape(m, (batchSize,1)), \n",
    "                    Maturity : np.reshape(t, (batchSize,1)) ,  \n",
    "                    vegaRef : np.ones((batchSize,1))}\n",
    "        return feedDict\n",
    "    \n",
    "    epochFeedDict = createFeedDict(scaledMoneyness, maturities)\n",
    "    \n",
    "    saver.restore(sess, modelName)  \n",
    "\n",
    "    evalList = sess.run(TensorList, feed_dict=epochFeedDict)\n",
    "    \n",
    "    sess.close()\n",
    "    \n",
    "    return pd.Series(evalList[1].flatten(), index = pd.MultiIndex.from_arrays([strikes, maturities], names=('Strike', 'Maturity')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r1raybsuVwOU"
   },
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-9F79bBcXEfV"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Dupire formula from exact derivative computation\n",
    "def dupireFormulaGatheral(HessianMoneyness, \n",
    "                          GradMoneyness,\n",
    "                          GradMaturity, \n",
    "                          totalVariance,\n",
    "                          ScaledMoneyness,\n",
    "                          scaleTensor,\n",
    "                          MoneynessMinTensor,\n",
    "                          IsTraining=True):\n",
    "  twoConstant = tf.constant(2.0)\n",
    "  oneConstant = tf.constant(1.0)\n",
    "  quarterConstant = tf.constant(0.25)\n",
    "  halfConstant = tf.constant(0.5)\n",
    "\n",
    "  moneyness = ScaledMoneyness * scaleTensor + MoneynessMinTensor \n",
    "  \n",
    "  dT = GradMaturity\n",
    "\n",
    "  dMoneyness = GradMoneyness / scaleTensor\n",
    "  dMoneynessFactor = (moneyness/totalVariance)\n",
    "  dMoneynessSquaredFactor = quarterConstant * (-quarterConstant - oneConstant/totalVariance + tf.square(dMoneynessFactor))\n",
    "\n",
    "  gMoneyness =  HessianMoneyness / tf.square(scaleTensor)\n",
    "  gMoneynessFactor = halfConstant\n",
    "  denominator = oneConstant - dMoneynessFactor * (dMoneyness) + dMoneynessSquaredFactor * tf.square(dMoneyness) + gMoneynessFactor *  gMoneyness\n",
    "  \n",
    "  gatheralVar = dT / denominator\n",
    "  #Initial weights of neural network can be random which lead to negative dupireVar\n",
    "  gatheralVolTensor = tf.sqrt(gatheralVar) \n",
    "  return gatheralVolTensor, gatheralVar, gatheralDenominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "irkHQi3rXEcY"
   },
   "outputs": [],
   "source": [
    "#Dupire formula with derivative obtained from native tensorflow algorithmic differentiation\n",
    "def rawDupireFormulaGatheral(totalVarianceTensor, \n",
    "                             scaledMoneynessTensor, \n",
    "                             maturityTensor,\n",
    "                             scaleTensor,\n",
    "                             moneynessMinTensor,\n",
    "                             IsTraining=True):\n",
    "  batchSize = tf.shape(scaledMoneynessTensor)[0]\n",
    "  twoConstant = tf.constant(2.0)\n",
    "  oneConstant = tf.constant(1.0)\n",
    "  quarterConstant = tf.constant(0.25)\n",
    "  halfConstant = tf.constant(0.5)\n",
    "\n",
    "  moneyness = scaledMoneynessTensor * scaleTensor + moneynessMinTensor \n",
    "\n",
    "  dMoneyness = tf.reshape(tf.gradients(totalVarianceTensor, scaledMoneynessTensor, name=\"dK\")[0], shape=[batchSize,-1]) / scaleTensor\n",
    "  dMoneynessFactor = (moneyness/totalVarianceTensor)\n",
    "  dMoneynessSquaredFactor = quarterConstant * (-quarterConstant - oneConstant/totalVarianceTensor + tf.square(dMoneynessFactor))\n",
    "\n",
    "  gMoneyness = tf.reshape(tf.gradients(dMoneyness, scaledMoneynessTensor, name=\"hK\")[0], shape=[batchSize,-1]) / scaleTensor\n",
    "  gMoneynessFactor = halfConstant\n",
    "\n",
    "\n",
    "  gatheralDenominator = oneConstant - dMoneynessFactor * (dMoneyness) + dMoneynessSquaredFactor * tf.square(dMoneyness) + gMoneynessFactor *  gMoneyness\n",
    "\n",
    "  dT = tf.reshape(tf.gradients(totalVarianceTensor,maturityTensor,name=\"dT\")[0], shape=[batchSize,-1])\n",
    "\n",
    "  #Initial weights of neural network can be random which lead to negative dupireVar\n",
    "  gatheralVar = dT / gatheralDenominator\n",
    "  gatheralVol = tf.sqrt(gatheralVar) \n",
    "  return  gatheralVol, dT, gMoneyness, gatheralVar, gatheralDenominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_YNWG3IbXEaN"
   },
   "outputs": [],
   "source": [
    "#Soft constraints for strike convexity and strike/maturity monotonicity  \n",
    "def arbitragePenalties(dT, gatheralDenominator, vegaRef, hyperparameters):\n",
    "    \n",
    "    lambdas = hyperparameters[\"lambdaSoft\"]  / tf.reduce_mean(vegaRef) \n",
    "    lowerBoundTheta = tf.constant(hyperparameters[\"lowerBoundTheta\"])\n",
    "    lowerBoundGamma = tf.constant(hyperparameters[\"lowerBoundGamma\"])\n",
    "    calendar_penalty = lambdas * tf.reduce_mean(tf.nn.relu(-dT + lowerBoundTheta ))\n",
    "    butterfly_penalty = lambdas * hyperparameters[\"lowerBoundGamma\"] * tf.reduce_mean(tf.nn.relu(-gatheralDenominator + lowerBoundGamma ))\n",
    "    \n",
    "    return [calendar_penalty, butterfly_penalty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ShAR4NOqugHJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def NNArchitectureVanillaSoftGatheralAckerer(n_units,\n",
    "                                             scaledMoneynessTensor,\n",
    "                                             maturityTensor,\n",
    "                                             scaleTensor,\n",
    "                                             moneynessMinTensor,\n",
    "                                             vegaRef,\n",
    "                                             hyperparameters,\n",
    "                                             IsTraining=True):\n",
    "  \n",
    "  inputLayer = tf.concat([scaledMoneynessTensor,maturityTensor], axis=-1)\n",
    "  #First layer\n",
    "  hidden1 = unconstrainedLayer(n_units = n_units,\n",
    "                               tensor = inputLayer,\n",
    "                               isTraining=IsTraining, \n",
    "                               name = \"Hidden1\")\n",
    "  #Second layer\n",
    "  hidden2 = unconstrainedLayer(n_units = n_units,\n",
    "                               tensor = hidden1,\n",
    "                               isTraining=IsTraining, \n",
    "                               name = \"Hidden2\")\n",
    "  #Third layer\n",
    "  hidden3 = unconstrainedLayer(n_units = n_units,\n",
    "                               tensor = hidden2,\n",
    "                               isTraining=IsTraining, \n",
    "                               name = \"Hidden3\")\n",
    "  #Output layer\n",
    "  out = unconstrainedLayer(n_units = 1,\n",
    "                           tensor = hidden3,\n",
    "                           isTraining=IsTraining, \n",
    "                           name = \"Output\",\n",
    "                           activation = None)\n",
    "  #Local volatility \n",
    "  gatheralVol, theta, hK, gatheralVar, gatheralDenominator = rawDupireFormulaGatheral(out * maturityTensor,\n",
    "                                                                                      scaledMoneynessTensor,\n",
    "                                                                                      maturityTensor,\n",
    "                                                                                      scaleTensor,\n",
    "                                                                                      moneynessMinTensor,\n",
    "                                                                                      IsTraining=IsTraining)\n",
    "  #Soft constraints for no arbitrage\n",
    "  penalties = arbitragePenalties(theta, gatheralDenominator, vegaRef, hyperparameters)\n",
    "  grad_penalty = penalties[0]\n",
    "  hessian_penalty = penalties[1]\n",
    "  \n",
    "  return out, [out, gatheralVol, theta, hK, gatheralVar], [grad_penalty, hessian_penalty], evalAndFormatDupireResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nndy4Ym6XEX6"
   },
   "outputs": [],
   "source": [
    "\n",
    "def NNArchitectureVanillaSoftGatheral(n_units, \n",
    "                                      scaledMoneynessTensor,\n",
    "                                      maturityTensor,\n",
    "                                      scaleTensor,\n",
    "                                      moneynessMinTensor,\n",
    "                                      vegaRef,\n",
    "                                      hyperparameters,\n",
    "                                      IsTraining=True):\n",
    "  \n",
    "  inputLayer = tf.concat([scaledMoneynessTensor,maturityTensor], axis=-1)\n",
    "  #First layer\n",
    "  hidden1 = unconstrainedLayer(n_units = n_units,\n",
    "                               tensor = inputLayer,\n",
    "                               isTraining=IsTraining, \n",
    "                               name = \"Hidden1\")\n",
    "  #Second layer\n",
    "  hidden2 = unconstrainedLayer(n_units = n_units,\n",
    "                               tensor = hidden1,\n",
    "                               isTraining=IsTraining, \n",
    "                               name = \"Hidden2\")\n",
    "  #Output layer\n",
    "  out = unconstrainedLayer(n_units = 1,\n",
    "                           tensor = hidden2,\n",
    "                           isTraining=IsTraining, \n",
    "                           name = \"Output\",\n",
    "                           activation = None)\n",
    "  #Local volatility \n",
    "  gatheralVol, theta, hK, gatheralVar, gatheralDenominator = rawDupireFormulaGatheral(out * maturityTensor,\n",
    "                                                                                      scaledMoneynessTensor,\n",
    "                                                                                      maturityTensor,\n",
    "                                                                                      scaleTensor,\n",
    "                                                                                      moneynessMinTensor,\n",
    "                                                                                      IsTraining=IsTraining)\n",
    "  #Soft constraints for no arbitrage\n",
    "  penalties = arbitragePenalties(theta, gatheralDenominator, vegaRef, hyperparameters)\n",
    "  grad_penalty = penalties[0]\n",
    "  hessian_penalty = penalties[1]\n",
    "  \n",
    "  return out, [out, gatheralVol, theta, hK, gatheralVar], [grad_penalty, hessian_penalty], evalAndFormatDupireResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DabeiqM_V0Cp"
   },
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jSD_zWzaFeD5"
   },
   "outputs": [],
   "source": [
    "hyperparameters = {}\n",
    "#penalization coefficient\n",
    "hyperparameters[\"lambdaLocVol\"] = 0.01 #100\n",
    "hyperparameters[\"lambdaSoft\"] = 10#10 #100 \n",
    "hyperparameters[\"lambdaGamma\"] = 10#10 #10000\n",
    "\n",
    "#Derivative soft constraints parameters\n",
    "hyperparameters[\"lowerBoundTheta\"] = 0.01\n",
    "hyperparameters[\"lowerBoundGamma\"] = 0.00001\n",
    "\n",
    "#Local variance parameters\n",
    "hyperparameters[\"DupireVarCap\"] = 10\n",
    "hyperparameters[\"DupireVolLowerBound\"] = 0.05\n",
    "hyperparameters[\"DupireVolUpperBound\"] = 0.40\n",
    "\n",
    "#Learning scheduler coefficient\n",
    "hyperparameters[\"LearningRateStart\"] = 0.1\n",
    "hyperparameters[\"Patience\"] = 100\n",
    "hyperparameters[\"batchSize\"] = 50\n",
    "hyperparameters[\"FinalLearningRate\"] = 1e-6\n",
    "hyperparameters[\"FixedLearningRate\"] = False\n",
    "\n",
    "#Training parameters\n",
    "hyperparameters[\"nbUnits\"] = 200 #number of units for hidden layers\n",
    "hyperparameters[\"maxEpoch\"] = 10000 #maximum number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a1g1xVliXEWM"
   },
   "outputs": [],
   "source": [
    "y_pred4G, volLocale4G, dNN_T4G, gNN_K4G, lossSerie4G = create_train_model_gatheral(NNArchitectureVanillaSoftGatheral,\n",
    "                                                                                   scaledDataSet[scaledDataSet.index.get_level_values(\"Maturity\") > 0.01],\n",
    "                                                                                   True,\n",
    "                                                                                   hyperparameters,\n",
    "                                                                                   modelName = \"convexSoftGatheralVolModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WIQ3RWuAXETq"
   },
   "outputs": [],
   "source": [
    "plotEpochLoss(lossSerie4G)\n",
    "lossSerie4G.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RrIfSfqNXERH"
   },
   "outputs": [],
   "source": [
    "y_pred4G, volLocale4G, dNN_T4G, gNN_K4G, lossSerie4G = create_eval_model(NNArchitectureVanillaSoftGatheral,\n",
    "                                                                         scaledDataSet[scaledDataSet.index.get_level_values(\"Maturity\") > 0.01],\n",
    "                                                                         True,\n",
    "                                                                         hyperparameters,\n",
    "                                                                         modelName = \"convexSoftGatheralVolModel\")\n",
    "modelSummaryGatheral(y_pred4G, volLocale4G, dNN_T4G, gNN_K4G, dataSet[dataSet.index.get_level_values(\"Maturity\") > 0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LjAFCm8IXEKH"
   },
   "outputs": [],
   "source": [
    "volLocale4G.loc[(midS0,slice(None))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BAdEtgkxEaSZ"
   },
   "outputs": [],
   "source": [
    "y_pred4TestG, volLocale4TestG, dNN_T4TestG, gNN_K4TestG, lossSerie4TestG = create_eval_model(NNArchitectureVanillaSoftGatheral,\n",
    "                                                                                             scaledDataSetTest[scaledDataSetTest.index.get_level_values(\"Maturity\") > 0.01],\n",
    "                                                                                             True,\n",
    "                                                                                             hyperparameters,\n",
    "                                                                                             modelName = \"convexSoftGatheralVolModel\")\n",
    "modelSummaryGatheral(y_pred4TestG, volLocale4TestG, dNN_T4TestG, gNN_K4TestG, dataSetTest[dataSetTest.index.get_level_values(\"Maturity\") > 0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uK-gPhr9EbEQ"
   },
   "outputs": [],
   "source": [
    "scaledDataSet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iI9h12E849gF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VhJjhSnk5BU5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1C2-5GSXEbBB"
   },
   "outputs": [],
   "source": [
    "modelSummaryGatheral(y_pred4G[y_pred4G.index.get_level_values(\"Maturity\") >= 0.19], \n",
    "                     volLocale4G[volLocale4G.index.get_level_values(\"Maturity\") >= 0.19], \n",
    "                     dNN_T4G[dNN_T4G.index.get_level_values(\"Maturity\") >= 0.19], \n",
    "                     gNN_K4G[gNN_K4G.index.get_level_values(\"Maturity\") >= 0.19], \n",
    "                     dataSet[dataSet.index.get_level_values(\"Maturity\") > 0.19],\n",
    "                     logMoneynessScale = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I65cbFSSZQ4K"
   },
   "outputs": [],
   "source": [
    "modelSummaryGatheral(y_pred4TestG, \n",
    "                     volLocale4TestG, \n",
    "                     dNN_T4TestG, \n",
    "                     gNN_K4TestG, \n",
    "                     dataSetTest[dataSetTest.index.get_level_values(\"Maturity\") > 0.01],\n",
    "                     logMoneynessScale = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3a3HFfj9Oa6O"
   },
   "source": [
    "#### Monte Carlo backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ft7IMrwOa6S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CToRLckiOa6V"
   },
   "outputs": [],
   "source": [
    "def neuralVolLocaleDugas(s,t):\n",
    "  vLoc = evalVolLocaleGatheral(NNArchitectureVanillaSoftGatheral,\n",
    "                               s, t,\n",
    "                               dataSetTest,\n",
    "                               hyperparameters,\n",
    "                               modelName = \"convexSoftGatheralVolModel\")\n",
    "  return vLoc.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V01XOkK-Oa6X"
   },
   "outputs": [],
   "source": [
    "volLocalInterp7 = neuralVolLocaleDugas(volLocaleGrid[0].flatten(),\n",
    "                                       volLocaleGrid[1].flatten())\n",
    "volLocalInterp7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VlLbCyxnOa6Z"
   },
   "outputs": [],
   "source": [
    "volLocalInterp8 = neuralVolLocaleDugas(dataSetTest.index.get_level_values(\"Strike\").values.flatten(),\n",
    "                                       dataSetTest.index.get_level_values(\"Maturity\").values.flatten())\n",
    "volLocalInterp8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KVPYoApyOa6b"
   },
   "outputs": [],
   "source": [
    "nnVolLocale7 = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp7, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YxtwrupUOa6c"
   },
   "outputs": [],
   "source": [
    "nnVolLocale8 = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp8, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hzjHMAcgOa6e"
   },
   "outputs": [],
   "source": [
    "plotSerie(volLocalInterp7,\n",
    "          Title = 'Interpolated Local Volatility Surface',\n",
    "          az=30,\n",
    "          yMin=0.0*S0,\n",
    "          yMax=2.0*S0, \n",
    "          zAsPercent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c5t5-3SFOa6g"
   },
   "outputs": [],
   "source": [
    "plotSerie(volLocalInterp8,\n",
    "          Title = 'Interpolated Local Volatility Surface',\n",
    "          az=30,\n",
    "          yMin=0.0*S0,\n",
    "          yMax=2.0*S0, \n",
    "          zAsPercent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WLm6CG5_Oa6i"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EOxtzoy5Oa6j"
   },
   "outputs": [],
   "source": [
    "mcResVolLocale7 = MonteCarloPricerVectorized(S0[0],\n",
    "                                             dataSetTest,\n",
    "                                             riskCurvespline,\n",
    "                                             divSpline,\n",
    "                                             nbPaths,\n",
    "                                             nbTimeStep,\n",
    "                                             nnVolLocale7)\n",
    "mcResVolLocale7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u4P1yOyHOa6l"
   },
   "outputs": [],
   "source": [
    "predictionDiagnosis(mcResVolLocale7, dataSetTest[\"Price\"], \" Price \", yMin=4100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jXn4kO1sEFGa"
   },
   "outputs": [],
   "source": [
    "mcResVolLocale7.to_csv(\"mcResVolLocale7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IxMQ_VNEOa6m"
   },
   "outputs": [],
   "source": [
    "mcResVolLocale8 = MonteCarloPricerVectorized(S0[0],\n",
    "                                             dataSetTest,\n",
    "                                             riskCurvespline,\n",
    "                                             divSpline,\n",
    "                                             nbPaths,\n",
    "                                             nbTimeStep,\n",
    "                                             nnVolLocale8)\n",
    "mcResVolLocale8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dQmDMVU8Oa6o"
   },
   "outputs": [],
   "source": [
    "predictionDiagnosis(mcResVolLocale8, dataSetTest[\"Price\"], \" Price \", yMin=4100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c9sfLc6iOa6p"
   },
   "outputs": [],
   "source": [
    "mcResVolLocale8.to_csv(\"mcResVolLocale8.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-FhHmEUAaIj"
   },
   "source": [
    "## Hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDIM91VNAk-s"
   },
   "outputs": [],
   "source": [
    "def selectHyperparameters(hyperparameters, parameterOfInterest, modelFactory, modelName, activateDupireReg, logGrid = True):\n",
    "    oldValue = hyperparameters[parameterOfInterest]\n",
    "    gridValue = oldValue * ( np.exp( np.log(10) * np.array([-2,-1, 0, 1, 2])) if logGrid else np.array([0.2, 0.5, 1, 2, 5]) )\n",
    "    \n",
    "    oldNbEpochs = hyperparameters[\"maxEpoch\"]\n",
    "    hyperparameters[\"maxEpoch\"] = int(oldNbEpochs / 10)\n",
    "    trainLoss = {}\n",
    "    arbitrageViolation = {}\n",
    "    for v in gridValue :\n",
    "        hyperparameters[parameterOfInterest] = int(v)\n",
    "        pred, volLoc, theta, gammaK, loss = create_train_model(modelFactory,\n",
    "                                                               scaledDataSet,\n",
    "                                                               activateDupireReg,\n",
    "                                                               hyperparameters,\n",
    "                                                               modelName = modelName)\n",
    "        nbArbitrageViolation = np.sum((theta <= 0)) + np.sum((gammaK <= 0))\n",
    "        trainLoss[v] = min(loss)\n",
    "        arbitrageViolation[v] = nbArbitrageViolation\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "    hyperparameters[\"maxEpoch\"] = oldNbEpochs\n",
    "    hyperparameters[parameterOfInterest] = oldValue\n",
    "    # Plot curves\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    if logGrid :\n",
    "        plt.xscale('symlog')\n",
    "    \n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Value')\n",
    "    ax1.set_ylabel('Loss', color=color)\n",
    "    ax1.plot(pd.Series(trainLoss), color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    \n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Arbitrage violation', color=color)  # we already handled the x-label with ax1\n",
    "    ax2.plot(pd.Series(arbitrageViolation), color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "REnyuzq2Ak5V"
   },
   "outputs": [],
   "source": [
    "def selectHyperparametersRandom(hyperparameters, \n",
    "                                parametersOfInterest, \n",
    "                                modelFactory, \n",
    "                                modelName, \n",
    "                                activateDupireReg, \n",
    "                                nbAttempts,\n",
    "                                logGrid = True):\n",
    "    oldValue = {} \n",
    "    for k in parametersOfInterest :\n",
    "        oldValue[k] = hyperparameters[k]\n",
    "    \n",
    "    gridValue = np.exp( np.log(10) * np.array([-2,-1, 0, 1, 2])) if logGrid else np.array([0.2, 0.5, 1, 2, 5]) \n",
    "    \n",
    "    oldNbEpochs = hyperparameters[\"maxEpoch\"]\n",
    "    hyperparameters[\"maxEpoch\"] = int(oldNbEpochs / 10)\n",
    "    trainLoss = {}\n",
    "    arbitrageViolation = {}\n",
    "    nbTry = nbAttempts\n",
    "    for v in range(nbTry) :\n",
    "        combination = np.random.randint(5, size = len(parametersOfInterest) )\n",
    "        for p in range(len(parametersOfInterest)):\n",
    "            hyperparameters[parametersOfInterest[p]] = oldValue[parametersOfInterest[p]] * gridValue[int(combination[p])]\n",
    "            print(parametersOfInterest[p] , \" : \", hyperparameters[parametersOfInterest[p]])\n",
    "        pred, volLoc, theta, gammaK, loss = create_train_model(modelFactory,\n",
    "                                                               scaledDataSet,\n",
    "                                                               activateDupireReg,\n",
    "                                                               hyperparameters,\n",
    "                                                               modelName = modelName)\n",
    "        nbArbitrageViolation = np.sum((theta <= 0)) + np.sum((gammaK <= 0))\n",
    "        print(\"loss : \", min(loss))\n",
    "        print(\"nbArbitrageViolation : \", nbArbitrageViolation)\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "    hyperparameters[\"maxEpoch\"] = oldNbEpochs\n",
    "    for k in parametersOfInterest :\n",
    "        hyperparameters[k] = oldValue[k]\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0P-fv_wMAk2y"
   },
   "outputs": [],
   "source": [
    "selectHyperparametersRandom(hyperparameters,\n",
    "                            [\"lambdaLocVol\",\"lambdaSoft\",\"lambdaGamma\"],\n",
    "                            NNArchitectureConstrainedRawDupire,\n",
    "                            \"hyperParameters\",\n",
    "                            True, \n",
    "                            100,\n",
    "                            logGrid = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eWz86KbKCqa-"
   },
   "outputs": [],
   "source": [
    "\n",
    "hyperparameters[\"lambdaLocVol\"] = 100\n",
    "hyperparameters[\"lambdaSoft\"] = 100 \n",
    "hyperparameters[\"lambdaGamma\"] = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1jSAmMeZ6V15"
   },
   "outputs": [],
   "source": [
    "selectHyperparameters(hyperparameters, \n",
    "                      \"lambdaLocVol\", \n",
    "                      NNArchitectureVanillaSoftDupire, \n",
    "                      \"hyperParameters\", \n",
    "                      True, \n",
    "                      logGrid = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dANDwRU6DEom"
   },
   "outputs": [],
   "source": [
    "selectHyperparameters(hyperparameters, \n",
    "                      \"DupireVarCap\", \n",
    "                      NNArchitectureConstrainedRawDupire, \n",
    "                      \"hyperParameters\", \n",
    "                      True, \n",
    "                      logGrid = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8-j6a1jpFt1_"
   },
   "outputs": [],
   "source": [
    "selectHyperparameters(hyperparameters, \n",
    "                      \"lambdaLocVol\", \n",
    "                      NNArchitectureUnconstrainedDupire, \n",
    "                      \"hyperParameters\", \n",
    "                      True, \n",
    "                      logGrid = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_thp08oSGj6m"
   },
   "outputs": [],
   "source": [
    "hyperparameters[\"lambdaLocVol\"] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Smxq-gfiGGV8"
   },
   "outputs": [],
   "source": [
    "selectHyperparameters(hyperparameters, \n",
    "                      \"lambdaLocVol\", \n",
    "                      NNArchitectureConstrainedRawDupire, \n",
    "                      \"hyperParameters\", \n",
    "                      True, \n",
    "                      logGrid = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GPSNSuGIGsMC"
   },
   "outputs": [],
   "source": [
    "hyperparameters[\"nbUnits\"] = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQb2l-ZqIKyC"
   },
   "outputs": [],
   "source": [
    "selectHyperparameters(hyperparameters, \n",
    "                      \"nbUnits\", \n",
    "                      NNArchitectureVanillaSoftDupire, \n",
    "                      \"hyperParameters\", \n",
    "                      True, \n",
    "                      logGrid = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_W3QYBDuMSu-"
   },
   "outputs": [],
   "source": [
    "hyperparameters[\"nbUnits\"] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q-lYydHFB8G1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dPQe7i1bQys_"
   },
   "source": [
    "## Dugas network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A_vQgZNRQ2Bv"
   },
   "outputs": [],
   "source": [
    "hyperparameters = {}\n",
    "#penalization coefficient\n",
    "hyperparameters[\"lambdaLocVol\"] = 1000\n",
    "hyperparameters[\"lambdaSoft\"] = 100 \n",
    "hyperparameters[\"lambdaGamma\"] = 10000\n",
    "\n",
    "#Derivative soft constraints parameters\n",
    "hyperparameters[\"lowerBoundTheta\"] = 0.01\n",
    "hyperparameters[\"lowerBoundGamma\"] = 0.00001\n",
    "\n",
    "#Local variance parameters\n",
    "hyperparameters[\"DupireVarCap\"] = 10\n",
    "hyperparameters[\"DupireVolLowerBound\"] = 0.05\n",
    "hyperparameters[\"DupireVolUpperBound\"] = 0.40\n",
    "\n",
    "#Learning scheduler coefficient\n",
    "hyperparameters[\"LearningRateStart\"] = 0.1\n",
    "hyperparameters[\"Patience\"] = 100\n",
    "hyperparameters[\"batchSize\"] = 50\n",
    "hyperparameters[\"FinalLearningRate\"] = 1e-6\n",
    "hyperparameters[\"FixedLearningRate\"] = False\n",
    "\n",
    "#Training parameters\n",
    "hyperparameters[\"nbUnits\"] = 200 #number of units for hidden layers\n",
    "hyperparameters[\"maxEpoch\"] = 10000 #maximum number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vdVwnXiqRDTM"
   },
   "outputs": [],
   "source": [
    "def convexDugasLayer(n_units,  tensor, isTraining, name):\n",
    "  with tf.name_scope(name):\n",
    "    nbInputFeatures = tensor.get_shape().as_list()[1]\n",
    "    bias = tf.Variable(initial_value = tf.zeros_initializer()([n_units], dtype=tf.float32), \n",
    "                       trainable = True, \n",
    "                       shape = [n_units],\n",
    "                       dtype = tf.float32, \n",
    "                       name = name + \"Bias\")\n",
    "    weights = tf.exp(tf.Variable(initial_value = tf.keras.initializers.glorot_normal()([nbInputFeatures, n_units], dtype=tf.float32), \n",
    "                                 trainable = True, \n",
    "                                 shape = [nbInputFeatures, n_units],\n",
    "                                 dtype = tf.float32, \n",
    "                                 name = name + \"Weights\"))\n",
    "    layer = tf.matmul(tensor, weights) + bias\n",
    "    return K.softplus(layer)\n",
    "\n",
    "def monotonicDugasLayer(n_units,  tensor, isTraining, name):\n",
    "  with tf.name_scope(name):\n",
    "    nbInputFeatures = tensor.get_shape().as_list()[1]\n",
    "    bias = tf.Variable(initial_value = tf.zeros_initializer()([n_units], dtype=tf.float32), \n",
    "                       trainable = True, \n",
    "                       shape = [n_units],\n",
    "                       dtype = tf.float32, \n",
    "                       name = name + \"Bias\")\n",
    "    weights = tf.exp(tf.Variable(initial_value = tf.keras.initializers.glorot_normal()([nbInputFeatures, n_units], dtype=tf.float32), \n",
    "                                 trainable = True, \n",
    "                                 shape = [nbInputFeatures, n_units],\n",
    "                                 dtype = tf.float32, \n",
    "                                 name = name + \"Weights\"))\n",
    "    layer = tf.matmul(tensor, weights) + bias\n",
    "    return K.sigmoid(layer)\n",
    "\n",
    "def convexDugasOutputLayer(tensor, isTraining, name):\n",
    "  with tf.name_scope(name):\n",
    "    nbInputFeatures = tensor.get_shape().as_list()[1]\n",
    "    bias = tf.exp(tf.Variable(initial_value = tf.zeros_initializer()([], dtype=tf.float32), \n",
    "                              shape = [],\n",
    "                              trainable = True, \n",
    "                              dtype = tf.float32, \n",
    "                              name = name + \"Bias\"))\n",
    "    weights = tf.exp(tf.Variable(initial_value = tf.keras.initializers.glorot_normal()([nbInputFeatures, 1], dtype=tf.float32), \n",
    "                                 shape = [nbInputFeatures, 1],\n",
    "                                 trainable = True, \n",
    "                                 dtype = tf.float32, \n",
    "                                 name = name + \"Weights\"))\n",
    "    layer = tf.matmul(tensor, weights) + bias\n",
    "    return layer\n",
    "\n",
    "\n",
    "\n",
    "def NNArchitectureHardConstrainedDugas(n_units, strikeTensor, \n",
    "                                       maturityTensor,\n",
    "                                       scaleTensor,\n",
    "                                       strikeMinTensor, \n",
    "                                       vegaRef,\n",
    "                                       hyperparameters,\n",
    "                                       IsTraining=True):\n",
    "  #First layer\n",
    "  hidden1S = convexDugasLayer(n_units = n_units,\n",
    "                              tensor = strikeTensor,\n",
    "                              isTraining=IsTraining,\n",
    "                              name = \"Hidden1S\")\n",
    "  \n",
    "  hidden1M = monotonicDugasLayer(n_units = n_units,\n",
    "                                 tensor = maturityTensor,\n",
    "                                 isTraining = IsTraining,\n",
    "                                 name = \"Hidden1M\")\n",
    "  \n",
    "  hidden1 = hidden1S * hidden1M\n",
    "  \n",
    "  #Second layer and output layer\n",
    "  out= convexDugasOutputLayer(tensor = hidden1,\n",
    "                              isTraining = IsTraining,\n",
    "                              name = \"Output\")\n",
    "  #Local volatility\n",
    "  dupireVol, theta, hK, dupireVar = rawDupireFormula(out, strikeTensor,\n",
    "                                                     maturityTensor,\n",
    "                                                     scaleTensor,\n",
    "                                                     strikeMinTensor,\n",
    "                                                     IsTraining=IsTraining)\n",
    "  \n",
    "  return out, [out, dupireVol, theta, hK, dupireVar], [], evalAndFormatDupireResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uD6MWTroRDTO"
   },
   "outputs": [],
   "source": [
    "y_predDugas, volLocaleDugas, dNN_TDugas, gNN_KDugas, lossSerieDugas = create_train_model(NNArchitectureHardConstrainedDugas,\n",
    "                                                                                         scaledDataSet,\n",
    "                                                                                         True,\n",
    "                                                                                         hyperparameters,\n",
    "                                                                                         modelName = \"convexHardDugasVolModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WyLhpBzXRDTQ"
   },
   "outputs": [],
   "source": [
    "plotEpochLoss(lossSerieDugas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_wyRPp5kRDTS"
   },
   "outputs": [],
   "source": [
    "lossSerieDugas.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RlUkuCZKRDTT"
   },
   "outputs": [],
   "source": [
    "y_predDugas, volLocaleDugas, dNN_TDugas, gNN_KDugas, lossSerieDugas = create_eval_model(NNArchitectureHardConstrainedDugas,\n",
    "                                                                                        scaledDataSet,\n",
    "                                                                                        True,\n",
    "                                                                                        hyperparameters,\n",
    "                                                                                        modelName = \"convexHardDugasVolModel\")\n",
    "modelSummary(y_predDugas, volLocaleDugas, dNN_TDugas, gNN_KDugas, dataSet)\n",
    "impVDugas = plotImpliedVol(y_predDugas, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gSGNfIEuRDTV"
   },
   "outputs": [],
   "source": [
    "volLocaleDugas.loc[(midS0,slice(None))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SbAyorEaRDTW"
   },
   "outputs": [],
   "source": [
    "y_predDugasTest, volLocaleDugasTest, dNN_TDugasTest, gNN_KDugasTest, lossSerie6Test = create_eval_model(NNArchitectureHardConstrainedDugas, \n",
    "                                                                                        scaledDataSetTest, \n",
    "                                                                                        True, \n",
    "                                                                                        hyperparameters,\n",
    "                                                                                        modelName = \"convexHardDugasVolModel\")\n",
    "modelSummary(y_predDugasTest, volLocaleDugasTest, dNN_TDugasTest, gNN_KDugasTest, dataSetTest)\n",
    "impVDugasTest = plotImpliedVol(y_predDugasTest, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_YGqIPnFRDTY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T-rXuxceMS-M"
   },
   "source": [
    "#### Monte Carlo backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t01R9oTaMk81"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0vGg9QuXMlVT"
   },
   "outputs": [],
   "source": [
    "def neuralVolLocaleDugas(s,t):\n",
    "  vLoc = evalVolLocale(NNArchitectureHardConstrainedDugas,\n",
    "                       s, t,\n",
    "                       dataSet,\n",
    "                       hyperparameters,\n",
    "                       modelName = \"convexHardDugasVolModel\")\n",
    "  return vLoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZZNSCOHhMlVX"
   },
   "outputs": [],
   "source": [
    "volLocalInterp9 = neuralVolLocaleDugas(volLocaleGrid[0].flatten(),\n",
    "                                       volLocaleGrid[1].flatten())\n",
    "volLocalInterp9.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HrCtvaDmMlVa"
   },
   "outputs": [],
   "source": [
    "volLocalInterp10 = neuralVolLocaleDugas(dataSetTest.index.get_level_values(\"Strike\").values.flatten(),\n",
    "                                        dataSetTest.index.get_level_values(\"Maturity\").values.flatten())\n",
    "volLocalInterp10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4cUpV8aqMlVd"
   },
   "outputs": [],
   "source": [
    "nnVolLocale9 = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp9, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gXcE78sEMlVf"
   },
   "outputs": [],
   "source": [
    "nnVolLocale10 = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp10, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-16ZM3CHMlVi"
   },
   "outputs": [],
   "source": [
    "plotSerie(volLocalInterp9,\n",
    "          Title = 'Interpolated Local Volatility Surface',\n",
    "          az=30,\n",
    "          yMin=0.0*S0,\n",
    "          yMax=2.0*S0, \n",
    "          zAsPercent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4bQyPsd0MlVk"
   },
   "outputs": [],
   "source": [
    "plotSerie(volLocalInterp10,\n",
    "          Title = 'Interpolated Local Volatility Surface',\n",
    "          az=30,\n",
    "          yMin=0.0*S0,\n",
    "          yMax=2.0*S0, \n",
    "          zAsPercent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d3TuWcvHMlVm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-QauSPwZICoJ"
   },
   "outputs": [],
   "source": [
    "mcResVolLocale9 = MonteCarloPricerVectorized(S0[0],\n",
    "                                             dataSet,\n",
    "                                             riskCurvespline,\n",
    "                                             divSpline,\n",
    "                                             nbPaths,\n",
    "                                             nbTimeStep,\n",
    "                                             nnVolLocale9)\n",
    "mcResVolLocale9.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bbcC3XNXICoN"
   },
   "outputs": [],
   "source": [
    "predictionDiagnosis(mcResVolLocale9, dataSet[\"Price\"], \" Price \", yMin=4100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YvbIoQXWICoP"
   },
   "outputs": [],
   "source": [
    "mcResVolLocale10 = MonteCarloPricerVectorized(S0[0],\n",
    "                                             dataSet,\n",
    "                                             riskCurvespline,\n",
    "                                             divSpline,\n",
    "                                             nbPaths,\n",
    "                                             nbTimeStep,\n",
    "                                             nnVolLocale10)\n",
    "mcResVolLocale10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S6QHWuSuICoS"
   },
   "outputs": [],
   "source": [
    "predictionDiagnosis(mcResVolLocale10, dataSet[\"Price\"], \" Price \", yMin=4100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O9RboM1EICoU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hQmQJZjxuYD7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BfNVpgxc51kJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vDsTnEfiorBj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "rQdgNr7Lt_rn",
    "0v-MujhxbB9L",
    "7--0wSGKZYhl",
    "VljyG0NV1CID",
    "JB4SRjCP1Wtv",
    "OlfrW4Gn2N4B",
    "gTX3rV8ka02z",
    "vNDkFJlYdi7h",
    "-qgxVPguB_6P",
    "KWdzg9DEv7eH",
    "s192l-r-B6lD",
    "hUMtwNuguMRu",
    "r9EvOdg0kU2P",
    "_eJG7_513hNC",
    "MrvDySFQWK-D",
    "NZBPdHPuB0CX",
    "NB3_-zutBt2y",
    "5yP7BP6avj5q",
    "nl9Gq8s4o_DH",
    "pMVAzDEKXe2Z",
    "s8feoYmarxmj",
    "3n8evyIxsB2W",
    "8ysLo9wxsTOj",
    "_sdUIUb_MwVJ",
    "_RU2VfS9UcAN",
    "LIWdTVy8tf59",
    "z3Hrt5KOVt9S",
    "Pwupw4shWcqI",
    "Z45IZtNnWfek",
    "LtubjBVY3vGh",
    "hwG8y4zzVlLj",
    "3sie49ZXVqJA",
    "r1raybsuVwOU",
    "DabeiqM_V0Cp",
    "3a3HFfj9Oa6O",
    "q-FhHmEUAaIj",
    "dPQe7i1bQys_"
   ],
   "name": "dupireNN08082001.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
