{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master File\n",
    "\n",
    "This notebook runs the GP backtest, NN-IV training and backtesting, SSVI backtesting and trinomial tree backtesting. Finally all results are compared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rQdgNr7Lt_rn"
   },
   "source": [
    "#### Help on Importing files\n",
    "\n",
    "If you run this notebook on google colab you need to upload python scripts on the left panel.\n",
    "To that end click on the left \"Files\" (or \"Fichiers\" in French) and drag and drop :\n",
    "- python scripts from the \"code\" folder of github repository.\n",
    "- csv files or xls files from \"data\" folder.\n",
    "- Tensorflow model (files with .data or .index extensions) in the \"Results\" folder if you want to use the neural network while avoiding the training step.\n",
    "- csv files in \"Results\" folder if you want to load Results presented in our paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       "  if (code_show) {\n",
       "    $('div.input').each(function(id) {\n",
       "      el = $(this).find('.cm-variable:first');\n",
       "      if (id == 0 || el.text() == 'hide_me') {\n",
       "        $(this).hide();\n",
       "      }\n",
       "    });\n",
       "    $('div.output_prompt').css('opacity', 0);\n",
       "  } else {\n",
       "    $('div.input').each(function(id) {\n",
       "      $(this).show();\n",
       "    });\n",
       "    $('div.output_prompt').css('opacity', 1);\n",
       "  }\n",
       "  code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input style=\"opacity:0\" type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a cell to hide code snippets from displaying\n",
    "# This must be at first cell!\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "hide_me = ''\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    "  if (code_show) {\n",
    "    $('div.input').each(function(id) {\n",
    "      el = $(this).find('.cm-variable:first');\n",
    "      if (id == 0 || el.text() == 'hide_me') {\n",
    "        $(this).hide();\n",
    "      }\n",
    "    });\n",
    "    $('div.output_prompt').css('opacity', 0);\n",
    "  } else {\n",
    "    $('div.input').each(function(id) {\n",
    "      $(this).show();\n",
    "    });\n",
    "    $('div.output_prompt').css('opacity', 1);\n",
    "  }\n",
    "  code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input style=\"opacity:0\" type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "\n",
    "import sys\n",
    "formerPath = sys.path\n",
    "sys.path.append('./code/')\n",
    "sys.path.append('./BS/')\n",
    "\n",
    "import os\n",
    "formerStdOut = sys.stdout\n",
    "\n",
    "import bootstrapping\n",
    "import dataSetConstruction\n",
    "import backtest\n",
    "import BS\n",
    "import loadData\n",
    "import plotTools\n",
    "import SSVI\n",
    "import SSVIFerhati\n",
    "import neuralNetwork\n",
    "\n",
    "\n",
    "import importlib\n",
    "\n",
    "sys.stdout = formerStdOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0v-MujhxbB9L"
   },
   "source": [
    "# Load data\n",
    "\n",
    "In order to reproduce our paper experiments, execute cells from part \"Load preformatted data\". \n",
    "\n",
    "Each source of data produces the following objects : \n",
    "- bootstrap manages discounting and dividend.\n",
    "- dataSet contains the training set.\n",
    "- dataSetTest contains the testing set.\n",
    "- $S0$ the spot value of the underlying "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XK3hBTZSZ1_J"
   },
   "source": [
    "#### Load preformatted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#You should call this function if you want to keep the same training set and testing set as those used in the paper.\n",
    "#\n",
    "#File required : \n",
    "#- testingDataSet.csv\n",
    "#- trainingDataSet.csv\n",
    "#- dfCurve.csv\n",
    "\n",
    "#SPX Data\n",
    "#File required : \n",
    "#- yieldCurve.dat.\n",
    "#- Option_SPX_18_Mai_2019Feuille2.xlsm\n",
    "def load_data(workingFolder, fileType=None):\n",
    "    \n",
    "    if fileType=='csv':\n",
    "        trainingSet, testingSet, bootstrap, S0 = loadData.loadDataFromCSV(workingFolder,\n",
    "                                                                  \"9_8_2001__filterdax\")\n",
    "    elif fileType=='dat':\n",
    "        trainingSet, testingSet, bootstrap, S0 = loadData.loadDataFromDat(workingFolder,\n",
    "                                                                  \"9_8_2001__filterdax\")\n",
    "    elif fileType=='xlsm':\n",
    "        fileName = \"Option_SPX_18_Mai_2019Feuille2.xlsm\"\n",
    "        asOfDate = \"2019-05-18\"\n",
    "        trainingSet, testingSet, bootstrap, S0 = loadData.loadCBOTData(workingFolder, fileName, asOfDate)\n",
    "    else:\n",
    "        trainingSet, testingSet, bootstrap, S0 = loadData.loadFormattedData(workingFolder)\n",
    "   \n",
    "    return trainingSet, testingSet, bootstrap, S0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideOutput": true,
    "id": "HVtY_MMcZ7uv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#importlib.reload(loadData)\n",
    "trainingSet, testingSet, bootstrap, S0 = load_data(\"./data/\",\"xlsm\")\n",
    "#Read csv files as dataFrames\n",
    "#trainingSet, testingSet, bootstrap, S0 = load_data(\"./data/09082001/\",\"csv\")\n",
    "#trainingSet, testingSet, bootstrap, S0 = load_data(\"./data/09082001/\",\"dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "id": "uix1XUaU8sa6"
   },
   "source": [
    "# Formatting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VljyG0NV1CID"
   },
   "source": [
    "### Boostsrapping Rate Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false,
    "id": "wRo7rm_oS-Fm"
   },
   "source": [
    "We assume a piecewise constant discount short rate $r$ and a piecewise constant dividend short rate $q$.\n",
    "\n",
    "We estimate the \"zero coupon dividend\" $D(T) = e^{-\\int_{0}^{T} q_s ds}$ by regressing it against maturity :\n",
    "$$e^{-\\int_{0}^{T} q_s ds} = \\frac{C(T,K) - P(T,K) + K e^{-\\int_{0}^{T} r_s ds}}{S_0}$$\n",
    "\n",
    "\n",
    "Then we have $\\hat{q}_t = - \\frac{ \\log{D(\\overline{T})} - \\log{D(\\underline{T})} }{ \\overline{T} - \\underline{T} }$ with $\\overline{T}$ the smallest discretized maturity greater than $T$ and $\\underline{T}$ the grestest discretized maturity inferior than $T$.\n",
    "\n",
    "bootstrap object has several members :\n",
    "- **riskFreeIntegral** corresponds to $I_T = \\int_{0}^{T} r_u du$.\n",
    "- **riskFreeSpline**  corresponds to $r_u$ evaluated on a subgrid. Interpolated as step function from zero coupons\n",
    "- **divSpreadIntegral** corresponds to $I_T = \\int_{0}^{T} q_u du$, can be negative.\n",
    "- **divSpline**  corresponds to $q_u$ evaluated on a subgrid, can be negative.\n",
    "\n",
    "These curve should satisfy the call-put parity.\n",
    "\n",
    "#### Change of variable\n",
    "\n",
    "Neural network on modified prices with modified strike as input such that discounting and dividend don't intervene in Dupire formula calculation.\n",
    "\n",
    "\n",
    "- In presence of dividend rate $d$ and risk free rate $r$ Dupire formula is :   $$\\sigma^2(T,K) = 2 \\frac{ \\partial_T P(T,K) + (r-q) K \\partial_K P(T,K) + qP(T,K)}{K² \\partial_{K}^2 P(T,K)}$$ \n",
    "with Strike $K$, Maturity $T$, dividend rate $q$ and risk-free rate $r$, $P$ our pricing function. \n",
    "- We apply the following change of variable : $$ w(T,k) = \\exp{(\\int_{0}^{T} q_t dt)} P(T,K)$$ with $K = k \\exp{(\\int_{0}^{T} (r_t - q_t) dt)} $.\n",
    "- Then Dupire equation becomes :  $\\sigma^2(T,K) = 2 \\frac{ \\partial_T w(T,k)}{k² \\partial_{k}^2 w(T,k)}$. \n",
    "- If we learn the mapping $v$ with a neural network then we obtain quickly by adjoint differentiation $\\partial_T w$ and $\\partial_{k²}^2 w$ and therefore $\\sigma$.\n",
    "- $k$ corresponds to \"ChangedStrike\" column.\n",
    "- $\\exp{(\\int_{0}^{T} q_t dt)}$ corresponds to \"DividendFactor\" column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hideCode": true,
    "hideOutput": true,
    "id": "QUxTjD2zY706",
    "scrolled": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainingSet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4c7404b8281c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataSet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainingSet\u001b[0m \u001b[0;31m#Training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataSetTest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestingSet\u001b[0m \u001b[0;31m#Testing set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m dfCurve = dataSetConstruction.savingData(bootstrap, \n\u001b[1;32m      4\u001b[0m                                          \u001b[0mdataSet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                          \u001b[0mdataSetTest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainingSet' is not defined"
     ]
    }
   ],
   "source": [
    "dataSet = trainingSet #Training set\n",
    "dataSetTest = testingSet #Testing set\n",
    "dfCurve = dataSetConstruction.savingData(bootstrap, \n",
    "                                         dataSet, \n",
    "                                         dataSetTest, \n",
    "                                         workingFolder)\n",
    "KMin, KMax, midS0, scaler, scaledDataSet, scaledDataSetTest = dataSetConstruction.minMaxScaling(dataSet, \n",
    "                                                                                                dataSetTest,\n",
    "                                                                                                S0)\n",
    "volLocaleGridDf = dataSetConstruction.generateOuterRectangleGrid(dataSet, dataSetTest, bootstrap, S0)\n",
    "dataSet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP Backtesting\n",
    "The purpose of this section is to load the GP local volatility surface and perform the Monte-Carlo backtest of the option prices uses the GP local volatility surface. Note that the GP local volatility is generated by running the Matlab code in the \"code/GP\" folder. See Section \"GP Local Volatility Backtests\" below for further details of the backtests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GP Results\n",
    "\n",
    "This section loads result from the Matlab experiments.\n",
    "See code/GP folder to access the matlab script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pathFolder = \"./data/\"\n",
    "fileName = \"GP_output_Put_Price_testing_set.xlsx\"\n",
    "putTestingGP, ImpVolPutTesting = loadData.loadGP(pathFolder, \n",
    "                                                 fileName, \n",
    "                                                 dataSetTest, \n",
    "                                                 S0, \n",
    "                                                 bootstrap)\n",
    "fileName = \"GP_output_Put_Price_training_set.xlsx\"\n",
    "putTrainingGP, ImpVolPutTraining = loadData.loadGP(pathFolder, \n",
    "                                                   fileName, \n",
    "                                                   dataSet, \n",
    "                                                   S0, \n",
    "                                                   bootstrap)\n",
    "putTrainingGP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load GP local volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hideOutput": true
   },
   "outputs": [],
   "source": [
    "workingFolder = \"./data/volLocaleAresky/\"\n",
    "#filename = \"local_vol_nx_10_nt_27.xlsx\" RMSE :  10.073967351737087\n",
    "#filename = \"local_vol_nx_12_nt_27.xlsx\" RMSE :  7.758802111118254\n",
    "#filename = \"local_vol_nx_15_nt_27.xlsx\" RMSE :  6.252799135416868\n",
    "#filename = \"local_vol_nx_18_nt_27.xlsx\" RMSE :  5.083806059940602\n",
    "#filename = \"local_vol_nx_20_nt_27.xlsx\" RMSE :  5.050554384554841\n",
    "#filename = \"local_vol_nx_25_nt_27.xlsx\" RMSE :  5.499835852015688\n",
    "filename = \"local_vol_nx_20_nt_27.xlsx\"\n",
    "locVolGP, volLocaleGridDf = loadData.loadGPLocVol(workingFolder, filename, bootstrap, S0, \n",
    "                                                  KMin, KMax, dataSet, dataSetTest)\n",
    "\n",
    "plotTools.saveDataModel(plotTools.removeDuplicateIndex(putTrainingGP[\"GP_Put_price\"]), \n",
    "                        plotTools.removeDuplicateIndex(nnGP(dataSet[\"Strike\"], dataSet[\"Maturity\"])), \n",
    "                        plotTools.removeDuplicateIndex(ImpVolPutTraining) , \n",
    "                        \"./Results/GPTraining\") \n",
    "plotTools.saveDataModel(plotTools.removeDuplicateIndex(putTestingGP[\"GP_Put_price\"]), \n",
    "                        plotTools.removeDuplicateIndex(nnGP(dataSetTest[\"Strike\"], dataSetTest[\"Maturity\"])), \n",
    "                        plotTools.removeDuplicateIndex(ImpVolPutTesting) , \n",
    "                        \"./Results/GPTesting\") \n",
    "\n",
    "locVolGP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nnGP = lambda x,y : backtest.interpolatedMCLocalVolatility(locVolGP[\"LocalVolatility\"], x, y)\n",
    "plotTools.plotSerie(nnGP(dataSetTest[\"Strike\"], dataSetTest[\"Maturity\"]),\n",
    "                    Title = 'Interpolated GP local volatility on testing nodes',\n",
    "                    az=30,\n",
    "                    yMin=KMin,\n",
    "                    yMax=KMax, \n",
    "                    zAsPercent=True)\n",
    "plotTools.plotSerie(nnGP(volLocaleGridDf[\"Strike\"], volLocaleGridDf[\"Maturity\"]),\n",
    "                    Title = 'Interpolated GP local volatility on backtesting nodes',\n",
    "                    az=30,\n",
    "                    yMin=KMin,\n",
    "                    yMax=KMax, \n",
    "                    zAsPercent=True)\n",
    "\n",
    "logMin = np.log(KMin/S0), \n",
    "logMax = 0.1 #np.log(KMax/S0),\n",
    "\n",
    "plotTools.plotSerie(plotTools.convertToLogMoneyness(nnGP(dataSetTest[\"Strike\"], dataSetTest[\"Maturity\"]), S0),\n",
    "                    Title = 'Interpolated GP local volatility on testing nodes',\n",
    "                    az=30,\n",
    "                    yMin=logMin,\n",
    "                    yMax=logMax, \n",
    "                    zAsPercent=True)\n",
    "\n",
    " \n",
    "plotTools.plot2Series(plotTools.convertToLogMoneyness(nnGP(dataSet[\"Strike\"], dataSet[\"Maturity\"]), S0), \n",
    "                      plotTools.convertToLogMoneyness(nnGP(dataSetTest[\"Strike\"], dataSetTest[\"Maturity\"]), S0), \n",
    "                      yMin=logMin,\n",
    "                      yMax=logMax,\n",
    "                      az = 340,\n",
    "                      Title = 'Interpolated Implied Vol Surface on testing nodes and training nodes')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v96wJTNM29IX"
   },
   "source": [
    "#### GP Local Volatility Backtests\n",
    "\n",
    "During Monte Carlo backtest, each option in testing set is priced with an underlying which is diffused with the following SDE : \n",
    "$$ dS_t = \\left( r_t - q_t - \\frac{\\sigma_{NN}^2(t, S_t)}{2} \\right) dt + \\sigma_{NN}(t, S_t) dW_t$$\n",
    "with $\\sigma_{NN}$ the neural local volatility function.\n",
    "\n",
    "Due to computation time issue we avoid to make millions of call to neural network and we interpolate linearly neural local volatility obtained on one the two possible grid :\n",
    "- the testing grid i.e. nodes $(T,K)$ of the testing set.\n",
    "- an artificial grid of 10000 points to check local volatility is correctly interpolated/extrapolated. That grid is the smallest rectangle containing the minimum and maximum maturities and the minimum and maximum strikes of our dataset (union of testing and training set).\n",
    "\n",
    "During PDE backtest, we used a crank-nicholson scheme to revaluate each option in our testing set.\n",
    "Time step corresponds to one day and space grid has 100 points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hideOutput": true
   },
   "outputs": [],
   "source": [
    "nbTimeStep = 100\n",
    "nbPaths = 100000\n",
    "nnGP = lambda x,y : backtest.interpolatedMCLocalVolatility(locVolGP[\"LocalVolatility\"], x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MC Backtest\n",
    "mcResGPTest = backtest.MonteCarloPricerVectorized(S0,\n",
    "                                                  dataSetTest,\n",
    "                                                  bootstrap,\n",
    "                                                  nbPaths,\n",
    "                                                  nbTimeStep,\n",
    "                                                  nnGP)\n",
    "workingFolder = \"./Results/\"\n",
    "mcResGPTest.to_csv(workingFolder + \"mcResGPTest.csv\")\n",
    "\n",
    "plotTools.predictionDiagnosis(mcResGPTest[\"Price\"], \n",
    "                              dataSetTest[\"Price\"], \n",
    "                              \" Monte Carlo Price \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)\n",
    "\n",
    "plotTools.plotSerie(mcResGPTest[\"stdPrice\"],\n",
    "                    Title = 'GP backtested price std',\n",
    "                    az=30,\n",
    "                    yMin=KMin,\n",
    "                    yMax=KMax, \n",
    "                    zAsPercent=True)\n",
    "\n",
    "mcResGPTest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#PDE Backtest\n",
    "pdeResSigmaGPTest = backtest.PDEPricerVectorized(dataSetTest, S0, nnGP, bootstrap)\n",
    "\n",
    "plotTools.predictionDiagnosis(pdeResSigmaGPTest, \n",
    "                              dataSetTest[\"Price\"], \n",
    "                              \" PDE Price \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)\n",
    "\n",
    "pdeResSigmaGPTest.to_csv(workingFolder + \"pdeResSigmaGPTest.csv\")\n",
    "\n",
    "pdeResSigmaGPTest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GX-r3Do7boHL"
   },
   "source": [
    "# Learning Implied volatility\n",
    "Yon can skip the training step by loading in the left panel in colab workspace tensorflow models. These models are contained in the results folder of github repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without arbitrage constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "id": "T8Fm__K0boHL"
   },
   "outputs": [],
   "source": [
    "hyperparameters = {}\n",
    "#penalization coefficient\n",
    "hyperparameters[\"lambdaLocVol\"] = 0.0#0.1#0.11#0.001#0.01 #100\n",
    "hyperparameters[\"lambdaSoft\"] = 0.0#100.0#1.0#0.0001#10#10 #100 \n",
    "hyperparameters[\"lambdaGamma\"] = 0.0#10000.0#10.0#100#10 #10000\n",
    "\n",
    "#Derivative soft constraints parameters\n",
    "hyperparameters[\"lowerBoundTheta\"] = 0.0001#0.01\n",
    "hyperparameters[\"lowerBoundGamma\"] = 0.0#0.00001\n",
    "\n",
    "#Local variance parameters\n",
    "hyperparameters[\"DupireVarCap\"] = 10.0\n",
    "hyperparameters[\"DupireVolLowerBound\"] = 0.03\n",
    "hyperparameters[\"DupireVolUpperBound\"] = 0.70\n",
    "\n",
    "#Learning scheduler coefficient\n",
    "hyperparameters[\"LearningRateStart\"] = 0.01\n",
    "hyperparameters[\"Patience\"] = 200\n",
    "hyperparameters[\"batchSize\"] = 50\n",
    "hyperparameters[\"FinalLearningRate\"] = 1e-6\n",
    "hyperparameters[\"FixedLearningRate\"] = False\n",
    "\n",
    "#Training parameters\n",
    "hyperparameters[\"nbUnits\"] = 100 #number of units for hidden layers\n",
    "hyperparameters[\"maxEpoch\"] = 10000#10000 #maximum number of epochs\n",
    "\n",
    "hyperparameters[\"UseLogMaturity\"] = True\n",
    "hyperparameters[\"nbEpochFork\"] = 0#10000\n",
    "hyperparameters[\"lambdaFork\"] = 0.0#1000.0\n",
    "hyperparameters[\"HolderExponent\"] =  4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hideOutput": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(neuralNetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "U7HthevAboHN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Execute this cell if you want to fit neural network with implied volatilities\n",
    "res = neuralNetwork.create_train_model_gatheral(neuralNetwork.NNArchitectureVanillaSoftGatheral,\n",
    "                                                dataSet,\n",
    "                                                False,\n",
    "                                                hyperparameters,\n",
    "                                                scaler,\n",
    "                                                modelName = \"unconstrainedConvexSoftGatheralVolModel\")\n",
    "\n",
    "\n",
    "y_pred4G, volLocale4G, dNN_T4G, gNN_K4G, lossSerie4G = res\n",
    "\n",
    "#Error plot\n",
    "plotTools.plotEpochLoss(lossSerie4G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hideCode": true,
    "id": "r4bMlOBqboHP",
    "outputId": "d59d8de1-0383-49be-e02e-fe906a6b9a77",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate results on the training set, you can execute that cell without training the model\n",
    "res = neuralNetwork.create_eval_model_gatheral(neuralNetwork.NNArchitectureVanillaSoftGatheral,\n",
    "                                               dataSet,\n",
    "                                               False,\n",
    "                                               hyperparameters,\n",
    "                                               scaler,\n",
    "                                               modelName = \"unconstrainedConvexSoftGatheralVolModel\")\n",
    "y_pred4G, volLocale4G, dNN_T4G, gNN_K4G, lossSerie4G = res\n",
    "\n",
    "plotTools.modelSummaryGatheral(y_pred4G, \n",
    "                               volLocale4G, \n",
    "                               dNN_T4G, \n",
    "                               gNN_K4G, \n",
    "                               dataSet,\n",
    "                               yMin = KMin,\n",
    "                               yMax = KMax, \n",
    "                               S0 = S0, \n",
    "                               bootstrap = bootstrap, \n",
    "                               thresholdPrice = None,\n",
    "                               savePath = \"./Results/NeuralUnconstrainedImpliedVolTrain\")\n",
    "\n",
    "print(\"ATM Local Volatility : \")\n",
    "print(volLocale4G.loc[(midS0,slice(None))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hideCode": true,
    "id": "zmEG0rnlboHU",
    "outputId": "9d02ef5c-1062-4352-ef25-9f2262f80284",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate results on the testing dataset, you can execute that cell without training the model\n",
    "res = neuralNetwork.create_eval_model_gatheral(neuralNetwork.NNArchitectureVanillaSoftGatheral,\n",
    "                                               dataSetTest,\n",
    "                                               False,\n",
    "                                               hyperparameters,\n",
    "                                               scaler,\n",
    "                                               modelName = \"unconstrainedConvexSoftGatheralVolModel\")\n",
    "y_pred4TestG, volLocale4TestG, dNN_T4TestG, gNN_K4TestG, lossSerie4TestG = res\n",
    "\n",
    "plotTools.modelSummaryGatheral(y_pred4TestG, \n",
    "                               volLocale4TestG, \n",
    "                               dNN_T4TestG, \n",
    "                               gNN_K4TestG, \n",
    "                               dataSetTest,\n",
    "                               yMin = KMin,\n",
    "                               yMax = KMax,\n",
    "                               S0 = S0, \n",
    "                               bootstrap = bootstrap, \n",
    "                               thresholdPrice = None,\n",
    "                               savePath = \"./Results/NeuralUnconstrainedImpliedVolTest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "plotTools.plotSerie(dataSet[\"Ask\"]-dataSet[\"Bid\"],\n",
    "                    Title = 'Bid-Ask price spread',\n",
    "                    az=30,\n",
    "                    yMin=KMin,\n",
    "                    yMax=KMax,\n",
    "                    zAsPercent=False)\n",
    "plotTools.plotSerie(dataSet[\"ImpVolAsk\"]-dataSet[\"ImpVolBid\"],\n",
    "                    Title = 'Bid-Ask implied vol spread',\n",
    "                    az=30,\n",
    "                    yMin=KMin,\n",
    "                    yMax=KMax,\n",
    "                    zAsPercent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hideCode": true,
    "id": "a4P363CRboHX",
    "outputId": "8e01eddb-309a-4cab-86ed-fa60b56cd274",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Diagnosis for training results with logMoneyness scale\n",
    "plotTools.modelSummaryGatheral(y_pred4G, \n",
    "                               volLocale4G, \n",
    "                               dNN_T4G, \n",
    "                               gNN_K4G, \n",
    "                               dataSet,\n",
    "                               logMoneynessScale = True,\n",
    "                               S0 = S0, \n",
    "                               bootstrap = bootstrap, \n",
    "                               thresholdPrice = None,\n",
    "                               yMin = KMin + 0.0001,\n",
    "                               yMax = KMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hideCode": true,
    "id": "B9_RyOgtboHY",
    "outputId": "49107305-be80-42c1-e4af-bbfad363888c"
   },
   "outputs": [],
   "source": [
    "#Diagnosis for testing results with logMoneyness scale\n",
    "plotTools.modelSummaryGatheral(y_pred4TestG, \n",
    "                               volLocale4TestG, \n",
    "                               dNN_T4TestG, \n",
    "                               gNN_K4TestG, \n",
    "                               dataSetTest,\n",
    "                               logMoneynessScale = True,\n",
    "                               S0 = S0, \n",
    "                               bootstrap = bootstrap, \n",
    "                               thresholdPrice = None,\n",
    "                               yMin = KMin + 0.0001,\n",
    "                               yMax = KMax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With arbitrage constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "T8Fm__K0boHL"
   },
   "outputs": [],
   "source": [
    "hyperparameters = {}\n",
    "#penalization coefficient\n",
    "hyperparameters[\"lambdaLocVol\"] = 1.0#0.1#0.0#0.1#0.11#0.001#0.01 #100\n",
    "hyperparameters[\"lambdaSoft\"] = 100.0#0.0#100.0#1.0#0.0001#10#10 #100 \n",
    "hyperparameters[\"lambdaGamma\"] = 10000.0#0.0#10000.0#10.0#100#10 #10000\n",
    "\n",
    "#Derivative soft constraints parameters\n",
    "hyperparameters[\"lowerBoundTheta\"] = 0.000001#0.01\n",
    "hyperparameters[\"lowerBoundGamma\"] = 0.0#0.00001\n",
    "\n",
    "#Local variance parameters\n",
    "hyperparameters[\"DupireVarCap\"] = 10.0\n",
    "hyperparameters[\"DupireVolLowerBound\"] = 0.03\n",
    "hyperparameters[\"DupireVolUpperBound\"] = 1.00\n",
    "\n",
    "#Learning scheduler coefficient\n",
    "hyperparameters[\"LearningRateStart\"] = 0.01\n",
    "hyperparameters[\"Patience\"] = 200\n",
    "hyperparameters[\"batchSize\"] = 50\n",
    "hyperparameters[\"FinalLearningRate\"] = 1e-6\n",
    "hyperparameters[\"FixedLearningRate\"] = False\n",
    "\n",
    "#Training parameters\n",
    "hyperparameters[\"nbUnits\"] = 100 #number of units for hidden layers\n",
    "hyperparameters[\"maxEpoch\"] = 10000#10000 #maximum number of epochs\n",
    "\n",
    "hyperparameters[\"UseLogMaturity\"] = True\n",
    "hyperparameters[\"nbEpochFork\"] = 0#10000\n",
    "hyperparameters[\"lambdaFork\"] = 0.0#1000.0\n",
    "hyperparameters[\"HolderExponent\"] =  2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(neuralNetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "id": "U7HthevAboHN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Execute this cell if you want to fit neural network with implied volatilities\n",
    "res = neuralNetwork.create_train_model_gatheral(neuralNetwork.NNArchitectureVanillaSoftGatheral,\n",
    "                                                dataSet,\n",
    "                                                True,\n",
    "                                                hyperparameters,\n",
    "                                                scaler,\n",
    "                                                modelName = \"convexSoftGatheralVolModel\")\n",
    "\n",
    "\n",
    "y_pred4G, volLocale4G, dNN_T4G, gNN_K4G, lossSerie4G = res\n",
    "\n",
    "#Error plot\n",
    "plotTools.plotEpochLoss(lossSerie4G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hideCode": true,
    "id": "r4bMlOBqboHP",
    "outputId": "d59d8de1-0383-49be-e02e-fe906a6b9a77",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate results on the training set, you can execute that cell without training the model\n",
    "res = neuralNetwork.create_eval_model_gatheral(neuralNetwork.NNArchitectureVanillaSoftGatheral,\n",
    "                                               dataSet,\n",
    "                                               True,\n",
    "                                               hyperparameters,\n",
    "                                               scaler,\n",
    "                                               modelName = \"convexSoftGatheralVolModel\")\n",
    "y_pred4G, volLocale4G, dNN_T4G, gNN_K4G, lossSerie4G = res\n",
    "\n",
    "plotTools.modelSummaryGatheral(y_pred4G, \n",
    "                               volLocale4G, \n",
    "                               dNN_T4G, \n",
    "                               gNN_K4G, \n",
    "                               dataSet,\n",
    "                               yMin = KMin,\n",
    "                               yMax = KMax, \n",
    "                               S0 = S0, \n",
    "                               bootstrap = bootstrap, \n",
    "                               thresholdPrice = None,\n",
    "                               savePath = \"./Results/NeuralImpliedVolTrain\")\n",
    "\n",
    "print(\"ATM Local Volatility : \")\n",
    "print(volLocale4G.loc[(midS0,slice(None))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hideCode": false,
    "id": "zmEG0rnlboHU",
    "outputId": "9d02ef5c-1062-4352-ef25-9f2262f80284",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate results on the testing dataset, you can execute that cell without training the model\n",
    "res = neuralNetwork.create_eval_model_gatheral(neuralNetwork.NNArchitectureVanillaSoftGatheral,\n",
    "                                               dataSetTest,\n",
    "                                               True,\n",
    "                                               hyperparameters,\n",
    "                                               scaler,\n",
    "                                               modelName = \"convexSoftGatheralVolModel\")\n",
    "y_pred4TestG, volLocale4TestG, dNN_T4TestG, gNN_K4TestG, lossSerie4TestG = res\n",
    "\n",
    "plotTools.modelSummaryGatheral(y_pred4TestG, \n",
    "                               volLocale4TestG, \n",
    "                               dNN_T4TestG, \n",
    "                               gNN_K4TestG, \n",
    "                               dataSetTest,\n",
    "                               yMin = KMin,\n",
    "                               yMax = KMax,\n",
    "                               S0 = S0, \n",
    "                               bootstrap = bootstrap, \n",
    "                               thresholdPrice = None,\n",
    "                               savePath = \"./Results/NeuralImpliedVolTest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "plotTools.plotSerie(dataSet[\"Ask\"]-dataSet[\"Bid\"],\n",
    "                    Title = 'Bid-Ask price spread',\n",
    "                    az=30,\n",
    "                    yMin=KMin,\n",
    "                    yMax=KMax,\n",
    "                    zAsPercent=False)\n",
    "plotTools.plotSerie(dataSet[\"ImpVolAsk\"]-dataSet[\"ImpVolBid\"],\n",
    "                    Title = 'Bid-Ask implied vol spread',\n",
    "                    az=30,\n",
    "                    yMin=KMin,\n",
    "                    yMax=KMax,\n",
    "                    zAsPercent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hideCode": true,
    "id": "a4P363CRboHX",
    "outputId": "8e01eddb-309a-4cab-86ed-fa60b56cd274",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Diagnosis for training results with logMoneyness scale\n",
    "plotTools.modelSummaryGatheral(y_pred4G, \n",
    "                               volLocale4G, \n",
    "                               dNN_T4G, \n",
    "                               gNN_K4G, \n",
    "                               dataSet,\n",
    "                               logMoneynessScale = True,\n",
    "                               S0 = S0, \n",
    "                               bootstrap = bootstrap, \n",
    "                               thresholdPrice = None,\n",
    "                               yMin = KMin + 0.0001,\n",
    "                               yMax = KMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hideCode": true,
    "id": "B9_RyOgtboHY",
    "outputId": "49107305-be80-42c1-e4af-bbfad363888c"
   },
   "outputs": [],
   "source": [
    "#Diagnosis for testing results with logMoneyness scale\n",
    "plotTools.modelSummaryGatheral(y_pred4TestG, \n",
    "                               volLocale4TestG, \n",
    "                               dNN_T4TestG, \n",
    "                               gNN_K4TestG, \n",
    "                               dataSetTest,\n",
    "                               logMoneynessScale = True,\n",
    "                               S0 = S0, \n",
    "                               bootstrap = bootstrap, \n",
    "                               thresholdPrice = None,\n",
    "                               yMin = KMin + 0.0001,\n",
    "                               yMax = KMax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lj7UueFUboHa"
   },
   "source": [
    "#### Monte Carlo and PDE repricing backtests\n",
    "\n",
    "During Monte Carlo backtest, each option in testing set is priced with an underlying which is diffused with the following SDE : \n",
    "$$ dS_t = \\left( r_t - q_t - \\frac{\\sigma_{NN}^2(t, S_t)}{2} \\right) dt + \\sigma_{NN}(t, S_t) dW_t$$\n",
    "with $\\sigma_{NN}$ the neural local volatility function.\n",
    "\n",
    "Due to computation time issue we avoid to make millions of call to neural network and we interpolate linearly neural local volatility obtained on one the two possible grid :\n",
    "- the testing grid i.e. nodes $(T,K)$ of the testing set.\n",
    "- an artificial grid of 10000 points to check local volatility is correctly interpolated/extrapolated. That grid is the smallest rectangle containing the minimum and maximum maturities and the minimum and maximum strikes of our dataset (union of testing and training set).\n",
    "\n",
    "During PDE backtest, we used a crank-nicholson scheme to revaluate each option in our testing set.\n",
    "Time step corresponds to one day and space grid has 100 points. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "s2RJpm74boHa"
   },
   "outputs": [],
   "source": [
    "# Function which evaluates neural local volatility when neural network is fitted on implied volatilities\n",
    "def neuralVolLocaleGatheral(s,t):\n",
    "    vLoc = neuralNetwork.evalVolLocaleGatheral(neuralNetwork.NNArchitectureVanillaSoftGatheral,\n",
    "                                               s, t,\n",
    "                                               dataSet,\n",
    "                                               hyperparameters,\n",
    "                                               scaler,\n",
    "                                               bootstrap,\n",
    "                                               S0,\n",
    "                                               modelName = \"convexSoftGatheralVolModel\")\n",
    "    return vLoc.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9W-q2_0rboHc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "volLocalInterp7 = neuralVolLocaleGatheral(volLocaleGridDf[\"Strike\"].values.flatten(),\n",
    "                                          volLocaleGridDf[\"Maturity\"].values.flatten())\n",
    "volLocalInterp8 = neuralVolLocaleGatheral(dataSetTest.index.get_level_values(\"Strike\").values.flatten(),\n",
    "                                          dataSetTest.index.get_level_values(\"Maturity\").values.flatten())\n",
    "\n",
    "#Local volatility function for backtests\n",
    "nnVolLocale7 = lambda x,y : backtest.interpolatedMCLocalVolatility(volLocalInterp7, x, y)\n",
    "\n",
    "#Local volatility function for backtests\n",
    "nnVolLocale8 = lambda x,y : backtest.interpolatedMCLocalVolatility(volLocalInterp8, x, y)\n",
    "\n",
    "plotTools.plotSerie(volLocalInterp7,\n",
    "                    Title = 'Interpolated Local Volatility on backtesting nodes',\n",
    "                    az=105,\n",
    "                    yMin=KMin,\n",
    "                    yMax=KMax,\n",
    "                    zAsPercent=False)\n",
    "\n",
    "plotTools.plotSerie(volLocalInterp8,\n",
    "                    Title = 'Local Volatility on testing nodes',\n",
    "                    az=30,\n",
    "                    yMin=KMin,\n",
    "                    yMax=KMax,\n",
    "                    zAsPercent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "volLocalInterp7.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "id": "J81DxZ_egiwa"
   },
   "outputs": [],
   "source": [
    "nbTimeStep = 100\n",
    "nbPaths = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8bNebQuboHm",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Local volatility is returned through linear interpolation on neural local volatilities obtained on the 10000 points grid.\n",
    "mcResVolLocale7 = backtest.MonteCarloPricerVectorized(S0,\n",
    "                                                      dataSetTest,\n",
    "                                                      bootstrap,\n",
    "                                                      nbPaths,\n",
    "                                                      nbTimeStep,\n",
    "                                                      nnVolLocale7)\n",
    "\n",
    "#Diagnose backtest results\n",
    "plotTools.predictionDiagnosis(mcResVolLocale7[\"Price\"], \n",
    "                              dataSetTest[\"Price\"], \n",
    "                              \" Monte Carlo Price \", \n",
    "                              yMin=KMin,\n",
    "                              yMax = KMax)\n",
    "workingFolder = \"./Results/\"\n",
    "mcResVolLocale7.to_csv(workingFolder + \"mcResVolLocale7.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest.rmse(mcResVolLocale7[\"Price\"][mcResVolLocale7[\"Price\"].index.get_level_values(\"Maturity\")>=1.0], \n",
    "              dataSetTest[\"Price\"][dataSetTest[\"Price\"].index.get_level_values(\"Maturity\")>=1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mcResVolLocale7[\"Price\"] - dataSetTest[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_awInfkboHo"
   },
   "outputs": [],
   "source": [
    "#PDE backtest with a cranck nicholson scheme\n",
    "pdeResVolLocale7 = backtest.PDEPricerVectorized(dataSetTest, S0, nnVolLocale7, bootstrap)\n",
    "\n",
    "#Backtest diagnosis\n",
    "plotTools.predictionDiagnosis(pdeResVolLocale7, \n",
    "                              dataSetTest[\"Price\"], \n",
    "                              \" PDE Price \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)\n",
    "pdeResVolLocale7.to_csv(workingFolder + \"pdeResVolLocale7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YSXE-ygvxPm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYxhx4e4boHr"
   },
   "outputs": [],
   "source": [
    "#Local volatility is returned through linear interpolation on neural local volatilities obtained on the testing grid.\n",
    "mcResVolLocale8 = backtest.MonteCarloPricerVectorized(S0,\n",
    "                                                      dataSetTest,\n",
    "                                                      bootstrap,\n",
    "                                                      nbPaths,\n",
    "                                                      nbTimeStep,\n",
    "                                                      nnVolLocale8)\n",
    "plotTools.predictionDiagnosis(mcResVolLocale8[\"Price\"], \n",
    "                              dataSetTest[\"Price\"], \n",
    "                              \" Price \", \n",
    "                              yMin=KMin,\n",
    "                              yMax = KMax)\n",
    "mcResVolLocale8.to_csv(workingFolder + \"mcResVolLocale8.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9f20-rMjboHs"
   },
   "outputs": [],
   "source": [
    "pdeResVolLocale8 = backtest.PDEPricerVectorized(dataSetTest, S0, nnVolLocale8, bootstrap)\n",
    "plotTools.predictionDiagnosis(pdeResVolLocale8, \n",
    "                              dataSetTest[\"Price\"], \n",
    "                              \" Price \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)\n",
    "pdeResVolLocale8.to_csv(workingFolder + \"pdeResVolLocale8.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZ5RauK0I0tS"
   },
   "source": [
    "#### NN: Learning Prices\n",
    "This section trains NNs to prices and not implied volatilities. You can skip the training step by loading in the left panel in colab workspace tensorflow models. These models are contained in the results folder of github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "D2sH0ejgI35p"
   },
   "outputs": [],
   "source": [
    "hyperparameters = {}\n",
    "#penalization coefficient\n",
    "hyperparameters[\"lambdaLocVol\"] = 10#1#0.001#0.01 #100\n",
    "hyperparameters[\"lambdaSoft\"] = 1000#100#0.0001#10#10 #100 \n",
    "hyperparameters[\"lambdaGamma\"] = 10000000#10000#100#10 #10000\n",
    "\n",
    "#Derivative soft constraints parameters\n",
    "hyperparameters[\"lowerBoundTheta\"] = 0.01\n",
    "hyperparameters[\"lowerBoundGamma\"] = 0.00001\n",
    "\n",
    "#Local variance parameters\n",
    "hyperparameters[\"DupireVarCap\"] = 10\n",
    "hyperparameters[\"DupireVolLowerBound\"] = 0.05\n",
    "hyperparameters[\"DupireVolUpperBound\"] = 0.40\n",
    "\n",
    "#Learning scheduler coefficient\n",
    "hyperparameters[\"LearningRateStart\"] = 0.1\n",
    "hyperparameters[\"Patience\"] = 100\n",
    "hyperparameters[\"batchSize\"] = 50\n",
    "hyperparameters[\"FinalLearningRate\"] = 1e-6\n",
    "hyperparameters[\"FixedLearningRate\"] = False\n",
    "\n",
    "#Training parameters\n",
    "hyperparameters[\"nbUnits\"] = 100 #number of units for hidden layers\n",
    "hyperparameters[\"maxEpoch\"] = 10000#10000 #maximum number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "TY67ZMLrI3wR"
   },
   "outputs": [],
   "source": [
    "#Execute this cell if you want to fit neural network with prices\n",
    "res = neuralNetwork.create_train_model(neuralNetwork.NNArchitectureVanillaSoftDupire,\n",
    "                                       scaledDataSet,\n",
    "                                       True,\n",
    "                                       hyperparameters,\n",
    "                                       scaler,\n",
    "                                       modelName = \"convexSoftVolModel\")\n",
    "y_pred4, volLocale4, dNN_T4, gNN_K4, lossSerie4 = res\n",
    "\n",
    "plotTools.plotEpochLoss(lossSerie4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hideCode": true,
    "id": "_hn7Ypu3I74I",
    "outputId": "8a07dd39-0c63-4fab-9277-f1d472fdde64"
   },
   "outputs": [],
   "source": [
    "# Evaluate results on the training set, you can execute that cell without training the model\n",
    "res = neuralNetwork.create_eval_model(neuralNetwork.NNArchitectureVanillaSoftDupire,\n",
    "                                      scaledDataSet,\n",
    "                                      True,\n",
    "                                      hyperparameters,\n",
    "                                      scaler,\n",
    "                                      modelName = \"convexSoftVolModel\")\n",
    "y_pred4, volLocale4, dNN_T4, gNN_K4, lossSerie4 = res\n",
    "\n",
    "plotTools.modelSummary(y_pred4,\n",
    "                       volLocale4, \n",
    "                       dNN_T4, \n",
    "                       gNN_K4, \n",
    "                       dataSet,\n",
    "                       S0,\n",
    "                       bootstrap,\n",
    "                       yMin = KMin,\n",
    "                       yMax = KMax,\n",
    "                       thresholdPrice = None, \n",
    "                       removeNaN = False,\n",
    "                       savePath = \"NeuralPriceTrain\")\n",
    "print(\"ATM Local Vol :\")\n",
    "print(volLocale4.loc[(midS0,slice(None))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hideCode": true,
    "id": "HPLM6CCYI7zC",
    "outputId": "917f3983-40cb-4d4a-ac29-f23a01bbbc06"
   },
   "outputs": [],
   "source": [
    "# Evaluate results on the testing dataset, you can execute that cell without training the model\n",
    "res = neuralNetwork.create_eval_model(neuralNetwork.NNArchitectureVanillaSoftDupire,\n",
    "                                      scaledDataSetTest,\n",
    "                                      True,\n",
    "                                      hyperparameters,\n",
    "                                      scaler,\n",
    "                                      modelName = \"convexSoftVolModel\")\n",
    "y_pred4Test, volLocale4Test, dNN_T4Test, gNN_K4Test, lossSerie4TestG = res\n",
    "\n",
    "plotTools.modelSummary(y_pred4Test,\n",
    "                       volLocale4Test,\n",
    "                       dNN_T4Test,\n",
    "                       gNN_K4Test,\n",
    "                       dataSetTest,\n",
    "                       S0,\n",
    "                       bootstrap,\n",
    "                       yMin = KMin,\n",
    "                       yMax = KMax,\n",
    "                       thresholdPrice = None, \n",
    "                       removeNaN = False,\n",
    "                       savePath = \"NeuralPriceTest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FRasRkcK9UR"
   },
   "source": [
    "#### Monte Carlo and PDE repricing backtests\n",
    "\n",
    "During Monte Carlo backtest, each option in testing set is priced with an underlying which is diffused with the following SDE : \n",
    "$$ dS_t = \\left( r_t - q_t - \\frac{\\sigma_{NN}^2(t, S_t)}{2} \\right) dt + \\sigma_{NN}(t, S_t) dW_t$$\n",
    "with $\\sigma_{NN}$ the neural local volatility function.\n",
    "\n",
    "Due to computation time issue we avoid to make millions of call to neural network and we interpolate linearly neural local volatility obtained on one the two possible grid :\n",
    "- the testing grid i.e. nodes $(T,K)$ of the testing set.\n",
    "- an artificial grid of 10000 points to check local volatility is correctly interpolated/extrapolated. That grid is the smallest rectangle containing the minimum and maximum maturities and the minimum and maximum strikes of our dataset (union of testing and training set).\n",
    "\n",
    "During PDE backtest, we used a crank-nicholson scheme to revaluate each option in our testing set.\n",
    "Time step corresponds to one day and space grid has 100 points. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FcJ7qM7cEpf"
   },
   "source": [
    "Monte Carlo backtest, each option in testing set is priced with an underlying which is diffused with the following SDE : \n",
    "$$ dS_t = \\left( r_t - q_t - \\frac{\\sigma_{NN}^2(t, S_t)}{2} \\right) dt + \\sigma_{NN}(t, S_t) dW_t$$\n",
    "with $\\sigma_{NN}$ the neural local volatility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "4Rg3N_h9K9UY"
   },
   "outputs": [],
   "source": [
    "# Function which evaluates neural local volatility when neural network is fitted on prices\n",
    "def neuralVolLocalePrix(s,t):\n",
    "    vLoc = neuralNetwork.evalVolLocale(neuralNetwork.NNArchitectureVanillaSoftDupire,\n",
    "                                       s, t,\n",
    "                                       dataSet,\n",
    "                                       hyperparameters,\n",
    "                                       scaler,\n",
    "                                       bootstrap,\n",
    "                                       S0,\n",
    "                                       modelName = \"convexSoftVolModel\")\n",
    "    return vLoc.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "URjRHuEPK9Ue"
   },
   "outputs": [],
   "source": [
    "volLocalInterp = neuralVolLocalePrix(volLocaleGrid[0].flatten(),\n",
    "                                     volLocaleGrid[1].flatten())\n",
    "\n",
    "volLocalInterp2 = neuralVolLocalePrix(dataSetTest.index.get_level_values(\"Strike\").values.flatten(),\n",
    "                                      dataSetTest.index.get_level_values(\"Maturity\").values.flatten())\n",
    "\n",
    "#Local volatility function for backtests\n",
    "nnVolLocale = lambda x,y : backtest.interpolatedMCLocalVolatility(volLocalInterp, x, y)\n",
    "\n",
    "#Local volatility function for backtests\n",
    "nnVolLocale2 = lambda x,y : backtest.interpolatedMCLocalVolatility(volLocalInterp2, x, y)\n",
    "\n",
    "plotTools.plotSerie(volLocalInterp,\n",
    "                    Title = 'Interpolated Local Volatility Surface',\n",
    "                    az=30,\n",
    "                    yMin=KMin,\n",
    "                    yMax=KMax,\n",
    "                    zAsPercent=True)\n",
    "\n",
    "plotTools.plotSerie(volLocalInterp2,\n",
    "                    Title = 'Interpolated Local Volatility Surface',\n",
    "                    az=30,\n",
    "                    yMin=KMin,\n",
    "                    yMax=KMax,\n",
    "                    zAsPercent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "RjiuaKvKK9Up"
   },
   "outputs": [],
   "source": [
    "nbTimeStep = 100\n",
    "nbPaths = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "iyWqs0CfK9Ur"
   },
   "outputs": [],
   "source": [
    "#Local volatility is returned through linear interpolation on neural local volatilities obtained on the 10000 points grid.\n",
    "mcResVolLocalePrix = backtest.MonteCarloPricerVectorized(S0,\n",
    "                                                         dataSetTest,\n",
    "                                                         bootstrap,\n",
    "                                                         nbPaths,\n",
    "                                                         nbTimeStep,\n",
    "                                                         nnVolLocale)\n",
    "plotTools.predictionDiagnosis(mcResVolLocalePrix[\"Price\"], \n",
    "                              dataSetTest[\"Price\"], \n",
    "                              \" Price \", \n",
    "                              yMin=KMin,\n",
    "                              yMax = KMax)\n",
    "workingFolder = \"./Results/\"\n",
    "mcResVolLocalePrix.to_csv(workingFolder + \"mcResVolLocalePrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "oJ7gDA2yK9Ut"
   },
   "outputs": [],
   "source": [
    "#PDE backtest with cranck-nicolson scheme\n",
    "pdeResVolLocale = backtest.PDEPricerVectorized(dataSetTest, S0, nnVolLocale, bootstrap)\n",
    "\n",
    "plotTools.predictionDiagnosis(pdeResVolLocale, \n",
    "                              dataSetTest[\"Price\"], \n",
    "                              \" Price \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)\n",
    "\n",
    "pdeResVolLocale.to_csv(workingFolder + \"pdeResVolLocale.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "zXjT-Ka-K9Ux"
   },
   "outputs": [],
   "source": [
    "#Local volatility is returned through linear interpolation on neural local volatilities obtained on the testing grid.\n",
    "mcResVolLocalePrix2 = backtest.MonteCarloPricerVectorized(S0,\n",
    "                                                          dataSetTest,\n",
    "                                                          bootstrap,\n",
    "                                                          nbPaths,\n",
    "                                                          nbTimeStep,\n",
    "                                                          nnVolLocale2)\n",
    "plotTools.predictionDiagnosis(mcResVolLocalePrix2[\"Price\"], \n",
    "                              dataSetTest[\"Price\"], \n",
    "                              \" Price \", \n",
    "                              yMin=KMin,\n",
    "                              yMax = KMax)\n",
    "mcResVolLocalePrix2.to_csv(workingFolder + \"mcResVolLocalePrix2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "fy2Rz-PPK9Uz"
   },
   "outputs": [],
   "source": [
    "#PDE backtest with cranck-nicolson scheme\n",
    "pdeResVolLocale2 = backtest.PDEPricerVectorized(dataSetTest, S0, nnVolLocale2, bootstrap)\n",
    "pdeResVolLocale2.head()\n",
    "plotTools.predictionDiagnosis(pdeResVolLocale2, \n",
    "                              dataSetTest[\"Price\"], \n",
    "                              \" Price \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)\n",
    "pdeResVolLocale2.to_csv(workingFolder + \"pdeResVolLocale2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2qWOo5QboHv"
   },
   "source": [
    "# Hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "xgCe1xMMboHv"
   },
   "outputs": [],
   "source": [
    "#Random selection of several hyperparameters \n",
    "neuralNetwork.selectHyperparametersRandom(hyperparameters,\n",
    "                                          [\"lambdaLocVol\",\"lambdaSoft\",\"lambdaGamma\"],\n",
    "                                          neuralNetwork.NNArchitectureVanillaSoftGatheral,\n",
    "                                          \"hyperParameters\",\n",
    "                                          True, \n",
    "                                          100,\n",
    "                                          scaledDataSet,\n",
    "                                          scaler,\n",
    "                                          trainedOnPrice = False,\n",
    "                                          logGrid = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "67-MaV1yboHw"
   },
   "outputs": [],
   "source": [
    "#hyperparameters[\"lambdaLocVol\"] = 100\n",
    "#hyperparameters[\"lambdaSoft\"] = 100 \n",
    "#hyperparameters[\"lambdaGamma\"] = 10000\n",
    "hyperparameters[\"lambdaLocVol\"] = 0.01#0.01 #100\n",
    "hyperparameters[\"lambdaSoft\"] = 0.01#10#10 #100 \n",
    "hyperparameters[\"lambdaGamma\"] = 100#10 #10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "0lqtfThIboHy"
   },
   "outputs": [],
   "source": [
    "#marginal selection of hyperparameters\n",
    "neuralNetwork.selectHyperparameters(hyperparameters, \n",
    "                                    \"lambdaLocVol\", \n",
    "                                    neuralNetwork.NNArchitectureVanillaSoftGatheral, \n",
    "                                    \"hyperParameters\", \n",
    "                                    True, \n",
    "                                    scaledDataSet,\n",
    "                                    scaler,\n",
    "                                    trainedOnPrice = False,\n",
    "                                    logGrid = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "P0yVsre-boHz"
   },
   "outputs": [],
   "source": [
    "neuralNetwork.selectHyperparameters(hyperparameters, \n",
    "                                    \"DupireVarCap\", \n",
    "                                    neuralNetwork.NNArchitectureVanillaSoftGatheral, \n",
    "                                    \"hyperParameters\", \n",
    "                                    True, \n",
    "                                    scaledDataSet,\n",
    "                                    scaler,\n",
    "                                    trainedOnPrice = False,\n",
    "                                    logGrid = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "xYjoiBznboH1"
   },
   "outputs": [],
   "source": [
    "neuralNetwork.selectHyperparameters(hyperparameters, \n",
    "                                    \"lambdaLocVol\", \n",
    "                                    neuralNetwork.NNArchitectureVanillaSoftGatheral, \n",
    "                                    \"hyperParameters\", \n",
    "                                    True, \n",
    "                                    scaledDataSet,\n",
    "                                    scaler,\n",
    "                                    trainedOnPrice = False,\n",
    "                                    logGrid = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "1T-giZuoboH2"
   },
   "outputs": [],
   "source": [
    "hyperparameters[\"lambdaLocVol\"] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "u6IaW1iLboH3"
   },
   "outputs": [],
   "source": [
    "neuralNetwork.selectHyperparameters(hyperparameters, \n",
    "                                    \"lambdaLocVol\", \n",
    "                                    neuralNetwork.NNArchitectureVanillaSoftGatheral, \n",
    "                                    \"hyperParameters\", \n",
    "                                    True, \n",
    "                                    scaledDataSet,\n",
    "                                    scaler,\n",
    "                                    trainedOnPrice = False,\n",
    "                                    logGrid = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "zm9rVu60boH5"
   },
   "outputs": [],
   "source": [
    "hyperparameters[\"nbUnits\"] = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "Wwq4DHV6boH6"
   },
   "outputs": [],
   "source": [
    "neuralNetwork.selectHyperparameters(hyperparameters, \n",
    "                                    \"nbUnits\", \n",
    "                                    neuralNetwork.NNArchitectureVanillaSoftGatheral, \n",
    "                                    \"hyperParameters\", \n",
    "                                    True, \n",
    "                                    scaledDataSet,\n",
    "                                    scaler,\n",
    "                                    trainedOnPrice = False,\n",
    "                                    logGrid = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "hdRi5oDNboH9"
   },
   "outputs": [],
   "source": [
    "hyperparameters[\"nbUnits\"] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "mwCVJCkOboH_"
   },
   "outputs": [],
   "source": [
    "neuralNetwork.selectHyperparameters(hyperparameters, \n",
    "                                    \"lambdaLocVol\", \n",
    "                                    neuralNetwork.NNArchitectureVanillaSoftGatheral, \n",
    "                                    \"hyperParameters\", \n",
    "                                    True, \n",
    "                                    scaledDataSet,\n",
    "                                    scaler,\n",
    "                                    trainedOnPrice = False,\n",
    "                                    logGrid = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "OS2naBJbboIB"
   },
   "outputs": [],
   "source": [
    "hyperparameters[\"nbUnits\"] = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "SkNQsJTjboIC"
   },
   "outputs": [],
   "source": [
    "neuralNetwork.selectHyperparameters(hyperparameters, \n",
    "                                    \"nbUnits\", \n",
    "                                    neuralNetwork.NNArchitectureVanillaSoftGatheral, \n",
    "                                    \"hyperParameters\", \n",
    "                                    True, \n",
    "                                    scaledDataSet,\n",
    "                                    scaler,\n",
    "                                    trainedOnPrice = False,\n",
    "                                    logGrid = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "id": "KNalh1ZRboIE"
   },
   "outputs": [],
   "source": [
    "hyperparameters[\"nbUnits\"] = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSVI Unconstrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run SSVI Model\n",
    "\n",
    "Implementation is inspired from Tahar Ferhati code : \n",
    "- Ferhati, T. (2020). Robust Calibration For SVI Model Arbitrage Free. Available at SSRN 3543766."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(SSVIFerhati)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#parameters, theta, maturities, pSSVI = SSVI.train_svi_surface(dataSet, S0)\n",
    "\n",
    "SSVIModel2 = SSVIFerhati.SSVIModelFerhati(S0, bootstrap)\n",
    "SSVIModel2.lambdaList = [0.0, 0.0, 0.0, 0.0, 0.0] #[1e-3, 1e-3, 1e-3, 1e-3, 1e-5]\n",
    "#SSVIModel2.automaticHyperparametersTuning(dataSet)\n",
    "SSVIModel2.fit(dataSet)\n",
    "\n",
    "serie = SSVIModel2.eval(dataSetTest)\n",
    "serieTrain = SSVIModel2.eval(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotTools.predictionDiagnosis(serieTrain , \n",
    "                              dataSet[BS.impliedVolColumn], \n",
    "                              \" Training Implied vol \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)\n",
    "\n",
    "impPriceTraining = plotTools.plotImpliedVolPrices(np.square(serieTrain)*serieTrain.index.get_level_values(\"Maturity\"),\n",
    "                                                  bootstrap, \n",
    "                                                  S0, \n",
    "                                                  dataSet,\n",
    "                                                  yMin = KMin,\n",
    "                                                  yMax = KMax, \n",
    "                                                  thresholdPrice = None)\n",
    "\n",
    "ImpVolPutTrainingSSVI = BS.vectorizedImpliedVolatilityCalibration(S0, bootstrap, \n",
    "                                                                 dataSet[\"Maturity\"], \n",
    "                                                                 dataSet[\"Strike\"], \n",
    "                                                                 dataSet[\"OptionType\"], \n",
    "                                                                 impPriceTraining,\n",
    "                                                                 removeNaN = False)\n",
    "ImpVolPutTrainingSSVI = pd.Series(ImpVolPutTrainingSSVI, index = dataSet.index).sort_index()\n",
    "\n",
    "plotTools.predictionDiagnosis(ImpVolPutTrainingSSVI, \n",
    "                              dataSet[BS.impliedVolColumn], \n",
    "                              \" Implied vol \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)\n",
    "\n",
    "plotTools.predictionDiagnosis(ImpVolPutTrainingSSVI, \n",
    "                              serieTrain, \n",
    "                              \" Implied vol \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "plotTools.predictionDiagnosis(SSVIFerhati.impliedVariance(serie), \n",
    "                              SSVIFerhati.impliedVariance(dataSetTest[BS.impliedVolColumn]), \n",
    "                              \" Implied total variance \", \n",
    "                              yMin=KMin, \n",
    "                              yMax=KMax)\n",
    "\n",
    "plotTools.predictionDiagnosis(serie, \n",
    "                              dataSetTest[BS.impliedVolColumn], \n",
    "                              \" Implied vol \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)\n",
    "threshold = None\n",
    "impPriceTesting = plotTools.plotImpliedVolPrices(np.square(serie)*serie.index.get_level_values(\"Maturity\"), \n",
    "                                                 bootstrap, \n",
    "                                                 S0, \n",
    "                                                 dataSetTest,\n",
    "                                                 yMin = KMin,\n",
    "                                                 yMax = KMax, \n",
    "                                                 thresholdPrice = threshold)\n",
    "\n",
    "ImpVolPutTestingSSVI = BS.vectorizedImpliedVolatilityCalibration(S0, bootstrap, \n",
    "                                                                 dataSetTest[\"Maturity\"], \n",
    "                                                                 dataSetTest[\"Strike\"], \n",
    "                                                                 dataSetTest[\"OptionType\"], \n",
    "                                                                 impPriceTesting,\n",
    "                                                                 removeNaN = False)\n",
    "ImpVolPutTestingSSVI = pd.Series(ImpVolPutTestingSSVI, index = dataSetTest.index).sort_index()\n",
    "\n",
    "plotTools.predictionDiagnosis(ImpVolPutTestingSSVI, \n",
    "                              dataSetTest[BS.impliedVolColumn], \n",
    "                              \" Implied vol \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate Local Volaitlity from SSVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dT, hk, dK, locVolSSVI, density = finiteDifferenceSVI(dataSet, interpolateWithSSVI)\n",
    "dTTrain, hkTrain, dKTrain, locVolSSVITrain, densityTrain = SSVIFerhati.finiteDifferenceSVI(dataSet, \n",
    "                                                                                           SSVIModel2.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotTools.diagnoseLocalVol(dTTrain,\n",
    "                           locVolSSVITrain,\n",
    "                           densityTrain,\n",
    "                           SSVIModel2.eval(dataSet),\n",
    "                           dataSet,\n",
    "                           az=320,\n",
    "                           yMin=KMin,\n",
    "                           yMax=KMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dT, hk, dK, locVolSSVI, density = finiteDifferenceSVI(dataSet, interpolateWithSSVI)\n",
    "dT, hk, dK, locVolSSVI, density = SSVIFerhati.finiteDifferenceSVI(dataSetTest, SSVIModel2.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotTools.diagnoseLocalVol(dT,\n",
    "                           locVolSSVI,\n",
    "                           density,\n",
    "                           SSVIModel2.eval(dataSetTest),\n",
    "                           dataSetTest,\n",
    "                           az=320,\n",
    "                           yMin=KMin,\n",
    "                           yMax=KMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "plotTools.saveDataModel(plotTools.removeDuplicateIndex(impPriceTraining), \n",
    "                        plotTools.removeDuplicateIndex(locVolSSVITrain), \n",
    "                        plotTools.removeDuplicateIndex(serieTrain) , \n",
    "                        \"./Results/SSVITrainingUnconstrained\") \n",
    "plotTools.saveDataModel(plotTools.removeDuplicateIndex(impPriceTesting), \n",
    "                        plotTools.removeDuplicateIndex(locVolSSVI), \n",
    "                        plotTools.removeDuplicateIndex(serie) , \n",
    "                        \"./Results/SSVITestingUnconstrained\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dT, hk, dK, locVolSSVI, density = SSVIFerhati.finiteDifferenceSVI(volLocaleGridDf, SSVIModel2.eval)\n",
    "\n",
    "plotTools.plotSerie(locVolSSVI.dropna(),\n",
    "                    yMin=KMin,\n",
    "                    yMax=KMax, \n",
    "                    az=105,\n",
    "                    Title = 'SSVI Local volatility')\n",
    "\n",
    "plotTools.plotSerie(locVolSSVI.dropna()[locVolSSVI.dropna() <= 1.0],\n",
    "                    yMin=KMin,\n",
    "                    yMax=KMax, \n",
    "                    az=105,\n",
    "                    Title = 'Filtered SSVI Local volatility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSVI Backtesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run SSVI Model\n",
    "\n",
    "Implementation is inspired from Matlab code of Philipp Rindler : \n",
    "- Philipp Rindler (2020). Gatherals and Jacquier's Arbitrage-Free SVI Volatility Surfaces (https://www.mathworks.com/matlabcentral/fileexchange/49962-gatherals-and-jacquier-s-arbitrage-free-svi-volatility-surfaces), MATLAB Central File Exchange. Retrieved November 20, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(SSVI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#parameters, theta, maturities, pSSVI = SSVI.train_svi_surface(dataSet, S0)\n",
    "np.seterr(all='warn')\n",
    "SSVIModel = SSVI.SSVIModel(S0, bootstrap)\n",
    "SSVIModel.fit(dataSet)\n",
    "\n",
    "serie = SSVIModel.eval(dataSetTest)\n",
    "serieTrain = SSVIModel.eval(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(pd.Series(SSVIModel.theta, index = SSVIModel.maturities))\n",
    "plt.show()\n",
    "pd.Series(SSVIModel.theta, index = SSVIModel.maturities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(pd.Series(SSVIModel.theta / SSVIModel.maturities, index = SSVIModel.maturities))\n",
    "plt.show()\n",
    "pd.Series(SSVIModel.theta / SSVIModel.maturities, index = SSVIModel.maturities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotTools.predictionDiagnosis(serieTrain , \n",
    "                              dataSet[BS.impliedVolColumn], \n",
    "                              \" Training Implied vol \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)\n",
    "\n",
    "impPriceTraining = plotTools.plotImpliedVolPrices(np.square(serieTrain)*serieTrain.index.get_level_values(\"Maturity\"),\n",
    "                                                  bootstrap, \n",
    "                                                  S0, \n",
    "                                                  dataSet,\n",
    "                                                  yMin = KMin,\n",
    "                                                  yMax = KMax, \n",
    "                                                  thresholdPrice = None)\n",
    "\n",
    "ImpVolPutTrainingSSVI = BS.vectorizedImpliedVolatilityCalibration(S0, bootstrap, \n",
    "                                                                 dataSet[\"Maturity\"], \n",
    "                                                                 dataSet[\"Strike\"], \n",
    "                                                                 dataSet[\"OptionType\"], \n",
    "                                                                 impPriceTraining,\n",
    "                                                                 removeNaN = False)\n",
    "ImpVolPutTrainingSSVI = pd.Series(ImpVolPutTrainingSSVI, index = dataSet.index).sort_index()\n",
    "\n",
    "plotTools.predictionDiagnosis(ImpVolPutTrainingSSVI, \n",
    "                              dataSet[BS.impliedVolColumn], \n",
    "                              \" Implied vol \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)\n",
    "\n",
    "plotTools.predictionDiagnosis(ImpVolPutTrainingSSVI, \n",
    "                              serieTrain, \n",
    "                              \" Implied vol \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "plotTools.predictionDiagnosis(SSVI.impliedVariance(serie), \n",
    "                              SSVI.impliedVariance(dataSetTest[BS.impliedVolColumn]), \n",
    "                              \" Implied total variance \", \n",
    "                              yMin=KMin, \n",
    "                              yMax=KMax)\n",
    "\n",
    "plotTools.predictionDiagnosis(serie, \n",
    "                              dataSetTest[BS.impliedVolColumn], \n",
    "                              \" Implied vol \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)\n",
    "threshold = None\n",
    "impPriceTesting = plotTools.plotImpliedVolPrices(np.square(serie)*serie.index.get_level_values(\"Maturity\"), \n",
    "                                                 bootstrap, \n",
    "                                                 S0, \n",
    "                                                 dataSetTest,\n",
    "                                                 yMin = KMin,\n",
    "                                                 yMax = KMax, \n",
    "                                                 thresholdPrice = threshold)\n",
    "\n",
    "ImpVolPutTestingSSVI = BS.vectorizedImpliedVolatilityCalibration(S0, bootstrap, \n",
    "                                                                 dataSetTest[\"Maturity\"], \n",
    "                                                                 dataSetTest[\"Strike\"], \n",
    "                                                                 dataSetTest[\"OptionType\"], \n",
    "                                                                 impPriceTesting,\n",
    "                                                                 removeNaN = False)\n",
    "ImpVolPutTestingSSVI = pd.Series(ImpVolPutTestingSSVI, index = dataSetTest.index).sort_index()\n",
    "\n",
    "plotTools.predictionDiagnosis(ImpVolPutTestingSSVI, \n",
    "                              dataSetTest[BS.impliedVolColumn], \n",
    "                              \" Implied vol \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate Local Volaitlity from SSVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dT, hk, dK, locVolSSVI, density = finiteDifferenceSVI(dataSet, interpolateWithSSVI)\n",
    "dTTrain, hkTrain, dKTrain, locVolSSVITrain, densityTrain = SSVI.finiteDifferenceSVI(dataSet, SSVIModel.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotTools.diagnoseLocalVol(dTTrain,\n",
    "                           locVolSSVITrain,\n",
    "                           densityTrain,\n",
    "                           SSVIModel.eval(dataSet),\n",
    "                           dataSet,\n",
    "                           az=320,\n",
    "                           yMin=KMin,\n",
    "                           yMax=KMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dT, hk, dK, locVolSSVI, density = finiteDifferenceSVI(dataSet, interpolateWithSSVI)\n",
    "dT, hk, dK, locVolSSVI, density = SSVI.finiteDifferenceSVI(dataSetTest, SSVIModel.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotTools.diagnoseLocalVol(dT,\n",
    "                           locVolSSVI,\n",
    "                           density,\n",
    "                           SSVIModel.eval(dataSetTest),\n",
    "                           dataSetTest,\n",
    "                           az=320,\n",
    "                           yMin=KMin,\n",
    "                           yMax=KMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "plotTools.saveDataModel(plotTools.removeDuplicateIndex(impPriceTraining), \n",
    "                        plotTools.removeDuplicateIndex(locVolSSVITrain), \n",
    "                        plotTools.removeDuplicateIndex(serieTrain) , \n",
    "                        \"./Results/SSVITraining\") \n",
    "plotTools.saveDataModel(plotTools.removeDuplicateIndex(impPriceTesting), \n",
    "                        plotTools.removeDuplicateIndex(locVolSSVI), \n",
    "                        plotTools.removeDuplicateIndex(serie) , \n",
    "                        \"./Results/SSVITesting\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dT, hk, dK, locVolSSVI, density = SSVI.finiteDifferenceSVI(volLocaleGridDf, SSVIModel.eval)\n",
    "\n",
    "plotTools.plotSerie(locVolSSVI.dropna(),\n",
    "                    yMin=KMin,\n",
    "                    yMax=KMax, \n",
    "                    az=105,\n",
    "                    Title = 'SSVI Local volatility')\n",
    "\n",
    "plotTools.plotSerie(locVolSSVI.dropna()[locVolSSVI.dropna() <= 1.0],\n",
    "                    yMin=KMin,\n",
    "                    yMax=KMax, \n",
    "                    az=105,\n",
    "                    Title = 'Filtered SSVI Local volatility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backtest Local volatility SSVI \n",
    "\n",
    "During Monte Carlo backtest, each option in testing set is priced with an underlying which is diffused with the following SDE : \n",
    "$$ dS_t = \\left( r_t - q_t - \\frac{\\sigma_{NN}^2(t, S_t)}{2} \\right) dt + \\sigma_{NN}(t, S_t) dW_t$$\n",
    "with $\\sigma_{NN}$ the neural local volatility function.\n",
    "\n",
    "Due to computation time issue we avoid to make millions of call to neural network and we interpolate linearly neural local volatility obtained on one the two possible grid :\n",
    "- the testing grid i.e. nodes $(T,K)$ of the testing set.\n",
    "- an artificial grid of 10000 points to check local volatility is correctly interpolated/extrapolated. That grid is the smallest rectangle containing the minimum and maximum maturities and the minimum and maximum strikes of our dataset (union of testing and training set).\n",
    "\n",
    "During PDE backtest, we used a crank-nicholson scheme to revaluate each option in our testing set.\n",
    "Time step corresponds to one day and space grid has 100 points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbTimeStep = 100\n",
    "nbPaths = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dT, hk, dK, locVolSSVI, density = finiteDifferenceSVI(dataSet, interpolateWithSSVI)\n",
    "dT, hk, dK, locVolSSVI, density = SSVI.finiteDifferenceSVI(dataSetTest, SSVIModel.eval)\n",
    "nnSSVI = lambda x,y : backtest.interpolatedMCLocalVolatility(locVolSSVI.dropna(), x, y)\n",
    "\n",
    "#dT, hk, dK, locVolSSVI, density = finiteDifferenceSVI(dataSet, interpolateWithSSVI)\n",
    "dT2, hk2, dK2, locVolSSVI2, density2 = SSVI.finiteDifferenceSVI(volLocaleGridDf, SSVIModel.eval)\n",
    "cleanValue = ~(dT2.isna() | density2.isna() | (dT2 < 0) | (density2 < 0) )\n",
    "dT2 = dT2[cleanValue.values]\n",
    "density2 = density2[cleanValue.values]\n",
    "hk2 = hk2[cleanValue.values]\n",
    "locVolSSVI2 = locVolSSVI2[cleanValue.values]\n",
    "dK2 = dK2[cleanValue.values]\n",
    "\n",
    "nnSSVI2 = lambda x,y : backtest.interpolatedMCLocalVolatility(locVolSSVI2.dropna(), x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mcResSSVITest = backtest.MonteCarloPricerVectorized(S0,\n",
    "                                                    dataSetTest,\n",
    "                                                    bootstrap,\n",
    "                                                    nbPaths,\n",
    "                                                    nbTimeStep,\n",
    "                                                    nnSSVI)\n",
    "workingFolder = \"./Results/\"\n",
    "mcResSSVITest.to_csv(workingFolder + \"mcResSSVITest.csv\")\n",
    "\n",
    "plotTools.predictionDiagnosis(mcResSSVITest[\"Price\"], \n",
    "                              dataSetTest[\"Price\"], \n",
    "                              \" Price \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdeResSigmaSSVITest = backtest.PDEPricerVectorized(dataSetTest, S0, nnSSVI, bootstrap)\n",
    "\n",
    "plotTools.predictionDiagnosis(pdeResSigmaSSVITest, \n",
    "                              dataSetTest[\"Price\"], \n",
    "                              \" Price \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)\n",
    "\n",
    "pdeResSigmaSSVITest.to_csv(workingFolder + \"pdeResSigmaSSVITest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "mcResSSVITest2 = backtest.MonteCarloPricerVectorized(S0,\n",
    "                                                     dataSetTest,\n",
    "                                                     bootstrap,\n",
    "                                                     nbPaths,\n",
    "                                                     nbTimeStep,\n",
    "                                                     nnSSVI2)\n",
    "\n",
    "plotTools.predictionDiagnosis(mcResSSVITest2[\"Price\"], \n",
    "                              dataSetTest[\"Price\"], \n",
    "                              \" Price \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)\n",
    "\n",
    "mcResSSVITest2.to_csv(workingFolder + \"mcResSSVITest2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "pdeResSigmaSSVITest2 = backtest.PDEPricerVectorized(dataSetTest, S0, nnSSVI2, bootstrap)\n",
    "\n",
    "plotTools.predictionDiagnosis(pdeResSigmaSSVITest2, \n",
    "                              dataSetTest[\"Price\"], \n",
    "                              \" Price \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)\n",
    "\n",
    "pdeResSigmaSSVITest2.to_csv(workingFolder + \"pdeResSigmaSSVITest2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trinomial Tree (Tikhonov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false
   },
   "source": [
    "### MC backtest \n",
    "\n",
    "During Monte Carlo backtest, each option in testing set is priced with an underlying which is diffused with the following SDE : \n",
    "$$ dS_t = \\left( r_t - q_t - \\frac{\\sigma_{NN}^2(t, S_t)}{2} \\right) dt + \\sigma_{NN}(t, S_t) dW_t$$\n",
    "with $\\sigma_{NN}$ the neural local volatility function.\n",
    "\n",
    "Due to computation time issue we avoid to make millions of call to neural network and we interpolate linearly neural local volatility obtained on one the two possible grid :\n",
    "- the testing grid i.e. nodes $(T,K)$ of the testing set.\n",
    "- an artificial grid of 10000 points to check local volatility is correctly interpolated/extrapolated. That grid is the smallest rectangle containing the minimum and maximum maturities and the minimum and maximum strikes of our dataset (union of testing and training set).\n",
    "\n",
    "For the tikhonov case we only use one grid which contains the node of the trinomial tree used for the calibration algorithm. See CREPEY, Stéphane. Calibration of the local volatility in a trinomial tree using Tikhonov regularization. Inverse Problems, 2002, vol. 19, no 1, p. 91.\n",
    "\n",
    "During PDE backtest, we used a crank-nicholson scheme to revaluate each option in our testing set.\n",
    "Time step corresponds to one day and space grid has 100 points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true
   },
   "source": [
    "#### Tikhonov calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "nbTimeStep = 100\n",
    "nbPaths = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "nnVolLocaleTykhonov = lambda x,y : backtest.interpolatedMCLocalVolatility(localVolatility[\"LocalVolatility\"], x, y)\n",
    "\n",
    "s = testingDataSetAresky.index.get_level_values(\"Strike\").values\n",
    "t = testingDataSetAresky.index.get_level_values(\"Maturity\").values\n",
    "tikhonovLocVol = nnVolLocaleTykhonov(s, t)\n",
    "\n",
    "plotTools.plotSerie(tikhonovLocVol[tikhonovLocVol.index.get_level_values(\"Maturity\") > 0.01],\n",
    "                    Title = 'Tikhonov Local Volatility Surface',\n",
    "                    az=30,\n",
    "                    yMin=0.0*S0,\n",
    "                    yMax=2.0*S0, \n",
    "                    zAsPercent=True)\n",
    "\n",
    "mcResTikhonovTest = backtest.MonteCarloPricerVectorized(S0,\n",
    "                                                        dataSetTest,\n",
    "                                                        bootstrap,\n",
    "                                                        nbPaths,\n",
    "                                                        nbTimeStep,\n",
    "                                                        nnVolLocaleTykhonov)\n",
    "\n",
    "plotTools.predictionDiagnosis(mcResTikhonovTest, \n",
    "                              dataSetTest[\"Price\"], \n",
    "                              \" Price \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)\n",
    "workingFolder = \"./Results/\"\n",
    "mcResTikhonovTest.to_csv(workingFolder + \"mcResTikhonovTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "mcResTikhonovTrain = backtest.MonteCarloPricerVectorized(S0,\n",
    "                                                         dataSet,\n",
    "                                                         bootstrap,\n",
    "                                                         nbPaths,\n",
    "                                                         nbTimeStep,\n",
    "                                                         nnVolLocaleTykhonov)\n",
    "\n",
    "plotTools.predictionDiagnosis(mcResTikhonovTrain, \n",
    "                              dataSet[\"Price\"], \n",
    "                              \" Price \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)\n",
    "\n",
    "mcResTikhonovTrain.to_csv(workingFolder + \"mcResTikhonovTrain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "pdeResSigmaTikhonovTrain = backtest.PDEPricerVectorized(dataSet, S0, nnVolLocaleTykhonov, bootstrap)\n",
    "pdeResSigmaTikhonovTrain.head()\n",
    "\n",
    "plotTools.predictionDiagnosis(pdeResSigmaTikhonovTrain, \n",
    "                              dataSet[\"Price\"], \n",
    "                              \" Price \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)\n",
    "\n",
    "pdeResSigmaTikhonovTrain.to_csv(workingFolder + \"pdeResSigmaTikhonovTrain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "pdeResSigmaTikhonovTest = backtest.PDEPricerVectorized(dataSetTest, S0, nnVolLocaleTykhonov, bootstrap)\n",
    "pdeResSigmaTikhonovTest.head()\n",
    "\n",
    "plotTools.predictionDiagnosis(pdeResSigmaTikhonovTest, \n",
    "                              dataSetTest[\"Price\"], \n",
    "                              \" Price \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)\n",
    "\n",
    "pdeResSigmaTikhonovTest.to_csv(workingFolder + \"pdeResSigmaTikhonovTest.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtest Result Comparisons (diagnostics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "mcResNNTest = backtest.loadMCPrices(\"./mcResVolLocale7.csv\", \n",
    "                                    parseHeader=0)\n",
    "mcResNNTest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdeResNNTest = backtest.loadMCPrices(\"./data/pdeResVolLocale7.csv\", \n",
    "                                     parseHeader=None)\n",
    "pdeResNNTest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdeResSSVITest = backtest.loadMCPrices(\"./data/pdeResSigmaSSVITest.csv\", \n",
    "                                      parseHeader=None)\n",
    "pdeResSSVITest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "backtest.rmse(dataSetTest[\"Price\"], pdeResSSVITest[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "backtest.rmse(dataSetTest[\"Price\"], pdeResNNTest[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mcResNNTest[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataSetTest[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "backtest.rmse(mcResSSVITest[\"Price\"], pdeResSSVITest[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "backtest.rmse(dataSetTest[\"Price\"], mcResSSVITest[\"Price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "backtest.rmse(dataSetTest[\"Price\"], mcResNNTest[\"Price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "mcResNNTest[\"Price\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotTools.predictionDiagnosis(mcResNNTest[\"Price\"], \n",
    "                              dataSetTest[\"Price\"], \n",
    "                              \" Price \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotTools.predictionDiagnosis(mcResSSVITest[\"Price\"], \n",
    "                              dataSetTest[\"Price\"], \n",
    "                              \" Price \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "pdeResSigmaGPTest = backtest.loadMCPrices(\"./Results/pdeResSigmaGPTest.csv\")\n",
    "pdeResSigmaGPTest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "evalVolImpli = lambda x : neuralNetwork.create_eval_model_gatheral(neuralNetwork.NNArchitectureVanillaSoftGatheral,\n",
    "                                                                   x,\n",
    "                                                                   True,\n",
    "                                                                   hyperparameters,\n",
    "                                                                   scaler,\n",
    "                                                                   modelName = \"convexSoftGatheralVolModel\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Implied Volatility and ignore miscalibrated implied volatilities\n",
    "\n",
    "This section removes nodes where implied volatility estimation is impossible for some models (GP, NN Price).\n",
    "This allow a fair comparison of implied volatility for model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detecting options which lead to implied volatilities miscalibration\n",
    "gpImpVolTraining = pd.Series(pd.read_csv(\"./Results\" + \"/GPTraining.csv\").set_index([\"Strike\",\"Maturity\"])[\"ImpliedVol\"],\n",
    "                             index = plotTools.removeDuplicateIndex(dataSet).index)\n",
    "gpImpVolTesting = pd.Series(pd.read_csv(\"./Results\" + \"/GPTesting.csv\").set_index([\"Strike\",\"Maturity\"])[\"ImpliedVol\"],\n",
    "                            index = plotTools.removeDuplicateIndex(dataSetTest).index)\n",
    "nnImpVolTraining = pd.Series(pd.read_csv(\"./Results\" + \"/NeuralPriceTrain.csv\").set_index([\"Strike\",\"Maturity\"])[\"ImpliedVol\"],\n",
    "                             index = plotTools.removeDuplicateIndex(dataSet).index)\n",
    "nnImpVolTesting = pd.Series(pd.read_csv(\"./Results\" + \"/NeuralPriceTest.csv\").set_index([\"Strike\",\"Maturity\"])[\"ImpliedVol\"],\n",
    "                            index = plotTools.removeDuplicateIndex(dataSetTest).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpImpVolTrainingError = gpImpVolTraining[gpImpVolTraining <= 1e-9].append(gpImpVolTraining[gpImpVolTraining >= 2])\n",
    "nnImpVolTrainingError = nnImpVolTraining[nnImpVolTraining <= 1e-9].append(nnImpVolTraining[nnImpVolTraining >= 2])\n",
    "trainingFilterImpVol = gpImpVolTrainingError.append(nnImpVolTrainingError).index.unique()\n",
    "\n",
    "gpImpVolTestingError = gpImpVolTesting[gpImpVolTesting <= 1e-9].append(gpImpVolTesting[gpImpVolTesting >= 2])\n",
    "nnImpVolTestingError = nnImpVolTesting[nnImpVolTesting <= 1e-9].append(nnImpVolTesting[nnImpVolTesting >= 2])\n",
    "testingFilterImpVol = gpImpVolTestingError.append(nnImpVolTestingError).index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load results from another model\n",
    "dfModelTraining = pd.read_csv(\"./Results\" + \"/NeuralImpliedVolTrain.csv\").set_index([\"Strike\",\"Maturity\"])\n",
    "dfModelTraining = pd.DataFrame(dfModelTraining.values, \n",
    "                               index = plotTools.removeDuplicateIndex(dataSet).index,\n",
    "                               columns = dfModelTraining.columns)\n",
    "\n",
    "keptVols = dataSet[BS.impliedVolColumn].drop(trainingFilterImpVol).index\n",
    "plotTools.predictionDiagnosis(plotTools.selectIndex(dfModelTraining[\"ImpliedVol\"], keptVols) , \n",
    "                              plotTools.selectIndex(dataSet[BS.impliedVolColumn], keptVols) , \n",
    "                              \" Implied vol \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfModelTesting = pd.read_csv(\"./Results\" + \"/NeuralImpliedVolTest.csv\").set_index([\"Strike\",\"Maturity\"])\n",
    "dfModelTesting = pd.DataFrame(dfModelTesting.values, \n",
    "                              index = plotTools.removeDuplicateIndex(dataSetTest).index,\n",
    "                              columns = dfModelTesting.columns)\n",
    "\n",
    "keptVols = dataSetTest[BS.impliedVolColumn].drop(testingFilterImpVol).index\n",
    "plotTools.predictionDiagnosis(plotTools.selectIndex(dfModelTesting[\"ImpliedVol\"], keptVols) , \n",
    "                              plotTools.selectIndex(dataSetTest[BS.impliedVolColumn], keptVols) , \n",
    "                              \" Implied vol \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "concatDf = pd.concat([dataSet, dataSetTest]).sort_index()\n",
    "concatDf = concatDf[~concatDf.index.duplicated(keep='first')]\n",
    "concatDf.index = loadData.roundMultiIndex(concatDf.index).rename([\"Strike\", \"Maturity\"])\n",
    "concatDfTrain = dataSet\n",
    "concatDfTrain.index = loadData.roundMultiIndex(concatDfTrain.index).rename([\"Strike\", \"Maturity\"])\n",
    "concatDfTest = dataSetTest\n",
    "concatDfTest.index = loadData.roundMultiIndex(concatDfTest.index).rename([\"Strike\", \"Maturity\"])\n",
    "\n",
    "GPTest = pd.read_csv('./Results/GPTesting.csv').set_index([\"Strike\", \"Maturity\"]) \n",
    "GPTrain = pd.read_csv('./Results/GPTraining.csv').set_index([\"Strike\", \"Maturity\"])\n",
    "GPResults = pd.concat([GPTrain, GPTest]).sort_index() \n",
    "GPResults.index = loadData.roundMultiIndex(GPResults.index).rename([\"Strike\", \"Maturity\"])\n",
    "\n",
    "NeuralTest = pd.read_csv('./Results/NeuralImpliedVolTest.csv').set_index([\"Strike\", \"Maturity\"]) \n",
    "NeuralTrain = pd.read_csv('./Results/NeuralImpliedVolTrain.csv').set_index([\"Strike\", \"Maturity\"])\n",
    "NeuralResults = pd.concat([NeuralTrain, NeuralTest]).sort_index()\n",
    "NeuralResults.index = loadData.roundMultiIndex(NeuralResults.index).rename([\"Strike\", \"Maturity\"])\n",
    "\n",
    "NeuralUnconstrainedTest = pd.read_csv('./Results/NeuralUnconstrainedImpliedVolTest.csv').set_index([\"Strike\", \"Maturity\"]) \n",
    "NeuralUnconstrainedTrain = pd.read_csv('./Results/NeuralUnconstrainedImpliedVolTrain.csv').set_index([\"Strike\", \"Maturity\"])\n",
    "NeuralUnconstrainedResults = pd.concat([NeuralUnconstrainedTrain, NeuralUnconstrainedTest]).sort_index()\n",
    "NeuralUnconstrainedResults.index = loadData.roundMultiIndex(NeuralUnconstrainedResults.index).rename([\"Strike\", \"Maturity\"])\n",
    "\n",
    "SSVITest = pd.read_csv('./Results/SSVITesting.csv').set_index([\"Strike\", \"Maturity\"]) \n",
    "SSVITest.index = loadData.roundMultiIndex(SSVITest.index).rename([\"Strike\", \"Maturity\"])\n",
    "SSVITrain = pd.read_csv('./Results/SSVITraining.csv').set_index([\"Strike\", \"Maturity\"])\n",
    "SSVITrain.index = loadData.roundMultiIndex(SSVITrain.index).rename([\"Strike\", \"Maturity\"])\n",
    "SSVIResults = pd.concat([SSVITrain, SSVITest]).sort_index()\n",
    "SSVIResults.index = loadData.roundMultiIndex(SSVIResults.index).rename([\"Strike\", \"Maturity\"])\n",
    "\n",
    "SSVITest2 = pd.read_csv('./Results/SSVITestingUnconstrained.csv').set_index([\"Strike\", \"Maturity\"]) \n",
    "SSVITest2.index = loadData.roundMultiIndex(SSVITest2.index).rename([\"Strike\", \"Maturity\"])\n",
    "SSVITrain2 = pd.read_csv('./Results/SSVITrainingUnconstrained.csv').set_index([\"Strike\", \"Maturity\"])\n",
    "SSVITrain2.index = loadData.roundMultiIndex(SSVITrain2.index).rename([\"Strike\", \"Maturity\"])\n",
    "SSVIResults2 = pd.concat([SSVITrain2, SSVITest2]).sort_index()\n",
    "SSVIResults2.index = loadData.roundMultiIndex(SSVIResults2.index).rename([\"Strike\", \"Maturity\"])\n",
    "\n",
    "NeuralResults.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(plotTools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SSVITrain.name = \"Arbitrage-free SSVI\"\n",
    "SSVITrain2.name = \"Unconstrained SSVI\"\n",
    "plotTools.plot2dSmiles(SSVITrain,\n",
    "                       SSVITrain2,\n",
    "                       None,\n",
    "                       dataSet,\n",
    "                       None,\n",
    "                       plotMarketData = False,\n",
    "                       nbObservationThreshold = 30,\n",
    "                       maturityList = None,\n",
    "                       showDiff = False,\n",
    "                       useLogMoneyness = True,\n",
    "                       gpQuantiles = None,\n",
    "                       legend = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SSVITest.name = \"Arbitrage-free SSVI\"\n",
    "SSVITest2.name = \"Unconstrained SSVI\"\n",
    "plotTools.plot2dSmiles(SSVITest,\n",
    "                       SSVITest2,\n",
    "                       None,\n",
    "                       None,\n",
    "                       dataSetTest,\n",
    "                       plotMarketData = False,\n",
    "                       nbObservationThreshold = 30,\n",
    "                       maturityList = None,\n",
    "                       showDiff = False,\n",
    "                       useLogMoneyness = True,\n",
    "                       gpQuantiles = None,\n",
    "                       legend = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SSVIResults.name = \"Arbitrage-free SSVI\"\n",
    "SSVIResults2.name = \"Unconstrained SSVI\"\n",
    "plotTools.plot2dSmiles(SSVIResults,\n",
    "                       None,\n",
    "                       SSVIResults2,\n",
    "                       dataSet,\n",
    "                       dataSetTest,\n",
    "                       plotMarketData = True,\n",
    "                       nbObservationThreshold = 100,\n",
    "                       maturityList = None,\n",
    "                       showDiff = False,\n",
    "                       useLogMoneyness = True,\n",
    "                       gpQuantiles = None,\n",
    "                       legend = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NeuralUnconstrainedResults.name = \"Unconstrained NN\"\n",
    "NeuralResults.name = \"Constrained NN\"\n",
    "plotTools.plot2dSmiles(NeuralUnconstrainedResults,\n",
    "                       None,\n",
    "                       NeuralResults,\n",
    "                       dataSet,\n",
    "                       dataSetTest,\n",
    "                       plotMarketData = True,\n",
    "                       nbObservationThreshold = 100,\n",
    "                       maturityList = None,\n",
    "                       showDiff = True,\n",
    "                       useLogMoneyness = True,\n",
    "                       gpQuantiles = None,\n",
    "                       legend = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SSVIResults.name = \"Arbitrage-free SSVI\"\n",
    "NeuralResults.name = \"Constrained NN\"\n",
    "plotTools.plot2dSmiles(SSVIResults,\n",
    "                       None,\n",
    "                       NeuralResults,\n",
    "                       dataSet,\n",
    "                       dataSetTest,\n",
    "                       plotMarketData = True,\n",
    "                       nbObservationThreshold = 100,\n",
    "                       maturityList = None,\n",
    "                       showDiff = True,\n",
    "                       useLogMoneyness = True,\n",
    "                       gpQuantiles = None,\n",
    "                       legend = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SSVIResults.name = \"Arbitrage-free SSVI\"\n",
    "NeuralResults.name = \"Constrained NN\"\n",
    "plotTools.plot2dSmiles(SSVIResults,\n",
    "                       None,\n",
    "                       NeuralResults,\n",
    "                       dataSet,\n",
    "                       dataSetTest,\n",
    "                       plotMarketData = True,\n",
    "                       nbObservationThreshold = 100,\n",
    "                       maturityList = None,\n",
    "                       showDiff = False,\n",
    "                       useLogMoneyness = True,\n",
    "                       gpQuantiles = None,\n",
    "                       legend = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SSVIResults.name = \"Arbitrage-free SSVI\"\n",
    "NeuralResults.name = \"Constrained NN\"\n",
    "NeuralUnconstrainedResults.name = \"Unconstrained NN\"\n",
    "plotTools.plot2dSmilesTotalVariance(SSVIResults,\n",
    "                                    NeuralUnconstrainedResults,\n",
    "                                    NeuralResults,\n",
    "                                    dataSet,\n",
    "                                    dataSetTest,\n",
    "                                    plotMarketData = False,\n",
    "                                    nbObservationThreshold = 0,\n",
    "                                    maturityList = [0.055, 0.074, 0.093, 0.189, 0.37, 0.841, 1.09,2.585],\n",
    "                                    showDiff = False,\n",
    "                                    useLogMoneyness = True,\n",
    "                                    gpQuantiles = None,\n",
    "                                    legend = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest.rmse(dataSetTest[\"Price\"], SSVITest[\"Price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest.rmse(dataSetTest[\"Price\"], NeuralResults[\"Price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest.rmse(dataSetTest[\"Price\"], NeuralUnconstrainedResults[\"Price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest.rmse(dataSetTest[\"Price\"], SSVIResults[\"Price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest.rmse(dataSetTest[\"Price\"], SSVIResults2[\"Price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest.rmse(dataSetTest[\"ImpVolCalibrated\"], SSVITest[\"ImpliedVol\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest.rmse(dataSetTest[\"ImpVolCalibrated\"], NeuralResults[\"ImpliedVol\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest.rmse(dataSetTest[\"ImpVolCalibrated\"], NeuralUnconstrainedResults[\"ImpliedVol\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest.rmse(dataSetTest[\"ImpVolCalibrated\"], SSVIResults2[\"ImpliedVol\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backtest Local volatility SSVI \n",
    "\n",
    "During Monte Carlo backtest, each option in testing set is priced with an underlying which is diffused with the following SDE : \n",
    "$$ dS_t = \\left( r_t - q_t - \\frac{\\sigma_{NN}^2(t, S_t)}{2} \\right) dt + \\sigma_{NN}(t, S_t) dW_t$$\n",
    "with $\\sigma_{NN}$ the neural local volatility function.\n",
    "\n",
    "Due to computation time issue we avoid to make millions of call to neural network and we interpolate linearly neural local volatility obtained on one the two possible grid :\n",
    "- the testing grid i.e. nodes $(T,K)$ of the testing set.\n",
    "- an artificial grid of 10000 points to check local volatility is correctly interpolated/extrapolated. That grid is the smallest rectangle containing the minimum and maximum maturities and the minimum and maximum strikes of our dataset (union of testing and training set).\n",
    "\n",
    "During PDE backtest, we used a crank-nicholson scheme to revaluate each option in our testing set.\n",
    "Time step corresponds to one day and space grid has 100 points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbTimeStep = 100\n",
    "nbPaths = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dT, hk, dK, locVolSSVI, density = finiteDifferenceSVI(dataSet, interpolateWithSSVI)\n",
    "dT, hk, dK, locVolSSVI, density = SSVIFerhati.finiteDifferenceSVI(dataSetTest, SSVIModel2.eval)\n",
    "nnSSVI = lambda x,y : backtest.interpolatedMCLocalVolatility(locVolSSVI, x, y)\n",
    "\n",
    "#dT, hk, dK, locVolSSVI, density = finiteDifferenceSVI(dataSet, interpolateWithSSVI)\n",
    "dT2, hk2, dK2, locVolSSVI2, density2 = SSVIFerhati.finiteDifferenceSVI(volLocaleGridDf, SSVIModel2.eval)\n",
    "cleanValue = ~(dT2.isna() | density2.isna() | (dT2 < 0) | (density2 < 0) )\n",
    "dT2 = dT2[cleanValue.values]\n",
    "density2 = density2[cleanValue.values]\n",
    "hk2 = hk2[cleanValue.values]\n",
    "locVolSSVI2 = locVolSSVI2[cleanValue.values]\n",
    "dK2 = dK2[cleanValue.values]\n",
    "\n",
    "nnSSVI2 = lambda x,y : backtest.interpolatedMCLocalVolatility(locVolSSVI2, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mcResSSVITest = backtest.MonteCarloPricerVectorized(S0,\n",
    "                                                    dataSetTest,\n",
    "                                                    bootstrap,\n",
    "                                                    nbPaths,\n",
    "                                                    nbTimeStep,\n",
    "                                                    nnSSVI)\n",
    "workingFolder = \"./Results/\"\n",
    "mcResSSVITest.to_csv(workingFolder + \"mcResSSVITest.csv\")\n",
    "\n",
    "plotTools.predictionDiagnosis(mcResSSVITest[\"Price\"], \n",
    "                              dataSetTest[\"Price\"], \n",
    "                              \" Price \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdeResSigmaSSVITest = backtest.PDEPricerVectorized(dataSetTest, S0, nnSSVI, bootstrap)\n",
    "\n",
    "plotTools.predictionDiagnosis(pdeResSigmaSSVITest, \n",
    "                              dataSetTest[\"Price\"], \n",
    "                              \" Price \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)\n",
    "\n",
    "pdeResSigmaSSVITest.to_csv(workingFolder + \"pdeResSigmaSSVITest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcResSSVITest2 = backtest.MonteCarloPricerVectorized(S0,\n",
    "                                                     dataSetTest,\n",
    "                                                     bootstrap,\n",
    "                                                     nbPaths,\n",
    "                                                     nbTimeStep,\n",
    "                                                     nnSSVI2)\n",
    "\n",
    "plotTools.predictionDiagnosis(mcResSSVITest2[\"Price\"], \n",
    "                              dataSetTest[\"Price\"], \n",
    "                              \" Price \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)\n",
    "\n",
    "mcResSSVITest2.to_csv(workingFolder + \"mcResSSVITest2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdeResSigmaSSVITest2 = backtest.PDEPricerVectorized(dataSetTest, S0, nnSSVI2, bootstrap)\n",
    "\n",
    "plotTools.predictionDiagnosis(pdeResSigmaSSVITest2, \n",
    "                              dataSetTest[\"Price\"], \n",
    "                              \" Price \", \n",
    "                              yMin=KMin,\n",
    "                              yMax=KMax)\n",
    "\n",
    "pdeResSigmaSSVITest2.to_csv(workingFolder + \"pdeResSigmaSSVITest2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "colab": {
   "name": "GP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
